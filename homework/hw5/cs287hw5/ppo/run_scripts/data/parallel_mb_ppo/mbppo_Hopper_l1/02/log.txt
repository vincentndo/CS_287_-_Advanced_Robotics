Logging to /home/vincent/workplace/CS_287_-_Advanced_Robotics/homework/hw5/cs287hw5/ppo/run_scripts/data/parallel_mb_ppo/mbppo_Hopper_l1//02

 ---------------- Iteration 0 ----------------
Obtaining random samples from the environment...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 0 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 1 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 2 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 3 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 4 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 5 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 6 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 7 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 8 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 9 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.191          |
| Data-EnvSampler-Poli... | 0.0408         |
| Data-EnvTrajs-Averag... | -284           |
| Data-EnvTrajs-MaxReturn | -172           |
| Data-EnvTrajs-MinReturn | -339           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 64.1           |
| Data-TimeEnvSampleProc  | 0.000506       |
| Data-TimeEnvSampling    | 0.243          |
| Iteration               | 0              |
| ItrTime                 | 9.57           |
| LossAfter               | -0.0073652556  |
| LossBefore              | -1.3811107e-05 |
| Model-TimeModelFit      | 3.1            |
| ModelSampler-n_times... | 40000          |
| Policy-AverageAbsPol... | 1.0634454      |
| Policy-AverageDiscou... | -2.1e+06       |
| Policy-AveragePolicyStd | 0.9644881      |
| Policy-AverageReturn    | -5.39e+06      |
| Policy-MaxReturn        | -5.33e+06      |
| Policy-MinReturn        | -5.45e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 3.56e+04       |
| Policy-TimeAlgoOpt      | 1.22           |
| Policy-TimeSampleProc   | 0.424          |
| Policy-TimeSampling     | 4.53           |
| Policy-TimeStep         | 6.23           |
| Time                    | 9.57           |
| n_timesteps             | 1000           |
--------------------------------------------

 ---------------- Iteration 1 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 10 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 11 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 12 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 13 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 14 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 15 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 16 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 17 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 18 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 19 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.327          |
| Data-EnvSampler-Poli... | 0.64           |
| Data-EnvTrajs-Averag... | -367           |
| Data-EnvTrajs-MaxReturn | -152           |
| Data-EnvTrajs-MinReturn | -440           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 109            |
| Data-TimeEnvSampleProc  | 0.000875       |
| Data-TimeEnvSampling    | 0.993          |
| Iteration               | 1              |
| ItrTime                 | 8.16           |
| LossAfter               | -0.005910823   |
| LossBefore              | -1.3559164e-05 |
| Model-TimeModelFit      | 4.63           |
| ModelSampler-n_times... | 80000          |
| Policy-AverageAbsPol... | 0.9709371      |
| Policy-AverageDiscou... | -1.92e+06      |
| Policy-AveragePolicyStd | 0.9385376      |
| Policy-AverageReturn    | -5.18e+06      |
| Policy-MaxReturn        | -5.14e+06      |
| Policy-MinReturn        | -5.22e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 2.47e+04       |
| Policy-TimeAlgoOpt      | 0.578          |
| Policy-TimeSampleProc   | 0.385          |
| Policy-TimeSampling     | 1.53           |
| Policy-TimeStep         | 2.53           |
| Time                    | 17.9           |
| n_timesteps             | 2000           |
--------------------------------------------

 ---------------- Iteration 2 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 20 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 21 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 22 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 23 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 24 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 25 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 26 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 27 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 28 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 29 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.338          |
| Data-EnvSampler-Poli... | 0.54           |
| Data-EnvTrajs-Averag... | -408           |
| Data-EnvTrajs-MaxReturn | -387           |
| Data-EnvTrajs-MinReturn | -442           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 18.6           |
| Data-TimeEnvSampleProc  | 0.000856       |
| Data-TimeEnvSampling    | 0.905          |
| Iteration               | 2              |
| ItrTime                 | 9.85           |
| LossAfter               | -0.006847385   |
| LossBefore              | -1.3415423e-05 |
| Model-TimeModelFit      | 6.52           |
| ModelSampler-n_times... | 120000         |
| Policy-AverageAbsPol... | 0.6501178      |
| Policy-AverageDiscou... | -7.42e+05      |
| Policy-AveragePolicyStd | 0.92654663     |
| Policy-AverageReturn    | -2.99e+06      |
| Policy-MaxReturn        | -4.93e+05      |
| Policy-MinReturn        | -4.44e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 8.12e+05       |
| Policy-TimeAlgoOpt      | 0.546          |
| Policy-TimeSampleProc   | 0.353          |
| Policy-TimeSampling     | 1.47           |
| Policy-TimeStep         | 2.42           |
| Time                    | 27.7           |
| n_timesteps             | 3000           |
--------------------------------------------

 ---------------- Iteration 3 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 30 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 31 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 32 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 33 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 34 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 35 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 36 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 37 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 38 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 39 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.321          |
| Data-EnvSampler-Poli... | 0.497          |
| Data-EnvTrajs-Averag... | -430           |
| Data-EnvTrajs-MaxReturn | -372           |
| Data-EnvTrajs-MinReturn | -468           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 36             |
| Data-TimeEnvSampleProc  | 0.000942       |
| Data-TimeEnvSampling    | 0.844          |
| Iteration               | 3              |
| ItrTime                 | 12.2           |
| LossAfter               | -0.0029753852  |
| LossBefore              | -1.3244285e-05 |
| Model-TimeModelFit      | 8.81           |
| ModelSampler-n_times... | 160000         |
| Policy-AverageAbsPol... | 0.56823486     |
| Policy-AverageDiscou... | -9.81e+05      |
| Policy-AveragePolicyStd | 0.9108076      |
| Policy-AverageReturn    | -3.47e+06      |
| Policy-MaxReturn        | -1.21e+06      |
| Policy-MinReturn        | -4.67e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.1e+06        |
| Policy-TimeAlgoOpt      | 0.537          |
| Policy-TimeSampleProc   | 0.406          |
| Policy-TimeSampling     | 1.54           |
| Policy-TimeStep         | 2.53           |
| Time                    | 39.9           |
| n_timesteps             | 4000           |
--------------------------------------------

 ---------------- Iteration 4 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 40 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 41 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 42 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 43 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 44 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 45 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 46 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 47 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 48 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 49 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.334          |
| Data-EnvSampler-Poli... | 0.493          |
| Data-EnvTrajs-Averag... | -455           |
| Data-EnvTrajs-MaxReturn | -426           |
| Data-EnvTrajs-MinReturn | -488           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 21.9           |
| Data-TimeEnvSampleProc  | 0.00104        |
| Data-TimeEnvSampling    | 0.854          |
| Iteration               | 4              |
| ItrTime                 | 14.1           |
| LossAfter               | -0.003473273   |
| LossBefore              | -1.3140224e-05 |
| Model-TimeModelFit      | 10.7           |
| ModelSampler-n_times... | 200000         |
| Policy-AverageAbsPol... | 0.3971542      |
| Policy-AverageDiscou... | -3.96e+04      |
| Policy-AveragePolicyStd | 0.90025735     |
| Policy-AverageReturn    | -1.61e+05      |
| Policy-MaxReturn        | -413           |
| Policy-MinReturn        | -3.21e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 7e+05          |
| Policy-TimeAlgoOpt      | 0.473          |
| Policy-TimeSampleProc   | 0.65           |
| Policy-TimeSampling     | 1.4            |
| Policy-TimeStep         | 2.55           |
| Time                    | 54             |
| n_timesteps             | 5000           |
--------------------------------------------

 ---------------- Iteration 5 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 50 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 51 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 52 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 53 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 54 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 55 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 56 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 57 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 58 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 59 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.32           |
| Data-EnvSampler-Poli... | 0.469          |
| Data-EnvTrajs-Averag... | -394           |
| Data-EnvTrajs-MaxReturn | -376           |
| Data-EnvTrajs-MinReturn | -407           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 10.5           |
| Data-TimeEnvSampleProc  | 0.000937       |
| Data-TimeEnvSampling    | 0.814          |
| Iteration               | 5              |
| ItrTime                 | 16.1           |
| LossAfter               | -0.005796197   |
| LossBefore              | -1.3129231e-05 |
| Model-TimeModelFit      | 12.5           |
| ModelSampler-n_times... | 240000         |
| Policy-AverageAbsPol... | 0.41429988     |
| Policy-AverageDiscou... | -1.75e+05      |
| Policy-AveragePolicyStd | 0.9001968      |
| Policy-AverageReturn    | -7.09e+05      |
| Policy-MaxReturn        | -266           |
| Policy-MinReturn        | -3.83e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.32e+06       |
| Policy-TimeAlgoOpt      | 0.582          |
| Policy-TimeSampleProc   | 0.519          |
| Policy-TimeSampling     | 1.7            |
| Policy-TimeStep         | 2.83           |
| Time                    | 70.1           |
| n_timesteps             | 6000           |
--------------------------------------------

 ---------------- Iteration 6 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 60 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 61 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 62 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 63 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 64 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 65 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 66 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 67 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 68 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 69 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.277          |
| Data-EnvSampler-Poli... | 0.376          |
| Data-EnvTrajs-Averag... | -393           |
| Data-EnvTrajs-MaxReturn | -376           |
| Data-EnvTrajs-MinReturn | -414           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 12.3           |
| Data-TimeEnvSampleProc  | 0.000795       |
| Data-TimeEnvSampling    | 0.674          |
| Iteration               | 6              |
| ItrTime                 | 18.1           |
| LossAfter               | -0.004947267   |
| LossBefore              | -1.3042626e-05 |
| Model-TimeModelFit      | 14.6           |
| ModelSampler-n_times... | 280000         |
| Policy-AverageAbsPol... | 0.5258774      |
| Policy-AverageDiscou... | -4.14e+05      |
| Policy-AveragePolicyStd | 0.892264       |
| Policy-AverageReturn    | -1.76e+06      |
| Policy-MaxReturn        | -132           |
| Policy-MinReturn        | -3.71e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.45e+06       |
| Policy-TimeAlgoOpt      | 0.604          |
| Policy-TimeSampleProc   | 0.439          |
| Policy-TimeSampling     | 1.63           |
| Policy-TimeStep         | 2.75           |
| Time                    | 88.2           |
| n_timesteps             | 7000           |
--------------------------------------------

 ---------------- Iteration 7 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 70 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 71 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 72 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 73 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 74 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 75 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 76 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 77 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 78 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 79 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.277         |
| Data-EnvSampler-Poli... | 0.374         |
| Data-EnvTrajs-Averag... | -411          |
| Data-EnvTrajs-MaxReturn | -392          |
| Data-EnvTrajs-MinReturn | -446          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 19            |
| Data-TimeEnvSampleProc  | 0.000847      |
| Data-TimeEnvSampling    | 0.674         |
| Iteration               | 7             |
| ItrTime                 | 21            |
| LossAfter               | -0.004001406  |
| LossBefore              | -1.295689e-05 |
| Model-TimeModelFit      | 17.6          |
| ModelSampler-n_times... | 320000        |
| Policy-AverageAbsPol... | 0.53623986    |
| Policy-AverageDiscou... | -149          |
| Policy-AveragePolicyStd | 0.88227135    |
| Policy-AverageReturn    | -502          |
| Policy-MaxReturn        | -395          |
| Policy-MinReturn        | -960          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 116           |
| Policy-TimeAlgoOpt      | 0.612         |
| Policy-TimeSampleProc   | 0.551         |
| Policy-TimeSampling     | 1.55          |
| Policy-TimeStep         | 2.77          |
| Time                    | 109           |
| n_timesteps             | 8000          |
-------------------------------------------

 ---------------- Iteration 8 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 80 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 81 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 82 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 83 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 84 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 85 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 86 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 87 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 88 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 89 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.287          |
| Data-EnvSampler-Poli... | 0.396          |
| Data-EnvTrajs-Averag... | -439           |
| Data-EnvTrajs-MaxReturn | -420           |
| Data-EnvTrajs-MinReturn | -459           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 13.8           |
| Data-TimeEnvSampleProc  | 0.000988       |
| Data-TimeEnvSampling    | 0.705          |
| Iteration               | 8              |
| ItrTime                 | 22.2           |
| LossAfter               | -0.0056050173  |
| LossBefore              | -1.2720371e-05 |
| Model-TimeModelFit      | 18.6           |
| ModelSampler-n_times... | 360000         |
| Policy-AverageAbsPol... | 0.5048777      |
| Policy-AverageDiscou... | -7.84e+05      |
| Policy-AveragePolicyStd | 0.8645705      |
| Policy-AverageReturn    | -3.19e+06      |
| Policy-MaxReturn        | -2.86e+06      |
| Policy-MinReturn        | -3.41e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.31e+05       |
| Policy-TimeAlgoOpt      | 0.586          |
| Policy-TimeSampleProc   | 0.544          |
| Policy-TimeSampling     | 1.76           |
| Policy-TimeStep         | 2.96           |
| Time                    | 131            |
| n_timesteps             | 9000           |
--------------------------------------------

 ---------------- Iteration 9 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 90 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 91 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 92 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 93 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 94 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 95 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 96 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 97 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 98 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 99 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.308          |
| Data-EnvSampler-Poli... | 0.422          |
| Data-EnvTrajs-Averag... | -213           |
| Data-EnvTrajs-MaxReturn | -144           |
| Data-EnvTrajs-MinReturn | -316           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 61.8           |
| Data-TimeEnvSampleProc  | 0.000797       |
| Data-TimeEnvSampling    | 0.752          |
| Iteration               | 9              |
| ItrTime                 | 24.2           |
| LossAfter               | -0.003698215   |
| LossBefore              | -1.2387351e-05 |
| Model-TimeModelFit      | 20.7           |
| ModelSampler-n_times... | 400000         |
| Policy-AverageAbsPol... | 0.45266747     |
| Policy-AverageDiscou... | -102           |
| Policy-AveragePolicyStd | 0.83441615     |
| Policy-AverageReturn    | -358           |
| Policy-MaxReturn        | -281           |
| Policy-MinReturn        | -448           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 41.5           |
| Policy-TimeAlgoOpt      | 0.504          |
| Policy-TimeSampleProc   | 0.573          |
| Policy-TimeSampling     | 1.6            |
| Policy-TimeStep         | 2.74           |
| Time                    | 156            |
| n_timesteps             | 10000          |
--------------------------------------------

 ---------------- Iteration 10 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 100 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 101 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 102 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 103 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 104 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 105 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 106 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 107 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 108 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 109 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.309          |
| Data-EnvSampler-Poli... | 0.419          |
| Data-EnvTrajs-Averag... | -109           |
| Data-EnvTrajs-MaxReturn | -91.9          |
| Data-EnvTrajs-MinReturn | -118           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 9.28           |
| Data-TimeEnvSampleProc  | 0.000902       |
| Data-TimeEnvSampling    | 0.751          |
| Iteration               | 10             |
| ItrTime                 | 26.4           |
| LossAfter               | -0.0048664175  |
| LossBefore              | -1.2309914e-05 |
| Model-TimeModelFit      | 22.8           |
| ModelSampler-n_times... | 440000         |
| Policy-AverageAbsPol... | 0.8753092      |
| Policy-AverageDiscou... | -17.9          |
| Policy-AveragePolicyStd | 0.82672167     |
| Policy-AverageReturn    | -160           |
| Policy-MaxReturn        | 38.6           |
| Policy-MinReturn        | -1.18e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 251            |
| Policy-TimeAlgoOpt      | 0.563          |
| Policy-TimeSampleProc   | 0.571          |
| Policy-TimeSampling     | 1.69           |
| Policy-TimeStep         | 2.84           |
| Time                    | 182            |
| n_timesteps             | 11000          |
--------------------------------------------

 ---------------- Iteration 11 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 110 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 111 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 112 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 113 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 114 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 115 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 116 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 117 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 118 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 119 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.315          |
| Data-EnvSampler-Poli... | 0.381          |
| Data-EnvTrajs-Averag... | -123           |
| Data-EnvTrajs-MaxReturn | -116           |
| Data-EnvTrajs-MinReturn | -136           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 8.13           |
| Data-TimeEnvSampleProc  | 0.000589       |
| Data-TimeEnvSampling    | 0.718          |
| Iteration               | 11             |
| ItrTime                 | 29             |
| LossAfter               | -0.00406089    |
| LossBefore              | -1.2003801e-05 |
| Model-TimeModelFit      | 25.6           |
| ModelSampler-n_times... | 480000         |
| Policy-AverageAbsPol... | 1.0010669      |
| Policy-AverageDiscou... | 53.8           |
| Policy-AveragePolicyStd | 0.8063759      |
| Policy-AverageReturn    | 65.5           |
| Policy-MaxReturn        | 126            |
| Policy-MinReturn        | -85.8          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 49.4           |
| Policy-TimeAlgoOpt      | 0.621          |
| Policy-TimeSampleProc   | 0.239          |
| Policy-TimeSampling     | 1.75           |
| Policy-TimeStep         | 2.64           |
| Time                    | 211            |
| n_timesteps             | 12000          |
--------------------------------------------

 ---------------- Iteration 12 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 120 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 121 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 122 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 123 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 124 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 125 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 126 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 127 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 128 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 129 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.306          |
| Data-EnvSampler-Poli... | 0.434          |
| Data-EnvTrajs-Averag... | -113           |
| Data-EnvTrajs-MaxReturn | -94.9          |
| Data-EnvTrajs-MinReturn | -128           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 11.3           |
| Data-TimeEnvSampleProc  | 0.000916       |
| Data-TimeEnvSampling    | 0.762          |
| Iteration               | 12             |
| ItrTime                 | 29.5           |
| LossAfter               | -0.0065836897  |
| LossBefore              | -1.1944161e-05 |
| Model-TimeModelFit      | 25.9           |
| ModelSampler-n_times... | 520000         |
| Policy-AverageAbsPol... | 0.5580253      |
| Policy-AverageDiscou... | 46.6           |
| Policy-AveragePolicyStd | 0.8009358      |
| Policy-AverageReturn    | -168           |
| Policy-MaxReturn        | 437            |
| Policy-MinReturn        | -3.33e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 786            |
| Policy-TimeAlgoOpt      | 0.59           |
| Policy-TimeSampleProc   | 0.541          |
| Policy-TimeSampling     | 1.6            |
| Policy-TimeStep         | 2.76           |
| Time                    | 241            |
| n_timesteps             | 13000          |
--------------------------------------------

 ---------------- Iteration 13 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 130 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 131 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 132 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 133 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 134 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 135 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 136 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 137 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 138 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 139 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.336          |
| Data-EnvSampler-Poli... | 0.489          |
| Data-EnvTrajs-Averag... | -98.3          |
| Data-EnvTrajs-MaxReturn | -88            |
| Data-EnvTrajs-MinReturn | -114           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 9.02           |
| Data-TimeEnvSampleProc  | 0.000849       |
| Data-TimeEnvSampling    | 0.851          |
| Iteration               | 13             |
| ItrTime                 | 28.8           |
| LossAfter               | -0.0046744724  |
| LossBefore              | -1.1592704e-05 |
| Model-TimeModelFit      | 25.4           |
| ModelSampler-n_times... | 560000         |
| Policy-AverageAbsPol... | 0.6858658      |
| Policy-AverageDiscou... | 83.8           |
| Policy-AveragePolicyStd | 0.77148414     |
| Policy-AverageReturn    | 156            |
| Policy-MaxReturn        | 198            |
| Policy-MinReturn        | -57            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 54.2           |
| Policy-TimeAlgoOpt      | 0.612          |
| Policy-TimeSampleProc   | 0.354          |
| Policy-TimeSampling     | 1.53           |
| Policy-TimeStep         | 2.52           |
| Time                    | 269            |
| n_timesteps             | 14000          |
--------------------------------------------

 ---------------- Iteration 14 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 140 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 141 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 142 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 143 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 144 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 145 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 146 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 147 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 148 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 149 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.334          |
| Data-EnvSampler-Poli... | 0.482          |
| Data-EnvTrajs-Averag... | -124           |
| Data-EnvTrajs-MaxReturn | -113           |
| Data-EnvTrajs-MinReturn | -131           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 6.17           |
| Data-TimeEnvSampleProc  | 0.000736       |
| Data-TimeEnvSampling    | 0.841          |
| Iteration               | 14             |
| ItrTime                 | 30.1           |
| LossAfter               | -0.0021606481  |
| LossBefore              | -1.1070934e-05 |
| Model-TimeModelFit      | 26.6           |
| ModelSampler-n_times... | 600000         |
| Policy-AverageAbsPol... | 0.9917661      |
| Policy-AverageDiscou... | 49.7           |
| Policy-AveragePolicyStd | 0.731227       |
| Policy-AverageReturn    | 18.3           |
| Policy-MaxReturn        | 47.9           |
| Policy-MinReturn        | -6.42          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 15.8           |
| Policy-TimeAlgoOpt      | 0.595          |
| Policy-TimeSampleProc   | 0.401          |
| Policy-TimeSampling     | 1.6            |
| Policy-TimeStep         | 2.67           |
| Time                    | 299            |
| n_timesteps             | 15000          |
--------------------------------------------

 ---------------- Iteration 15 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 150 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 151 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 152 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 153 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 154 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 155 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 156 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 157 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 158 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 159 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.369          |
| Data-EnvSampler-Poli... | 0.537          |
| Data-EnvTrajs-Averag... | -89.8          |
| Data-EnvTrajs-MaxReturn | -57.2          |
| Data-EnvTrajs-MinReturn | -108           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 17.6           |
| Data-TimeEnvSampleProc  | 0.000996       |
| Data-TimeEnvSampling    | 0.934          |
| Iteration               | 15             |
| ItrTime                 | 29.9           |
| LossAfter               | -0.0056040394  |
| LossBefore              | -1.0791366e-05 |
| Model-TimeModelFit      | 26.5           |
| ModelSampler-n_times... | 640000         |
| Policy-AverageAbsPol... | 0.64700687     |
| Policy-AverageDiscou... | 43             |
| Policy-AveragePolicyStd | 0.71183676     |
| Policy-AverageReturn    | -232           |
| Policy-MaxReturn        | 147            |
| Policy-MinReturn        | -3.9e+03       |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 870            |
| Policy-TimeAlgoOpt      | 0.526          |
| Policy-TimeSampleProc   | 0.348          |
| Policy-TimeSampling     | 1.41           |
| Policy-TimeStep         | 2.44           |
| Time                    | 329            |
| n_timesteps             | 16000          |
--------------------------------------------

 ---------------- Iteration 16 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 160 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 161 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 162 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 163 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 164 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 165 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 166 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 167 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 168 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 169 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.332          |
| Data-EnvSampler-Poli... | 0.471          |
| Data-EnvTrajs-Averag... | -110           |
| Data-EnvTrajs-MaxReturn | -78            |
| Data-EnvTrajs-MinReturn | -128           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 17.3           |
| Data-TimeEnvSampleProc  | 0.000947       |
| Data-TimeEnvSampling    | 0.829          |
| Iteration               | 16             |
| ItrTime                 | 31.2           |
| LossAfter               | -0.0073249033  |
| LossBefore              | -1.0736727e-05 |
| Model-TimeModelFit      | 27.8           |
| ModelSampler-n_times... | 680000         |
| Policy-AverageAbsPol... | 0.6043667      |
| Policy-AverageDiscou... | 59.2           |
| Policy-AveragePolicyStd | 0.7082738      |
| Policy-AverageReturn    | -165           |
| Policy-MaxReturn        | 360            |
| Policy-MinReturn        | -4.18e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 929            |
| Policy-TimeAlgoOpt      | 0.62           |
| Policy-TimeSampleProc   | 0.438          |
| Policy-TimeSampling     | 1.4            |
| Policy-TimeStep         | 2.49           |
| Time                    | 361            |
| n_timesteps             | 17000          |
--------------------------------------------

 ---------------- Iteration 17 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 170 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 171 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 172 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 173 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 174 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 175 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 176 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 177 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 178 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 179 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.404          |
| Data-EnvSampler-Poli... | 0.644          |
| Data-EnvTrajs-Averag... | -56.3          |
| Data-EnvTrajs-MaxReturn | -23            |
| Data-EnvTrajs-MinReturn | -87.8          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 26.5           |
| Data-TimeEnvSampleProc  | 0.000667       |
| Data-TimeEnvSampling    | 1.08           |
| Iteration               | 17             |
| ItrTime                 | 30.4           |
| LossAfter               | -0.0057064733  |
| LossBefore              | -1.0718956e-05 |
| Model-TimeModelFit      | 26.6           |
| ModelSampler-n_times... | 720000         |
| Policy-AverageAbsPol... | 0.56057435     |
| Policy-AverageDiscou... | -75.8          |
| Policy-AveragePolicyStd | 0.70709425     |
| Policy-AverageReturn    | -819           |
| Policy-MaxReturn        | -68            |
| Policy-MinReturn        | -4.75e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.25e+03       |
| Policy-TimeAlgoOpt      | 0.481          |
| Policy-TimeSampleProc   | 0.591          |
| Policy-TimeSampling     | 1.6            |
| Policy-TimeStep         | 2.71           |
| Time                    | 391            |
| n_timesteps             | 18000          |
--------------------------------------------

 ---------------- Iteration 18 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 180 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 181 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 182 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 183 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 184 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 185 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 186 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 187 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 188 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 189 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.415          |
| Data-EnvSampler-Poli... | 0.675          |
| Data-EnvTrajs-Averag... | 96.3           |
| Data-EnvTrajs-MaxReturn | 140            |
| Data-EnvTrajs-MinReturn | 41.8           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 33.7           |
| Data-TimeEnvSampleProc  | 0.000774       |
| Data-TimeEnvSampling    | 1.12           |
| Iteration               | 18             |
| ItrTime                 | 30             |
| LossAfter               | -0.005383976   |
| LossBefore              | -1.0486083e-05 |
| Model-TimeModelFit      | 26.5           |
| ModelSampler-n_times... | 760000         |
| Policy-AverageAbsPol... | 0.80726904     |
| Policy-AverageDiscou... | -281           |
| Policy-AveragePolicyStd | 0.69114095     |
| Policy-AverageReturn    | -2.42e+03      |
| Policy-MaxReturn        | 248            |
| Policy-MinReturn        | -3.57e+04      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 8.06e+03       |
| Policy-TimeAlgoOpt      | 0.499          |
| Policy-TimeSampleProc   | 0.431          |
| Policy-TimeSampling     | 1.41           |
| Policy-TimeStep         | 2.36           |
| Time                    | 421            |
| n_timesteps             | 19000          |
--------------------------------------------

 ---------------- Iteration 19 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 190 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 191 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 192 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 193 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 194 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 195 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 196 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 197 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 198 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 199 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.407          |
| Data-EnvSampler-Poli... | 0.668          |
| Data-EnvTrajs-Averag... | 135            |
| Data-EnvTrajs-MaxReturn | 264            |
| Data-EnvTrajs-MinReturn | 45.8           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 74.1           |
| Data-TimeEnvSampleProc  | 0.00104        |
| Data-TimeEnvSampling    | 1.11           |
| Iteration               | 19             |
| ItrTime                 | 31.2           |
| LossAfter               | -0.0042660055  |
| LossBefore              | -1.0507802e-05 |
| Model-TimeModelFit      | 27.8           |
| ModelSampler-n_times... | 800000         |
| Policy-AverageAbsPol... | 0.64679986     |
| Policy-AverageDiscou... | -55.8          |
| Policy-AveragePolicyStd | 0.6899354      |
| Policy-AverageReturn    | -757           |
| Policy-MaxReturn        | 3.9            |
| Policy-MinReturn        | -6.76e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.76e+03       |
| Policy-TimeAlgoOpt      | 0.528          |
| Policy-TimeSampleProc   | 0.31           |
| Policy-TimeSampling     | 1.4            |
| Policy-TimeStep         | 2.27           |
| Time                    | 452            |
| n_timesteps             | 20000          |
--------------------------------------------

 ---------------- Iteration 20 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 200 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 201 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 202 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 203 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 204 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 205 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 206 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 207 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 208 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 209 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.407          |
| Data-EnvSampler-Poli... | 0.683          |
| Data-EnvTrajs-Averag... | 228            |
| Data-EnvTrajs-MaxReturn | 267            |
| Data-EnvTrajs-MinReturn | 149            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 46.2           |
| Data-TimeEnvSampleProc  | 0.000921       |
| Data-TimeEnvSampling    | 1.12           |
| Iteration               | 20             |
| ItrTime                 | 30.7           |
| LossAfter               | -0.003360177   |
| LossBefore              | -1.0309911e-05 |
| Model-TimeModelFit      | 27.1           |
| ModelSampler-n_times... | 840000         |
| Policy-AverageAbsPol... | 0.62375194     |
| Policy-AverageDiscou... | 98.6           |
| Policy-AveragePolicyStd | 0.6778564      |
| Policy-AverageReturn    | 226            |
| Policy-MaxReturn        | 255            |
| Policy-MinReturn        | 160            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 21             |
| Policy-TimeAlgoOpt      | 0.496          |
| Policy-TimeSampleProc   | 0.575          |
| Policy-TimeSampling     | 1.37           |
| Policy-TimeStep         | 2.5            |
| Time                    | 483            |
| n_timesteps             | 21000          |
--------------------------------------------

 ---------------- Iteration 21 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 210 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 211 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 212 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 213 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 214 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 215 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 216 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 217 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 218 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 219 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.434          |
| Data-EnvSampler-Poli... | 0.692          |
| Data-EnvTrajs-Averag... | 148            |
| Data-EnvTrajs-MaxReturn | 169            |
| Data-EnvTrajs-MinReturn | 108            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 23.6           |
| Data-TimeEnvSampleProc  | 0.00104        |
| Data-TimeEnvSampling    | 1.16           |
| Iteration               | 21             |
| ItrTime                 | 31.3           |
| LossAfter               | -0.0044490285  |
| LossBefore              | -1.0214697e-05 |
| Model-TimeModelFit      | 27.8           |
| ModelSampler-n_times... | 880000         |
| Policy-AverageAbsPol... | 0.59477466     |
| Policy-AverageDiscou... | -8.08e+03      |
| Policy-AveragePolicyStd | 0.6728712      |
| Policy-AverageReturn    | -5.02e+04      |
| Policy-MaxReturn        | -237           |
| Policy-MinReturn        | -9.96e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 2.17e+05       |
| Policy-TimeAlgoOpt      | 0.556          |
| Policy-TimeSampleProc   | 0.302          |
| Policy-TimeSampling     | 1.45           |
| Policy-TimeStep         | 2.33           |
| Time                    | 514            |
| n_timesteps             | 22000          |
--------------------------------------------

 ---------------- Iteration 22 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 220 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 221 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 222 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 223 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 224 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 225 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 226 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 227 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 228 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 229 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.488         |
| Data-EnvSampler-Poli... | 0.804         |
| Data-EnvTrajs-Averag... | -27.8         |
| Data-EnvTrajs-MaxReturn | 4.05          |
| Data-EnvTrajs-MinReturn | -57.3         |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 23.4          |
| Data-TimeEnvSampleProc  | 0.00109       |
| Data-TimeEnvSampling    | 1.33          |
| Iteration               | 22            |
| ItrTime                 | 7.18          |
| LossAfter               | -0.0056378306 |
| LossBefore              | -1.019433e-05 |
| Model-TimeModelFit      | 3.22          |
| ModelSampler-n_times... | 920000        |
| Policy-AverageAbsPol... | 1.1367373     |
| Policy-AverageDiscou... | -3.21e+05     |
| Policy-AveragePolicyStd | 0.6703385     |
| Policy-AverageReturn    | -1.63e+06     |
| Policy-MaxReturn        | -43.7         |
| Policy-MinReturn        | -2.51e+06     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 8.59e+05      |
| Policy-TimeAlgoOpt      | 0.612         |
| Policy-TimeSampleProc   | 0.37          |
| Policy-TimeSampling     | 1.61          |
| Policy-TimeStep         | 2.63          |
| Time                    | 521           |
| n_timesteps             | 23000         |
-------------------------------------------

 ---------------- Iteration 23 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 230 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 231 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 232 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 233 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 234 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 235 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 236 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 237 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 238 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 239 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.501          |
| Data-EnvSampler-Poli... | 0.846          |
| Data-EnvTrajs-Averag... | -29.3          |
| Data-EnvTrajs-MaxReturn | 22.2           |
| Data-EnvTrajs-MinReturn | -69.4          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 34.5           |
| Data-TimeEnvSampleProc  | 0.00111        |
| Data-TimeEnvSampling    | 1.39           |
| Iteration               | 23             |
| ItrTime                 | 31.8           |
| LossAfter               | -0.0071487874  |
| LossBefore              | -1.0198426e-05 |
| Model-TimeModelFit      | 28             |
| ModelSampler-n_times... | 960000         |
| Policy-AverageAbsPol... | 0.6491766      |
| Policy-AverageDiscou... | 34.2           |
| Policy-AveragePolicyStd | 0.67141676     |
| Policy-AverageReturn    | -140           |
| Policy-MaxReturn        | 113            |
| Policy-MinReturn        | -236           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 69.3           |
| Policy-TimeAlgoOpt      | 0.566          |
| Policy-TimeSampleProc   | 0.379          |
| Policy-TimeSampling     | 1.51           |
| Policy-TimeStep         | 2.5            |
| Time                    | 553            |
| n_timesteps             | 24000          |
--------------------------------------------

 ---------------- Iteration 24 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 240 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 241 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 242 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 243 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 244 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 245 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 246 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 247 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 248 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 249 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.458          |
| Data-EnvSampler-Poli... | 0.873          |
| Data-EnvTrajs-Averag... | 25.1           |
| Data-EnvTrajs-MaxReturn | 90.3           |
| Data-EnvTrajs-MinReturn | -32.7          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 43.4           |
| Data-TimeEnvSampleProc  | 0.0011         |
| Data-TimeEnvSampling    | 1.37           |
| Iteration               | 24             |
| ItrTime                 | 31.6           |
| LossAfter               | -0.004556743   |
| LossBefore              | -1.0148805e-05 |
| Model-TimeModelFit      | 27.5           |
| ModelSampler-n_times... | 1000000        |
| Policy-AverageAbsPol... | 0.47875544     |
| Policy-AverageDiscou... | -1.28e+03      |
| Policy-AveragePolicyStd | 0.6684648      |
| Policy-AverageReturn    | -9.65e+03      |
| Policy-MaxReturn        | 236            |
| Policy-MinReturn        | -1.96e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 4.29e+04       |
| Policy-TimeAlgoOpt      | 0.572          |
| Policy-TimeSampleProc   | 0.491          |
| Policy-TimeSampling     | 1.69           |
| Policy-TimeStep         | 2.8            |
| Time                    | 585            |
| n_timesteps             | 25000          |
--------------------------------------------

 ---------------- Iteration 25 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 250 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 251 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 252 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 253 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 254 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 255 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 256 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 257 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 258 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 259 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.47           |
| Data-EnvSampler-Poli... | 0.852          |
| Data-EnvTrajs-Averag... | 175            |
| Data-EnvTrajs-MaxReturn | 236            |
| Data-EnvTrajs-MinReturn | 110            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 49.6           |
| Data-TimeEnvSampleProc  | 0.00104        |
| Data-TimeEnvSampling    | 1.36           |
| Iteration               | 25             |
| ItrTime                 | 32             |
| LossAfter               | -0.002953479   |
| LossBefore              | -9.8836035e-06 |
| Model-TimeModelFit      | 28             |
| ModelSampler-n_times... | 1040000        |
| Policy-AverageAbsPol... | 0.4849209      |
| Policy-AverageDiscou... | 94.8           |
| Policy-AveragePolicyStd | 0.64870137     |
| Policy-AverageReturn    | 191            |
| Policy-MaxReturn        | 217            |
| Policy-MinReturn        | 155            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 17.9           |
| Policy-TimeAlgoOpt      | 0.54           |
| Policy-TimeSampleProc   | 0.352          |
| Policy-TimeSampling     | 1.69           |
| Policy-TimeStep         | 2.63           |
| Time                    | 617            |
| n_timesteps             | 26000          |
--------------------------------------------

 ---------------- Iteration 26 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 260 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 261 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 262 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 263 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 264 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 265 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 266 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 267 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 268 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 269 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.463         |
| Data-EnvSampler-Poli... | 0.875         |
| Data-EnvTrajs-Averag... | 280           |
| Data-EnvTrajs-MaxReturn | 300           |
| Data-EnvTrajs-MinReturn | 264           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 12            |
| Data-TimeEnvSampleProc  | 0.000953      |
| Data-TimeEnvSampling    | 1.38          |
| Iteration               | 26            |
| ItrTime                 | 18.1          |
| LossAfter               | -0.007263777  |
| LossBefore              | -9.579668e-06 |
| Model-TimeModelFit      | 14.2          |
| ModelSampler-n_times... | 1080000       |
| Policy-AverageAbsPol... | 0.58593935    |
| Policy-AverageDiscou... | -1.92e+04     |
| Policy-AveragePolicyStd | 0.63038284    |
| Policy-AverageReturn    | -1.27e+05     |
| Policy-MaxReturn        | 231           |
| Policy-MinReturn        | -7.74e+05     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 2.61e+05      |
| Policy-TimeAlgoOpt      | 0.56          |
| Policy-TimeSampleProc   | 0.406         |
| Policy-TimeSampling     | 1.43          |
| Policy-TimeStep         | 2.46          |
| Time                    | 635           |
| n_timesteps             | 27000         |
-------------------------------------------

 ---------------- Iteration 27 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 270 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 271 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 272 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 273 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 274 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 275 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 276 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 277 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 278 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 279 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.503         |
| Data-EnvSampler-Poli... | 0.872         |
| Data-EnvTrajs-Averag... | 128           |
| Data-EnvTrajs-MaxReturn | 163           |
| Data-EnvTrajs-MinReturn | 104           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 20.5          |
| Data-TimeEnvSampleProc  | 0.000917      |
| Data-TimeEnvSampling    | 1.41          |
| Iteration               | 27            |
| ItrTime                 | 31.6          |
| LossAfter               | -0.008254081  |
| LossBefore              | -9.510257e-06 |
| Model-TimeModelFit      | 27.6          |
| ModelSampler-n_times... | 1120000       |
| Policy-AverageAbsPol... | 0.58912474    |
| Policy-AverageDiscou... | 79.5          |
| Policy-AveragePolicyStd | 0.6266872     |
| Policy-AverageReturn    | 231           |
| Policy-MaxReturn        | 300           |
| Policy-MinReturn        | 177           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 37.9          |
| Policy-TimeAlgoOpt      | 0.595         |
| Policy-TimeSampleProc   | 0.421         |
| Policy-TimeSampling     | 1.47          |
| Policy-TimeStep         | 2.55          |
| Time                    | 667           |
| n_timesteps             | 28000         |
-------------------------------------------

 ---------------- Iteration 28 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 280 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 281 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 282 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 283 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 284 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 285 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 286 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 287 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 288 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 289 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.542         |
| Data-EnvSampler-Poli... | 0.916         |
| Data-EnvTrajs-Averag... | 173           |
| Data-EnvTrajs-MaxReturn | 196           |
| Data-EnvTrajs-MinReturn | 150           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 14.5          |
| Data-TimeEnvSampleProc  | 0.00122       |
| Data-TimeEnvSampling    | 1.5           |
| Iteration               | 28            |
| ItrTime                 | 31.8          |
| LossAfter               | -0.004843831  |
| LossBefore              | -9.300074e-06 |
| Model-TimeModelFit      | 27.9          |
| ModelSampler-n_times... | 1160000       |
| Policy-AverageAbsPol... | 0.6186556     |
| Policy-AverageDiscou... | -3.6e+03      |
| Policy-AveragePolicyStd | 0.6152802     |
| Policy-AverageReturn    | -2.48e+04     |
| Policy-MaxReturn        | 312           |
| Policy-MinReturn        | -5.01e+05     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 1.09e+05      |
| Policy-TimeAlgoOpt      | 0.556         |
| Policy-TimeSampleProc   | 0.365         |
| Policy-TimeSampling     | 1.5           |
| Policy-TimeStep         | 2.45          |
| Time                    | 698           |
| n_timesteps             | 29000         |
-------------------------------------------

 ---------------- Iteration 29 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 290 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 291 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 292 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 293 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 294 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 295 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 296 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 297 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 298 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 299 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.575         |
| Data-EnvSampler-Poli... | 0.939         |
| Data-EnvTrajs-Averag... | 98.5          |
| Data-EnvTrajs-MaxReturn | 143           |
| Data-EnvTrajs-MinReturn | 52.2          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 38.1          |
| Data-TimeEnvSampleProc  | 0.000963      |
| Data-TimeEnvSampling    | 1.57          |
| Iteration               | 29            |
| ItrTime                 | 32.2          |
| LossAfter               | -0.008780812  |
| LossBefore              | -9.283734e-06 |
| Model-TimeModelFit      | 28.3          |
| ModelSampler-n_times... | 1200000       |
| Policy-AverageAbsPol... | 0.4605498     |
| Policy-AverageDiscou... | 93.2          |
| Policy-AveragePolicyStd | 0.61163145    |
| Policy-AverageReturn    | 222           |
| Policy-MaxReturn        | 243           |
| Policy-MinReturn        | 191           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 11.3          |
| Policy-TimeAlgoOpt      | 0.655         |
| Policy-TimeSampleProc   | 0.273         |
| Policy-TimeSampling     | 1.44          |
| Policy-TimeStep         | 2.41          |
| Time                    | 731           |
| n_timesteps             | 30000         |
-------------------------------------------

 ---------------- Iteration 30 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 300 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 301 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 302 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 303 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 304 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 305 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 306 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 307 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 308 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 309 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.558         |
| Data-EnvSampler-Poli... | 0.972         |
| Data-EnvTrajs-Averag... | 263           |
| Data-EnvTrajs-MaxReturn | 268           |
| Data-EnvTrajs-MinReturn | 251           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.13          |
| Data-TimeEnvSampleProc  | 0.000719      |
| Data-TimeEnvSampling    | 1.57          |
| Iteration               | 30            |
| ItrTime                 | 32.4          |
| LossAfter               | -0.006171322  |
| LossBefore              | -8.754462e-06 |
| Model-TimeModelFit      | 28.4          |
| ModelSampler-n_times... | 1240000       |
| Policy-AverageAbsPol... | 0.45815805    |
| Policy-AverageDiscou... | 101           |
| Policy-AveragePolicyStd | 0.58114594    |
| Policy-AverageReturn    | 205           |
| Policy-MaxReturn        | 224           |
| Policy-MinReturn        | 165           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 12.8          |
| Policy-TimeAlgoOpt      | 0.582         |
| Policy-TimeSampleProc   | 0.286         |
| Policy-TimeSampling     | 1.54          |
| Policy-TimeStep         | 2.43          |
| Time                    | 763           |
| n_timesteps             | 31000         |
-------------------------------------------

 ---------------- Iteration 31 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 310 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 311 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 312 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 313 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 314 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 315 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 316 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 317 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 318 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 319 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.521         |
| Data-EnvSampler-Poli... | 0.948         |
| Data-EnvTrajs-Averag... | 278           |
| Data-EnvTrajs-MaxReturn | 319           |
| Data-EnvTrajs-MinReturn | 222           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 33.2          |
| Data-TimeEnvSampleProc  | 0.00302       |
| Data-TimeEnvSampling    | 1.51          |
| Iteration               | 31            |
| ItrTime                 | 27.7          |
| LossAfter               | -0.002991928  |
| LossBefore              | -8.456881e-06 |
| Model-TimeModelFit      | 23.7          |
| ModelSampler-n_times... | 1280000       |
| Policy-AverageAbsPol... | 0.6257692     |
| Policy-AverageDiscou... | -1.87e+04     |
| Policy-AveragePolicyStd | 0.56418204    |
| Policy-AverageReturn    | -1.2e+05      |
| Policy-MaxReturn        | 293           |
| Policy-MinReturn        | -1.23e+06     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 2.9e+05       |
| Policy-TimeAlgoOpt      | 0.599         |
| Policy-TimeSampleProc   | 0.349         |
| Policy-TimeSampling     | 1.45          |
| Policy-TimeStep         | 2.44          |
| Time                    | 791           |
| n_timesteps             | 32000         |
-------------------------------------------

 ---------------- Iteration 32 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 320 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 321 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 322 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 323 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 324 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 325 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 326 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 327 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 328 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 329 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.503         |
| Data-EnvSampler-Poli... | 0.832         |
| Data-EnvTrajs-Averag... | 249           |
| Data-EnvTrajs-MaxReturn | 256           |
| Data-EnvTrajs-MinReturn | 238           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.08          |
| Data-TimeEnvSampleProc  | 0.000599      |
| Data-TimeEnvSampling    | 1.37          |
| Iteration               | 32            |
| ItrTime                 | 32.2          |
| LossAfter               | -0.004132786  |
| LossBefore              | -8.200643e-06 |
| Model-TimeModelFit      | 28.4          |
| ModelSampler-n_times... | 1320000       |
| Policy-AverageAbsPol... | 0.42562068    |
| Policy-AverageDiscou... | 104           |
| Policy-AveragePolicyStd | 0.5503045     |
| Policy-AverageReturn    | 226           |
| Policy-MaxReturn        | 237           |
| Policy-MinReturn        | 215           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 6.19          |
| Policy-TimeAlgoOpt      | 0.564         |
| Policy-TimeSampleProc   | 0.303         |
| Policy-TimeSampling     | 1.48          |
| Policy-TimeStep         | 2.4           |
| Time                    | 823           |
| n_timesteps             | 33000         |
-------------------------------------------

 ---------------- Iteration 33 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 330 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 331 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 332 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 333 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 334 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 335 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 336 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 337 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 338 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 339 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.466         |
| Data-EnvSampler-Poli... | 0.86          |
| Data-EnvTrajs-Averag... | 284           |
| Data-EnvTrajs-MaxReturn | 288           |
| Data-EnvTrajs-MinReturn | 277           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 4.09          |
| Data-TimeEnvSampleProc  | 0.000992      |
| Data-TimeEnvSampling    | 1.37          |
| Iteration               | 33            |
| ItrTime                 | 33.3          |
| LossAfter               | -0.0043804264 |
| LossBefore              | -7.834507e-06 |
| Model-TimeModelFit      | 29.6          |
| ModelSampler-n_times... | 1360000       |
| Policy-AverageAbsPol... | 0.33958578    |
| Policy-AverageDiscou... | 118           |
| Policy-AveragePolicyStd | 0.5294784     |
| Policy-AverageReturn    | 244           |
| Policy-MaxReturn        | 267           |
| Policy-MinReturn        | 127           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 28            |
| Policy-TimeAlgoOpt      | 0.547         |
| Policy-TimeSampleProc   | 0.274         |
| Policy-TimeSampling     | 1.43          |
| Policy-TimeStep         | 2.29          |
| Time                    | 856           |
| n_timesteps             | 34000         |
-------------------------------------------

 ---------------- Iteration 34 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 340 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 341 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 342 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 343 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 344 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 345 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 346 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 347 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 348 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 349 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.431         |
| Data-EnvSampler-Poli... | 0.81          |
| Data-EnvTrajs-Averag... | 164           |
| Data-EnvTrajs-MaxReturn | 231           |
| Data-EnvTrajs-MinReturn | 68.6          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 74.8          |
| Data-TimeEnvSampleProc  | 0.000969      |
| Data-TimeEnvSampling    | 1.29          |
| Iteration               | 34            |
| ItrTime                 | 7.5           |
| LossAfter               | -0.004279825  |
| LossBefore              | -7.667529e-06 |
| Model-TimeModelFit      | 3.55          |
| ModelSampler-n_times... | 1400000       |
| Policy-AverageAbsPol... | 0.44342196    |
| Policy-AverageDiscou... | -5.56e+04     |
| Policy-AveragePolicyStd | 0.5201937     |
| Policy-AverageReturn    | -2.29e+05     |
| Policy-MaxReturn        | 300           |
| Policy-MinReturn        | -3.58e+06     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 7.99e+05      |
| Policy-TimeAlgoOpt      | 0.578         |
| Policy-TimeSampleProc   | 0.259         |
| Policy-TimeSampling     | 1.78          |
| Policy-TimeStep         | 2.66          |
| Time                    | 864           |
| n_timesteps             | 35000         |
-------------------------------------------

 ---------------- Iteration 35 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 350 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 351 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 352 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 353 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 354 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 355 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 356 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 357 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 358 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 359 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.436         |
| Data-EnvSampler-Poli... | 0.666         |
| Data-EnvTrajs-Averag... | 272           |
| Data-EnvTrajs-MaxReturn | 297           |
| Data-EnvTrajs-MinReturn | 236           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 19.9          |
| Data-TimeEnvSampleProc  | 0.00104       |
| Data-TimeEnvSampling    | 1.13          |
| Iteration               | 35            |
| ItrTime                 | 33            |
| LossAfter               | -0.0037327905 |
| LossBefore              | -7.368312e-06 |
| Model-TimeModelFit      | 29.3          |
| ModelSampler-n_times... | 1440000       |
| Policy-AverageAbsPol... | 0.43817827    |
| Policy-AverageDiscou... | 91.2          |
| Policy-AveragePolicyStd | 0.5058538     |
| Policy-AverageReturn    | 182           |
| Policy-MaxReturn        | 242           |
| Policy-MinReturn        | 48.9          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 49.3          |
| Policy-TimeAlgoOpt      | 0.573         |
| Policy-TimeSampleProc   | 0.434         |
| Policy-TimeSampling     | 1.53          |
| Policy-TimeStep         | 2.56          |
| Time                    | 897           |
| n_timesteps             | 36000         |
-------------------------------------------

 ---------------- Iteration 36 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 360 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 361 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 362 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 363 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 364 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 365 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 366 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 367 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 368 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 369 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.449         |
| Data-EnvSampler-Poli... | 0.771         |
| Data-EnvTrajs-Averag... | 265           |
| Data-EnvTrajs-MaxReturn | 279           |
| Data-EnvTrajs-MinReturn | 243           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 13.5          |
| Data-TimeEnvSampleProc  | 0.00108       |
| Data-TimeEnvSampling    | 1.26          |
| Iteration               | 36            |
| ItrTime                 | 34.4          |
| LossAfter               | -0.0018934376 |
| LossBefore              | -7.064015e-06 |
| Model-TimeModelFit      | 30.5          |
| ModelSampler-n_times... | 1480000       |
| Policy-AverageAbsPol... | 0.86111253    |
| Policy-AverageDiscou... | -6.84e+05     |
| Policy-AveragePolicyStd | 0.49055648    |
| Policy-AverageReturn    | -2.43e+06     |
| Policy-MaxReturn        | 221           |
| Policy-MinReturn        | -4.42e+06     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 1.84e+06      |
| Policy-TimeAlgoOpt      | 0.552         |
| Policy-TimeSampleProc   | 0.326         |
| Policy-TimeSampling     | 1.71          |
| Policy-TimeStep         | 2.61          |
| Time                    | 931           |
| n_timesteps             | 37000         |
-------------------------------------------

 ---------------- Iteration 37 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 370 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 371 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 372 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 373 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 374 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 375 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 376 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 377 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 378 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 379 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.45           |
| Data-EnvSampler-Poli... | 0.695          |
| Data-EnvTrajs-Averag... | 217            |
| Data-EnvTrajs-MaxReturn | 229            |
| Data-EnvTrajs-MinReturn | 207            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 7.52           |
| Data-TimeEnvSampleProc  | 0.00297        |
| Data-TimeEnvSampling    | 1.18           |
| Iteration               | 37             |
| ItrTime                 | 34.2           |
| LossAfter               | -0.00436059    |
| LossBefore              | -7.0547458e-06 |
| Model-TimeModelFit      | 30.4           |
| ModelSampler-n_times... | 1520000        |
| Policy-AverageAbsPol... | 0.6548224      |
| Policy-AverageDiscou... | -1.08e+05      |
| Policy-AveragePolicyStd | 0.4903178      |
| Policy-AverageReturn    | -5.93e+05      |
| Policy-MaxReturn        | 298            |
| Policy-MinReturn        | -2.18e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 7.8e+05        |
| Policy-TimeAlgoOpt      | 0.542          |
| Policy-TimeSampleProc   | 0.456          |
| Policy-TimeSampling     | 1.61           |
| Policy-TimeStep         | 2.65           |
| Time                    | 965            |
| n_timesteps             | 38000          |
--------------------------------------------

 ---------------- Iteration 38 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 380 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 381 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 382 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 383 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 384 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 385 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 386 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 387 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 388 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 389 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.577         |
| Data-EnvSampler-Poli... | 0.847         |
| Data-EnvTrajs-Averag... | 214           |
| Data-EnvTrajs-MaxReturn | 225           |
| Data-EnvTrajs-MinReturn | 209           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 5.65          |
| Data-TimeEnvSampleProc  | 0.00112       |
| Data-TimeEnvSampling    | 1.46          |
| Iteration               | 38            |
| ItrTime                 | 34.4          |
| LossAfter               | -0.0046505607 |
| LossBefore              | -7.034408e-06 |
| Model-TimeModelFit      | 30.5          |
| ModelSampler-n_times... | 1560000       |
| Policy-AverageAbsPol... | 0.4190955     |
| Policy-AverageDiscou... | -5.43e+03     |
| Policy-AveragePolicyStd | 0.48962113    |
| Policy-AverageReturn    | -3.73e+04     |
| Policy-MaxReturn        | -660          |
| Policy-MinReturn        | -3.67e+05     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 1.08e+05      |
| Policy-TimeAlgoOpt      | 0.547         |
| Policy-TimeSampleProc   | 0.343         |
| Policy-TimeSampling     | 1.51          |
| Policy-TimeStep         | 2.43          |
| Time                    | 1e+03         |
| n_timesteps             | 39000         |
-------------------------------------------

 ---------------- Iteration 39 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 390 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 391 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 392 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 393 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 394 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 395 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 396 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 397 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 398 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 399 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.552          |
| Data-EnvSampler-Poli... | 0.897          |
| Data-EnvTrajs-Averag... | 225            |
| Data-EnvTrajs-MaxReturn | 252            |
| Data-EnvTrajs-MinReturn | 207            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 15.7           |
| Data-TimeEnvSampleProc  | 0.00153        |
| Data-TimeEnvSampling    | 1.49           |
| Iteration               | 39             |
| ItrTime                 | 33.7           |
| LossAfter               | -0.010097844   |
| LossBefore              | -6.9003677e-06 |
| Model-TimeModelFit      | 29.9           |
| ModelSampler-n_times... | 1600000        |
| Policy-AverageAbsPol... | 0.4540719      |
| Policy-AverageDiscou... | -1.85e+04      |
| Policy-AveragePolicyStd | 0.48280093     |
| Policy-AverageReturn    | -1.07e+05      |
| Policy-MaxReturn        | 300            |
| Policy-MinReturn        | -1.65e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 3.7e+05        |
| Policy-TimeAlgoOpt      | 0.61           |
| Policy-TimeSampleProc   | 0.269          |
| Policy-TimeSampling     | 1.46           |
| Policy-TimeStep         | 2.36           |
| Time                    | 1.03e+03       |
| n_timesteps             | 40000          |
--------------------------------------------

 ---------------- Iteration 40 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 400 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 401 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 402 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 403 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 404 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 405 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 406 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 407 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 408 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 409 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.526         |
| Data-EnvSampler-Poli... | 0.904         |
| Data-EnvTrajs-Averag... | 248           |
| Data-EnvTrajs-MaxReturn | 280           |
| Data-EnvTrajs-MinReturn | 215           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 25.9          |
| Data-TimeEnvSampleProc  | 0.000534      |
| Data-TimeEnvSampling    | 1.48          |
| Iteration               | 40            |
| ItrTime                 | 33.7          |
| LossAfter               | -0.008143793  |
| LossBefore              | -6.745525e-06 |
| Model-TimeModelFit      | 29.9          |
| ModelSampler-n_times... | 1640000       |
| Policy-AverageAbsPol... | 0.49473998    |
| Policy-AverageDiscou... | 84.1          |
| Policy-AveragePolicyStd | 0.47531185    |
| Policy-AverageReturn    | 199           |
| Policy-MaxReturn        | 272           |
| Policy-MinReturn        | 164           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 25.9          |
| Policy-TimeAlgoOpt      | 0.542         |
| Policy-TimeSampleProc   | 0.367         |
| Policy-TimeSampling     | 1.41          |
| Policy-TimeStep         | 2.37          |
| Time                    | 1.07e+03      |
| n_timesteps             | 41000         |
-------------------------------------------

 ---------------- Iteration 41 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 410 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 411 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 412 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 413 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 414 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 415 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 416 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 417 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 418 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 419 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.515          |
| Data-EnvSampler-Poli... | 0.929          |
| Data-EnvTrajs-Averag... | 229            |
| Data-EnvTrajs-MaxReturn | 245            |
| Data-EnvTrajs-MinReturn | 214            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 10.2           |
| Data-TimeEnvSampleProc  | 0.00076        |
| Data-TimeEnvSampling    | 1.48           |
| Iteration               | 41             |
| ItrTime                 | 34.6           |
| LossAfter               | -0.003926138   |
| LossBefore              | -6.6425305e-06 |
| Model-TimeModelFit      | 30.7           |
| ModelSampler-n_times... | 1680000        |
| Policy-AverageAbsPol... | 0.47538817     |
| Policy-AverageDiscou... | 109            |
| Policy-AveragePolicyStd | 0.4701019      |
| Policy-AverageReturn    | 257            |
| Policy-MaxReturn        | 299            |
| Policy-MinReturn        | 144            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 41.3           |
| Policy-TimeAlgoOpt      | 0.538          |
| Policy-TimeSampleProc   | 0.334          |
| Policy-TimeSampling     | 1.49           |
| Policy-TimeStep         | 2.4            |
| Time                    | 1.1e+03        |
| n_timesteps             | 42000          |
--------------------------------------------

 ---------------- Iteration 42 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 420 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 421 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 422 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 423 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 424 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 425 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 426 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 427 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 428 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 429 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.532         |
| Data-EnvSampler-Poli... | 0.87          |
| Data-EnvTrajs-Averag... | 221           |
| Data-EnvTrajs-MaxReturn | 243           |
| Data-EnvTrajs-MinReturn | 203           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 14.2          |
| Data-TimeEnvSampleProc  | 0.00136       |
| Data-TimeEnvSampling    | 1.44          |
| Iteration               | 42            |
| ItrTime                 | 34.4          |
| LossAfter               | -0.0063672806 |
| LossBefore              | -6.636422e-06 |
| Model-TimeModelFit      | 30.6          |
| ModelSampler-n_times... | 1720000       |
| Policy-AverageAbsPol... | 0.4230145     |
| Policy-AverageDiscou... | 120           |
| Policy-AveragePolicyStd | 0.4693991     |
| Policy-AverageReturn    | 309           |
| Policy-MaxReturn        | 368           |
| Policy-MinReturn        | 255           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 30.2          |
| Policy-TimeAlgoOpt      | 0.59          |
| Policy-TimeSampleProc   | 0.281         |
| Policy-TimeSampling     | 1.43          |
| Policy-TimeStep         | 2.33          |
| Time                    | 1.14e+03      |
| n_timesteps             | 43000         |
-------------------------------------------

 ---------------- Iteration 43 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 430 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 431 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 432 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 433 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 434 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 435 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 436 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 437 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 438 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 439 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.571         |
| Data-EnvSampler-Poli... | 0.885         |
| Data-EnvTrajs-Averag... | 122           |
| Data-EnvTrajs-MaxReturn | 165           |
| Data-EnvTrajs-MinReturn | 44.6          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 41.4          |
| Data-TimeEnvSampleProc  | 0.000992      |
| Data-TimeEnvSampling    | 1.51          |
| Iteration               | 43            |
| ItrTime                 | 34.4          |
| LossAfter               | -0.0039225593 |
| LossBefore              | -6.469423e-06 |
| Model-TimeModelFit      | 30.3          |
| ModelSampler-n_times... | 1760000       |
| Policy-AverageAbsPol... | 0.40341642    |
| Policy-AverageDiscou... | 115           |
| Policy-AveragePolicyStd | 0.4623483     |
| Policy-AverageReturn    | 264           |
| Policy-MaxReturn        | 278           |
| Policy-MinReturn        | 232           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 12            |
| Policy-TimeAlgoOpt      | 0.614         |
| Policy-TimeSampleProc   | 0.268         |
| Policy-TimeSampling     | 1.7           |
| Policy-TimeStep         | 2.6           |
| Time                    | 1.17e+03      |
| n_timesteps             | 44000         |
-------------------------------------------

 ---------------- Iteration 44 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 440 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 441 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 442 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 443 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 444 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 445 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 446 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 447 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 448 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 449 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.584          |
| Data-EnvSampler-Poli... | 1.03           |
| Data-EnvTrajs-Averag... | 270            |
| Data-EnvTrajs-MaxReturn | 283            |
| Data-EnvTrajs-MinReturn | 246            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 13.3           |
| Data-TimeEnvSampleProc  | 0.00105        |
| Data-TimeEnvSampling    | 1.66           |
| Iteration               | 44             |
| ItrTime                 | 35.8           |
| LossAfter               | -0.0053272815  |
| LossBefore              | -6.3137877e-06 |
| Model-TimeModelFit      | 31.5           |
| ModelSampler-n_times... | 1800000        |
| Policy-AverageAbsPol... | 0.44322714     |
| Policy-AverageDiscou... | -2.52e+04      |
| Policy-AveragePolicyStd | 0.4569169      |
| Policy-AverageReturn    | -1.2e+05       |
| Policy-MaxReturn        | 278            |
| Policy-MinReturn        | -2.41e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 5.25e+05       |
| Policy-TimeAlgoOpt      | 0.597          |
| Policy-TimeSampleProc   | 0.416          |
| Policy-TimeSampling     | 1.57           |
| Policy-TimeStep         | 2.61           |
| Time                    | 1.21e+03       |
| n_timesteps             | 45000          |
--------------------------------------------

 ---------------- Iteration 45 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 450 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 451 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 452 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 453 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 454 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 455 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 456 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 457 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 458 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 459 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.605          |
| Data-EnvSampler-Poli... | 0.87           |
| Data-EnvTrajs-Averag... | 254            |
| Data-EnvTrajs-MaxReturn | 268            |
| Data-EnvTrajs-MinReturn | 241            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 9.05           |
| Data-TimeEnvSampleProc  | 0.00106        |
| Data-TimeEnvSampling    | 1.51           |
| Iteration               | 45             |
| ItrTime                 | 38             |
| LossAfter               | -0.008577089   |
| LossBefore              | -6.2432673e-06 |
| Model-TimeModelFit      | 33.7           |
| ModelSampler-n_times... | 1840000        |
| Policy-AverageAbsPol... | 0.37512872     |
| Policy-AverageDiscou... | -1.38e+04      |
| Policy-AveragePolicyStd | 0.45253012     |
| Policy-AverageReturn    | -7.76e+04      |
| Policy-MaxReturn        | 276            |
| Policy-MinReturn        | -1.56e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 3.39e+05       |
| Policy-TimeAlgoOpt      | 0.596          |
| Policy-TimeSampleProc   | 0.433          |
| Policy-TimeSampling     | 1.67           |
| Policy-TimeStep         | 2.76           |
| Time                    | 1.24e+03       |
| n_timesteps             | 46000          |
--------------------------------------------

 ---------------- Iteration 46 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 460 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 461 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 462 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 463 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 464 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 465 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 466 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 467 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 468 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 469 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.529          |
| Data-EnvSampler-Poli... | 0.975          |
| Data-EnvTrajs-Averag... | 128            |
| Data-EnvTrajs-MaxReturn | 211            |
| Data-EnvTrajs-MinReturn | 75.3           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 48.8           |
| Data-TimeEnvSampleProc  | 0.000688       |
| Data-TimeEnvSampling    | 1.54           |
| Iteration               | 46             |
| ItrTime                 | 34.6           |
| LossAfter               | -0.010956142   |
| LossBefore              | -5.9641584e-06 |
| Model-TimeModelFit      | 30.4           |
| ModelSampler-n_times... | 1880000        |
| Policy-AverageAbsPol... | 0.2758492      |
| Policy-AverageDiscou... | 116            |
| Policy-AveragePolicyStd | 0.4403925      |
| Policy-AverageReturn    | 268            |
| Policy-MaxReturn        | 284            |
| Policy-MinReturn        | 251            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 9.06           |
| Policy-TimeAlgoOpt      | 0.644          |
| Policy-TimeSampleProc   | 0.32           |
| Policy-TimeSampling     | 1.63           |
| Policy-TimeStep         | 2.62           |
| Time                    | 1.28e+03       |
| n_timesteps             | 47000          |
--------------------------------------------

 ---------------- Iteration 47 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 470 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 471 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 472 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 473 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 474 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 475 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 476 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 477 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 478 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 479 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.504        |
| Data-EnvSampler-Poli... | 0.827        |
| Data-EnvTrajs-Averag... | 47.4         |
| Data-EnvTrajs-MaxReturn | 126          |
| Data-EnvTrajs-MinReturn | -27.5        |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 63.9         |
| Data-TimeEnvSampleProc  | 0.000895     |
| Data-TimeEnvSampling    | 1.38         |
| Iteration               | 47           |
| ItrTime                 | 35.3         |
| LossAfter               | -0.007393732 |
| LossBefore              | -5.86923e-06 |
| Model-TimeModelFit      | 31.2         |
| ModelSampler-n_times... | 1920000      |
| Policy-AverageAbsPol... | 0.39571443   |
| Policy-AverageDiscou... | -1.83e+05    |
| Policy-AveragePolicyStd | 0.43603006   |
| Policy-AverageReturn    | -6.87e+05    |
| Policy-MaxReturn        | 301          |
| Policy-MinReturn        | -4.13e+06    |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 1.4e+06      |
| Policy-TimeAlgoOpt      | 0.568        |
| Policy-TimeSampleProc   | 0.47         |
| Policy-TimeSampling     | 1.61         |
| Policy-TimeStep         | 2.7          |
| Time                    | 1.31e+03     |
| n_timesteps             | 48000        |
------------------------------------------

 ---------------- Iteration 48 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 480 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 481 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 482 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 483 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 484 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 485 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 486 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 487 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 488 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 489 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.51           |
| Data-EnvSampler-Poli... | 0.863          |
| Data-EnvTrajs-Averag... | 233            |
| Data-EnvTrajs-MaxReturn | 268            |
| Data-EnvTrajs-MinReturn | 173            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 32.7           |
| Data-TimeEnvSampleProc  | 0.000858       |
| Data-TimeEnvSampling    | 1.41           |
| Iteration               | 48             |
| ItrTime                 | 35.4           |
| LossAfter               | -0.0070786285  |
| LossBefore              | -5.7507687e-06 |
| Model-TimeModelFit      | 31.2           |
| ModelSampler-n_times... | 1960000        |
| Policy-AverageAbsPol... | 0.28601542     |
| Policy-AverageDiscou... | 121            |
| Policy-AveragePolicyStd | 0.43086645     |
| Policy-AverageReturn    | 285            |
| Policy-MaxReturn        | 298            |
| Policy-MinReturn        | 269            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 6.93           |
| Policy-TimeAlgoOpt      | 0.596          |
| Policy-TimeSampleProc   | 0.526          |
| Policy-TimeSampling     | 1.66           |
| Policy-TimeStep         | 2.8            |
| Time                    | 1.35e+03       |
| n_timesteps             | 49000          |
--------------------------------------------

 ---------------- Iteration 49 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 490 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 491 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 492 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 493 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 494 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 495 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 496 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 497 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 498 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 499 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.51          |
| Data-EnvSampler-Poli... | 0.867         |
| Data-EnvTrajs-Averag... | 217           |
| Data-EnvTrajs-MaxReturn | 266           |
| Data-EnvTrajs-MinReturn | 178           |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 31.3          |
| Data-TimeEnvSampleProc  | 0.00073       |
| Data-TimeEnvSampling    | 1.42          |
| Iteration               | 49            |
| ItrTime                 | 36.2          |
| LossAfter               | -0.0031895651 |
| LossBefore              | -5.339444e-06 |
| Model-TimeModelFit      | 32.1          |
| ModelSampler-n_times... | 2000000       |
| Policy-AverageAbsPol... | 0.4097355     |
| Policy-AverageDiscou... | 116           |
| Policy-AveragePolicyStd | 0.41302443    |
| Policy-AverageReturn    | 254           |
| Policy-MaxReturn        | 287           |
| Policy-MinReturn        | 145           |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 32.8          |
| Policy-TimeAlgoOpt      | 0.6           |
| Policy-TimeSampleProc   | 0.371         |
| Policy-TimeSampling     | 1.7           |
| Policy-TimeStep         | 2.69          |
| Time                    | 1.39e+03      |
| n_timesteps             | 50000         |
-------------------------------------------

 ---------------- Iteration 50 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 500 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 501 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 502 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 503 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 504 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 505 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 506 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 507 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 508 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 509 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.529          |
| Data-EnvSampler-Poli... | 0.873          |
| Data-EnvTrajs-Averag... | 263            |
| Data-EnvTrajs-MaxReturn | 276            |
| Data-EnvTrajs-MinReturn | 251            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 7.89           |
| Data-TimeEnvSampleProc  | 0.00078        |
| Data-TimeEnvSampling    | 1.44           |
| Iteration               | 50             |
| ItrTime                 | 36.5           |
| LossAfter               | -0.0042772354  |
| LossBefore              | -5.1188913e-06 |
| Model-TimeModelFit      | 32.3           |
| ModelSampler-n_times... | 2040000        |
| Policy-AverageAbsPol... | 0.35243717     |
| Policy-AverageDiscou... | 109            |
| Policy-AveragePolicyStd | 0.40390325     |
| Policy-AverageReturn    | 255            |
| Policy-MaxReturn        | 272            |
| Policy-MinReturn        | 220            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 12.7           |
| Policy-TimeAlgoOpt      | 0.691          |
| Policy-TimeSampleProc   | 0.27           |
| Policy-TimeSampling     | 1.76           |
| Policy-TimeStep         | 2.76           |
| Time                    | 1.42e+03       |
| n_timesteps             | 51000          |
--------------------------------------------
Training finished
