Logging to /home/vincent/workplace/CS_287_-_Advanced_Robotics/homework/hw5/cs287hw5/ppo/run_scripts/data/parallel_mb_ppo/mbppo_Swimmer//01

 ---------------- Iteration 0 ----------------
Obtaining random samples from the environment...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 0 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 1 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 2 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 3 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 4 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 5 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 6 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 7 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 8 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 9 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.143          |
| Data-EnvSampler-Poli... | 0.0429         |
| Data-EnvTrajs-Averag... | 0.811          |
| Data-EnvTrajs-MaxReturn | 6.15           |
| Data-EnvTrajs-MinReturn | -6.45          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 4.22           |
| Data-TimeEnvSampleProc  | 0.000567       |
| Data-TimeEnvSampling    | 0.197          |
| Iteration               | 0              |
| ItrTime                 | 9.81           |
| LossAfter               | -0.004050891   |
| LossBefore              | -1.4006061e-05 |
| Model-TimeModelFit      | 3.35           |
| ModelSampler-n_times... | 40000          |
| Policy-AverageAbsPol... | 0.48421675     |
| Policy-AverageDiscou... | -243           |
| Policy-AveragePolicyStd | 0.9798422      |
| Policy-AverageReturn    | -1.1e+03       |
| Policy-MaxReturn        | -167           |
| Policy-MinReturn        | -1.34e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 289            |
| Policy-TimeAlgoOpt      | 1.12           |
| Policy-TimeSampleProc   | 0.603          |
| Policy-TimeSampling     | 4.49           |
| Policy-TimeStep         | 6.26           |
| Time                    | 9.81           |
| n_timesteps             | 1000           |
--------------------------------------------

 ---------------- Iteration 1 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 10 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 11 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 12 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 13 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 14 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 15 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 16 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 17 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 18 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 19 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.231          |
| Data-EnvSampler-Poli... | 0.521          |
| Data-EnvTrajs-Averag... | -7.18          |
| Data-EnvTrajs-MaxReturn | 3.87           |
| Data-EnvTrajs-MinReturn | -14.8          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 8.16           |
| Data-TimeEnvSampleProc  | 0.00106        |
| Data-TimeEnvSampling    | 0.774          |
| Iteration               | 1              |
| ItrTime                 | 8.17           |
| LossAfter               | -0.0026888254  |
| LossBefore              | -1.3818147e-05 |
| Model-TimeModelFit      | 4.67           |
| ModelSampler-n_times... | 80000          |
| Policy-AverageAbsPol... | 0.38059464     |
| Policy-AverageDiscou... | -32.1          |
| Policy-AveragePolicyStd | 0.96429104     |
| Policy-AverageReturn    | -132           |
| Policy-MaxReturn        | 57             |
| Policy-MinReturn        | -1.5e+03       |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 336            |
| Policy-TimeAlgoOpt      | 0.49           |
| Policy-TimeSampleProc   | 0.485          |
| Policy-TimeSampling     | 1.69           |
| Policy-TimeStep         | 2.72           |
| Time                    | 18.1           |
| n_timesteps             | 2000           |
--------------------------------------------

 ---------------- Iteration 2 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 20 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 21 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 22 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 23 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 24 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 25 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 26 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 27 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 28 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 29 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.226          |
| Data-EnvSampler-Poli... | 0.414          |
| Data-EnvTrajs-Averag... | 4.24           |
| Data-EnvTrajs-MaxReturn | 12.4           |
| Data-EnvTrajs-MinReturn | -2.58          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 5.15           |
| Data-TimeEnvSampleProc  | 0.000669       |
| Data-TimeEnvSampling    | 0.662          |
| Iteration               | 2              |
| ItrTime                 | 10.7           |
| LossAfter               | -0.008828862   |
| LossBefore              | -1.3633449e-05 |
| Model-TimeModelFit      | 7.13           |
| ModelSampler-n_times... | 120000         |
| Policy-AverageAbsPol... | 0.67163074     |
| Policy-AverageDiscou... | 660            |
| Policy-AveragePolicyStd | 0.9455801      |
| Policy-AverageReturn    | 2.42e+03       |
| Policy-MaxReturn        | 2.63e+03       |
| Policy-MinReturn        | 2.07e+03       |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 160            |
| Policy-TimeAlgoOpt      | 0.546          |
| Policy-TimeSampleProc   | 0.585          |
| Policy-TimeSampling     | 1.74           |
| Policy-TimeStep         | 2.89           |
| Time                    | 28.8           |
| n_timesteps             | 3000           |
--------------------------------------------

 ---------------- Iteration 3 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 30 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 31 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 32 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 33 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 34 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 35 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 36 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 37 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 38 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 39 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.255          |
| Data-EnvSampler-Poli... | 0.442          |
| Data-EnvTrajs-Averag... | -1.96          |
| Data-EnvTrajs-MaxReturn | 3.94           |
| Data-EnvTrajs-MinReturn | -7.85          |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 4.36           |
| Data-TimeEnvSampleProc  | 0.00103        |
| Data-TimeEnvSampling    | 0.72           |
| Iteration               | 3              |
| ItrTime                 | 12.1           |
| LossAfter               | -0.0021438573  |
| LossBefore              | -1.3543899e-05 |
| Model-TimeModelFit      | 8.58           |
| ModelSampler-n_times... | 160000         |
| Policy-AverageAbsPol... | 0.5245146      |
| Policy-AverageDiscou... | -25.4          |
| Policy-AveragePolicyStd | 0.93776184     |
| Policy-AverageReturn    | -122           |
| Policy-MaxReturn        | 5.72           |
| Policy-MinReturn        | -1.94e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 418            |
| Policy-TimeAlgoOpt      | 0.549          |
| Policy-TimeSampleProc   | 0.353          |
| Policy-TimeSampling     | 1.87           |
| Policy-TimeStep         | 2.79           |
| Time                    | 40.9           |
| n_timesteps             | 4000           |
--------------------------------------------

 ---------------- Iteration 4 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 40 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 41 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 42 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 43 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 44 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 45 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 46 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 47 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 48 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 49 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.244          |
| Data-EnvSampler-Poli... | 0.42           |
| Data-EnvTrajs-Averag... | -0.488         |
| Data-EnvTrajs-MaxReturn | 10.6           |
| Data-EnvTrajs-MinReturn | -12            |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 8.48           |
| Data-TimeEnvSampleProc  | 0.000951       |
| Data-TimeEnvSampling    | 0.686          |
| Iteration               | 4              |
| ItrTime                 | 14.8           |
| LossAfter               | -0.00417935    |
| LossBefore              | -1.3529444e-05 |
| Model-TimeModelFit      | 11.4           |
| ModelSampler-n_times... | 200000         |
| Policy-AverageAbsPol... | 0.5145407      |
| Policy-AverageDiscou... | -31.1          |
| Policy-AveragePolicyStd | 0.93659776     |
| Policy-AverageReturn    | -112           |
| Policy-MaxReturn        | 26.4           |
| Policy-MinReturn        | -2.29e+03      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 501            |
| Policy-TimeAlgoOpt      | 0.635          |
| Policy-TimeSampleProc   | 0.373          |
| Policy-TimeSampling     | 1.68           |
| Policy-TimeStep         | 2.73           |
| Time                    | 55.7           |
| n_timesteps             | 5000           |
--------------------------------------------

 ---------------- Iteration 5 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 50 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 51 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 52 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 53 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 54 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 55 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 56 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 57 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 58 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 59 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
---------------------------------------------
| Data-EnvSampler-EnvE... | 0.265           |
| Data-EnvSampler-Poli... | 0.477           |
| Data-EnvTrajs-Averag... | 2.24            |
| Data-EnvTrajs-MaxReturn | 13              |
| Data-EnvTrajs-MinReturn | -8.46           |
| Data-EnvTrajs-NumTrajs  | 5               |
| Data-EnvTrajs-StdReturn | 6.96            |
| Data-TimeEnvSampleProc  | 0.000798        |
| Data-TimeEnvSampling    | 0.768           |
| Iteration               | 5               |
| ItrTime                 | 16.5            |
| LossAfter               | -0.0055423784   |
| LossBefore              | -1.34127185e-05 |
| Model-TimeModelFit      | 12.9            |
| ModelSampler-n_times... | 240000          |
| Policy-AverageAbsPol... | 0.820077        |
| Policy-AverageDiscou... | -167            |
| Policy-AveragePolicyStd | 0.9264828       |
| Policy-AverageReturn    | -551            |
| Policy-MaxReturn        | -5.75           |
| Policy-MinReturn        | -5.95e+03       |
| Policy-NumTrajs         | 20              |
| Policy-StdReturn        | 1.61e+03        |
| Policy-TimeAlgoOpt      | 0.619           |
| Policy-TimeSampleProc   | 0.448           |
| Policy-TimeSampling     | 1.74            |
| Policy-TimeStep         | 2.85            |
| Time                    | 72.3            |
| n_timesteps             | 6000            |
---------------------------------------------

 ---------------- Iteration 6 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 60 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 61 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 62 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 63 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 64 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 65 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 66 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 67 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 68 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 69 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.271          |
| Data-EnvSampler-Poli... | 0.517          |
| Data-EnvTrajs-Averag... | 25.9           |
| Data-EnvTrajs-MaxReturn | 37.3           |
| Data-EnvTrajs-MinReturn | 16.9           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 7.05           |
| Data-TimeEnvSampleProc  | 0.000989       |
| Data-TimeEnvSampling    | 0.813          |
| Iteration               | 6              |
| ItrTime                 | 18.3           |
| LossAfter               | -0.003640034   |
| LossBefore              | -1.3288975e-05 |
| Model-TimeModelFit      | 15             |
| ModelSampler-n_times... | 280000         |
| Policy-AverageAbsPol... | 0.9329594      |
| Policy-AverageDiscou... | 15.2           |
| Policy-AveragePolicyStd | 0.9147743      |
| Policy-AverageReturn    | 37.1           |
| Policy-MaxReturn        | 64.3           |
| Policy-MinReturn        | 13.5           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 12.8           |
| Policy-TimeAlgoOpt      | 0.536          |
| Policy-TimeSampleProc   | 0.373          |
| Policy-TimeSampling     | 1.55           |
| Policy-TimeStep         | 2.48           |
| Time                    | 90.6           |
| n_timesteps             | 7000           |
--------------------------------------------

 ---------------- Iteration 7 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 70 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 71 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 72 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 73 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 74 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 75 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 76 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 77 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 78 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 79 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.29           |
| Data-EnvSampler-Poli... | 0.614          |
| Data-EnvTrajs-Averag... | 19.4           |
| Data-EnvTrajs-MaxReturn | 24             |
| Data-EnvTrajs-MinReturn | 16.9           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 2.49           |
| Data-TimeEnvSampleProc  | 0.00094        |
| Data-TimeEnvSampling    | 0.932          |
| Iteration               | 7              |
| ItrTime                 | 21             |
| LossAfter               | -0.0026645192  |
| LossBefore              | -1.3055362e-05 |
| Model-TimeModelFit      | 17.4           |
| ModelSampler-n_times... | 320000         |
| Policy-AverageAbsPol... | 1.1709673      |
| Policy-AverageDiscou... | 23.5           |
| Policy-AveragePolicyStd | 0.8924043      |
| Policy-AverageReturn    | 54.5           |
| Policy-MaxReturn        | 64.4           |
| Policy-MinReturn        | 44.3           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 5.52           |
| Policy-TimeAlgoOpt      | 0.515          |
| Policy-TimeSampleProc   | 0.49           |
| Policy-TimeSampling     | 1.54           |
| Policy-TimeStep         | 2.58           |
| Time                    | 112            |
| n_timesteps             | 8000           |
--------------------------------------------

 ---------------- Iteration 8 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 80 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 81 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 82 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 83 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 84 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 85 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 86 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 87 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 88 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 89 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.314          |
| Data-EnvSampler-Poli... | 0.686          |
| Data-EnvTrajs-Averag... | 26.4           |
| Data-EnvTrajs-MaxReturn | 38.2           |
| Data-EnvTrajs-MinReturn | 16.4           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 7.17           |
| Data-TimeEnvSampleProc  | 0.00222        |
| Data-TimeEnvSampling    | 1.03           |
| Iteration               | 8              |
| ItrTime                 | 23.4           |
| LossAfter               | -0.0035813116  |
| LossBefore              | -1.2642236e-05 |
| Model-TimeModelFit      | 19.6           |
| ModelSampler-n_times... | 360000         |
| Policy-AverageAbsPol... | 1.420834       |
| Policy-AverageDiscou... | 34.2           |
| Policy-AveragePolicyStd | 0.8582008      |
| Policy-AverageReturn    | 76.7           |
| Policy-MaxReturn        | 98.1           |
| Policy-MinReturn        | 61.3           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 9.97           |
| Policy-TimeAlgoOpt      | 0.582          |
| Policy-TimeSampleProc   | 0.482          |
| Policy-TimeSampling     | 1.75           |
| Policy-TimeStep         | 2.84           |
| Time                    | 135            |
| n_timesteps             | 9000           |
--------------------------------------------

 ---------------- Iteration 9 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 90 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 91 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 92 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 93 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 94 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 95 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 96 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 97 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 98 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 99 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.362          |
| Data-EnvSampler-Poli... | 0.898          |
| Data-EnvTrajs-Averag... | 24.9           |
| Data-EnvTrajs-MaxReturn | 45.6           |
| Data-EnvTrajs-MinReturn | 9.62           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 14.1           |
| Data-TimeEnvSampleProc  | 0.000987       |
| Data-TimeEnvSampling    | 1.3            |
| Iteration               | 9              |
| ItrTime                 | 25.9           |
| LossAfter               | -0.0026723363  |
| LossBefore              | -1.2371061e-05 |
| Model-TimeModelFit      | 21.8           |
| ModelSampler-n_times... | 400000         |
| Policy-AverageAbsPol... | 1.4338782      |
| Policy-AverageDiscou... | 40.9           |
| Policy-AveragePolicyStd | 0.8315234      |
| Policy-AverageReturn    | 110            |
| Policy-MaxReturn        | 138            |
| Policy-MinReturn        | 74.2           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 18.4           |
| Policy-TimeAlgoOpt      | 0.526          |
| Policy-TimeSampleProc   | 0.579          |
| Policy-TimeSampling     | 1.71           |
| Policy-TimeStep         | 2.85           |
| Time                    | 161            |
| n_timesteps             | 10000          |
--------------------------------------------

 ---------------- Iteration 10 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 100 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 101 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 102 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 103 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 104 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 105 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 106 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 107 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 108 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 109 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.354          |
| Data-EnvSampler-Poli... | 0.879          |
| Data-EnvTrajs-Averag... | 29.3           |
| Data-EnvTrajs-MaxReturn | 33.9           |
| Data-EnvTrajs-MinReturn | 23.7           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 3.48           |
| Data-TimeEnvSampleProc  | 0.00088        |
| Data-TimeEnvSampling    | 1.27           |
| Iteration               | 10             |
| ItrTime                 | 28.9           |
| LossAfter               | -0.005720685   |
| LossBefore              | -1.2246349e-05 |
| Model-TimeModelFit      | 25             |
| ModelSampler-n_times... | 440000         |
| Policy-AverageAbsPol... | 1.4203607      |
| Policy-AverageDiscou... | -3.64          |
| Policy-AveragePolicyStd | 0.8245873      |
| Policy-AverageReturn    | -14.3          |
| Policy-MaxReturn        | 6.7            |
| Policy-MinReturn        | -39.5          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 13.6           |
| Policy-TimeAlgoOpt      | 0.615          |
| Policy-TimeSampleProc   | 0.437          |
| Policy-TimeSampling     | 1.55           |
| Policy-TimeStep         | 2.63           |
| Time                    | 190            |
| n_timesteps             | 11000          |
--------------------------------------------

 ---------------- Iteration 11 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 110 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 111 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 112 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 113 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 114 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 115 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 116 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 117 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 118 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 119 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.379          |
| Data-EnvSampler-Poli... | 0.894          |
| Data-EnvTrajs-Averag... | 31.1           |
| Data-EnvTrajs-MaxReturn | 33.8           |
| Data-EnvTrajs-MinReturn | 29.1           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 1.84           |
| Data-TimeEnvSampleProc  | 0.00064        |
| Data-TimeEnvSampling    | 1.31           |
| Iteration               | 11             |
| ItrTime                 | 31.4           |
| LossAfter               | -0.0035166882  |
| LossBefore              | -1.2124424e-05 |
| Model-TimeModelFit      | 27.3           |
| ModelSampler-n_times... | 480000         |
| Policy-AverageAbsPol... | 1.7739043      |
| Policy-AverageDiscou... | 39.8           |
| Policy-AveragePolicyStd | 0.8145779      |
| Policy-AverageReturn    | 97.2           |
| Policy-MaxReturn        | 161            |
| Policy-MinReturn        | 57.6           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 23.6           |
| Policy-TimeAlgoOpt      | 0.535          |
| Policy-TimeSampleProc   | 0.426          |
| Policy-TimeSampling     | 1.81           |
| Policy-TimeStep         | 2.78           |
| Time                    | 221            |
| n_timesteps             | 12000          |
--------------------------------------------

 ---------------- Iteration 12 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 120 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 121 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 122 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 123 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 124 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 125 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 126 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 127 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 128 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 129 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.387          |
| Data-EnvSampler-Poli... | 0.854          |
| Data-EnvTrajs-Averag... | 37.4           |
| Data-EnvTrajs-MaxReturn | 47.1           |
| Data-EnvTrajs-MinReturn | 26.1           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 7.01           |
| Data-TimeEnvSampleProc  | 0.00108        |
| Data-TimeEnvSampling    | 1.28           |
| Iteration               | 12             |
| ItrTime                 | 5.66           |
| LossAfter               | -0.0031552587  |
| LossBefore              | -1.2015309e-05 |
| Model-TimeModelFit      | 1.65           |
| ModelSampler-n_times... | 520000         |
| Policy-AverageAbsPol... | 1.6422107      |
| Policy-AverageDiscou... | 9.86           |
| Policy-AveragePolicyStd | 0.80732703     |
| Policy-AverageReturn    | 22.1           |
| Policy-MaxReturn        | 162            |
| Policy-MinReturn        | -17            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 38.6           |
| Policy-TimeAlgoOpt      | 0.629          |
| Policy-TimeSampleProc   | 0.373          |
| Policy-TimeSampling     | 1.7            |
| Policy-TimeStep         | 2.73           |
| Time                    | 227            |
| n_timesteps             | 13000          |
--------------------------------------------

 ---------------- Iteration 13 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 130 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 131 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 132 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 133 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 134 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 135 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 136 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 137 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 138 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 139 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.341          |
| Data-EnvSampler-Poli... | 0.874          |
| Data-EnvTrajs-Averag... | 29.8           |
| Data-EnvTrajs-MaxReturn | 41.3           |
| Data-EnvTrajs-MinReturn | 20.1           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 7.91           |
| Data-TimeEnvSampleProc  | 0.000583       |
| Data-TimeEnvSampling    | 1.25           |
| Iteration               | 13             |
| ItrTime                 | 30.9           |
| LossAfter               | -0.004370463   |
| LossBefore              | -1.1511944e-05 |
| Model-TimeModelFit      | 27.3           |
| ModelSampler-n_times... | 560000         |
| Policy-AverageAbsPol... | 2.0522873      |
| Policy-AverageDiscou... | 62.6           |
| Policy-AveragePolicyStd | 0.76604897     |
| Policy-AverageReturn    | 155            |
| Policy-MaxReturn        | 178            |
| Policy-MinReturn        | 134            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 11.3           |
| Policy-TimeAlgoOpt      | 0.541          |
| Policy-TimeSampleProc   | 0.358          |
| Policy-TimeSampling     | 1.43           |
| Policy-TimeStep         | 2.38           |
| Time                    | 258            |
| n_timesteps             | 14000          |
--------------------------------------------

 ---------------- Iteration 14 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 140 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 141 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 142 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 143 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 144 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 145 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 146 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 147 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 148 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 149 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.33           |
| Data-EnvSampler-Poli... | 0.824          |
| Data-EnvTrajs-Averag... | 27.8           |
| Data-EnvTrajs-MaxReturn | 45.2           |
| Data-EnvTrajs-MinReturn | 12.1           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 10.9           |
| Data-TimeEnvSampleProc  | 0.000978       |
| Data-TimeEnvSampling    | 1.19           |
| Iteration               | 14             |
| ItrTime                 | 32.4           |
| LossAfter               | -0.0017626416  |
| LossBefore              | -1.1243203e-05 |
| Model-TimeModelFit      | 28.5           |
| ModelSampler-n_times... | 600000         |
| Policy-AverageAbsPol... | 2.028619       |
| Policy-AverageDiscou... | 93.5           |
| Policy-AveragePolicyStd | 0.7451951      |
| Policy-AverageReturn    | 265            |
| Policy-MaxReturn        | 408            |
| Policy-MinReturn        | 94.1           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 65.4           |
| Policy-TimeAlgoOpt      | 0.564          |
| Policy-TimeSampleProc   | 0.534          |
| Policy-TimeSampling     | 1.51           |
| Policy-TimeStep         | 2.66           |
| Time                    | 290            |
| n_timesteps             | 15000          |
--------------------------------------------

 ---------------- Iteration 15 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 150 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 151 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 152 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 153 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 154 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 155 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 156 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 157 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 158 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 159 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.356          |
| Data-EnvSampler-Poli... | 0.868          |
| Data-EnvTrajs-Averag... | 12.9           |
| Data-EnvTrajs-MaxReturn | 22.4           |
| Data-EnvTrajs-MinReturn | 5.03           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 5.9            |
| Data-TimeEnvSampleProc  | 0.000628       |
| Data-TimeEnvSampling    | 1.26           |
| Iteration               | 15             |
| ItrTime                 | 32.6           |
| LossAfter               | -0.0029100694  |
| LossBefore              | -1.1136023e-05 |
| Model-TimeModelFit      | 28.8           |
| ModelSampler-n_times... | 640000         |
| Policy-AverageAbsPol... | 2.3661342      |
| Policy-AverageDiscou... | 18.6           |
| Policy-AveragePolicyStd | 0.7390253      |
| Policy-AverageReturn    | 49.1           |
| Policy-MaxReturn        | 122            |
| Policy-MinReturn        | -3.6           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 27.5           |
| Policy-TimeAlgoOpt      | 0.588          |
| Policy-TimeSampleProc   | 0.411          |
| Policy-TimeSampling     | 1.55           |
| Policy-TimeStep         | 2.59           |
| Time                    | 323            |
| n_timesteps             | 16000          |
--------------------------------------------

 ---------------- Iteration 16 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 160 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 161 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 162 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 163 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 164 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 165 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 166 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 167 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 168 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 169 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.369          |
| Data-EnvSampler-Poli... | 0.87           |
| Data-EnvTrajs-Averag... | 14.2           |
| Data-EnvTrajs-MaxReturn | 21.7           |
| Data-EnvTrajs-MinReturn | 6.07           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 5.27           |
| Data-TimeEnvSampleProc  | 0.00129        |
| Data-TimeEnvSampling    | 1.28           |
| Iteration               | 16             |
| ItrTime                 | 32.4           |
| LossAfter               | -0.0048668296  |
| LossBefore              | -1.1014837e-05 |
| Model-TimeModelFit      | 28.4           |
| ModelSampler-n_times... | 680000         |
| Policy-AverageAbsPol... | 2.151706       |
| Policy-AverageDiscou... | 67.7           |
| Policy-AveragePolicyStd | 0.7293269      |
| Policy-AverageReturn    | 181            |
| Policy-MaxReturn        | 201            |
| Policy-MinReturn        | 143            |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 13.4           |
| Policy-TimeAlgoOpt      | 0.614          |
| Policy-TimeSampleProc   | 0.401          |
| Policy-TimeSampling     | 1.71           |
| Policy-TimeStep         | 2.74           |
| Time                    | 355            |
| n_timesteps             | 17000          |
--------------------------------------------

 ---------------- Iteration 17 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 170 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 171 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 172 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 173 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 174 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 175 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 176 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 177 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 178 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 179 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.336          |
| Data-EnvSampler-Poli... | 0.798          |
| Data-EnvTrajs-Averag... | 31             |
| Data-EnvTrajs-MaxReturn | 39.3           |
| Data-EnvTrajs-MinReturn | 24.7           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 5.6            |
| Data-TimeEnvSampleProc  | 0.00105        |
| Data-TimeEnvSampling    | 1.17           |
| Iteration               | 17             |
| ItrTime                 | 32.7           |
| LossAfter               | -0.003438223   |
| LossBefore              | -1.0798085e-05 |
| Model-TimeModelFit      | 29             |
| ModelSampler-n_times... | 720000         |
| Policy-AverageAbsPol... | 2.3461435      |
| Policy-AverageDiscou... | -5.02          |
| Policy-AveragePolicyStd | 0.71455675     |
| Policy-AverageReturn    | -12.5          |
| Policy-MaxReturn        | 11.5           |
| Policy-MinReturn        | -39.8          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 11.8           |
| Policy-TimeAlgoOpt      | 0.541          |
| Policy-TimeSampleProc   | 0.358          |
| Policy-TimeSampling     | 1.57           |
| Policy-TimeStep         | 2.51           |
| Time                    | 388            |
| n_timesteps             | 18000          |
--------------------------------------------

 ---------------- Iteration 18 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 180 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 181 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 182 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 183 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 184 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 185 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 186 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 187 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 188 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 189 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.362          |
| Data-EnvSampler-Poli... | 0.888          |
| Data-EnvTrajs-Averag... | 34             |
| Data-EnvTrajs-MaxReturn | 35.9           |
| Data-EnvTrajs-MinReturn | 32.7           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 1.13           |
| Data-TimeEnvSampleProc  | 0.00104        |
| Data-TimeEnvSampling    | 1.28           |
| Iteration               | 18             |
| ItrTime                 | 32.6           |
| LossAfter               | -0.0030450283  |
| LossBefore              | -1.0655943e-05 |
| Model-TimeModelFit      | 28.8           |
| ModelSampler-n_times... | 760000         |
| Policy-AverageAbsPol... | 2.234506       |
| Policy-AverageDiscou... | 22.2           |
| Policy-AveragePolicyStd | 0.7050376      |
| Policy-AverageReturn    | 57.7           |
| Policy-MaxReturn        | 90.7           |
| Policy-MinReturn        | 26.1           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 17.8           |
| Policy-TimeAlgoOpt      | 0.559          |
| Policy-TimeSampleProc   | 0.397          |
| Policy-TimeSampling     | 1.57           |
| Policy-TimeStep         | 2.55           |
| Time                    | 421            |
| n_timesteps             | 19000          |
--------------------------------------------

 ---------------- Iteration 19 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 190 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 191 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 192 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 193 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 194 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 195 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 196 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 197 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 198 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 199 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.356          |
| Data-EnvSampler-Poli... | 0.826          |
| Data-EnvTrajs-Averag... | 26             |
| Data-EnvTrajs-MaxReturn | 34.9           |
| Data-EnvTrajs-MinReturn | 11.6           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 8.67           |
| Data-TimeEnvSampleProc  | 0.00107        |
| Data-TimeEnvSampling    | 1.22           |
| Iteration               | 19             |
| ItrTime                 | 27.3           |
| LossAfter               | -0.0061328076  |
| LossBefore              | -1.0538551e-05 |
| Model-TimeModelFit      | 23.6           |
| ModelSampler-n_times... | 800000         |
| Policy-AverageAbsPol... | 2.0956895      |
| Policy-AverageDiscou... | -3.48          |
| Policy-AveragePolicyStd | 0.69472444     |
| Policy-AverageReturn    | -11.1          |
| Policy-MaxReturn        | 59.2           |
| Policy-MinReturn        | -56.1          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 22.4           |
| Policy-TimeAlgoOpt      | 0.555          |
| Policy-TimeSampleProc   | 0.382          |
| Policy-TimeSampling     | 1.57           |
| Policy-TimeStep         | 2.55           |
| Time                    | 448            |
| n_timesteps             | 20000          |
--------------------------------------------

 ---------------- Iteration 20 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 200 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 201 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 202 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 203 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 204 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 205 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 206 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 207 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 208 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 209 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.369          |
| Data-EnvSampler-Poli... | 0.819          |
| Data-EnvTrajs-Averag... | 35.1           |
| Data-EnvTrajs-MaxReturn | 44.5           |
| Data-EnvTrajs-MinReturn | 19.5           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 9.35           |
| Data-TimeEnvSampleProc  | 0.000746       |
| Data-TimeEnvSampling    | 1.22           |
| Iteration               | 20             |
| ItrTime                 | 6.07           |
| LossAfter               | -0.00425543    |
| LossBefore              | -1.0345113e-05 |
| Model-TimeModelFit      | 2.3            |
| ModelSampler-n_times... | 840000         |
| Policy-AverageAbsPol... | 1.9845592      |
| Policy-AverageDiscou... | 37.4           |
| Policy-AveragePolicyStd | 0.68266356     |
| Policy-AverageReturn    | 91.7           |
| Policy-MaxReturn        | 120            |
| Policy-MinReturn        | 52.9           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 16.6           |
| Policy-TimeAlgoOpt      | 0.48           |
| Policy-TimeSampleProc   | 0.467          |
| Policy-TimeSampling     | 1.57           |
| Policy-TimeStep         | 2.54           |
| Time                    | 454            |
| n_timesteps             | 21000          |
--------------------------------------------

 ---------------- Iteration 21 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 210 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 211 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 212 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 213 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 214 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 215 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 216 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 217 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 218 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 219 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.358          |
| Data-EnvSampler-Poli... | 0.891          |
| Data-EnvTrajs-Averag... | 44.6           |
| Data-EnvTrajs-MaxReturn | 47.7           |
| Data-EnvTrajs-MinReturn | 40.9           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 2.9            |
| Data-TimeEnvSampleProc  | 0.00135        |
| Data-TimeEnvSampling    | 1.28           |
| Iteration               | 21             |
| ItrTime                 | 30.8           |
| LossAfter               | -0.0029758266  |
| LossBefore              | -1.0065854e-05 |
| Model-TimeModelFit      | 27.1           |
| ModelSampler-n_times... | 880000         |
| Policy-AverageAbsPol... | 1.839532       |
| Policy-AverageDiscou... | 15.6           |
| Policy-AveragePolicyStd | 0.66435605     |
| Policy-AverageReturn    | 38.9           |
| Policy-MaxReturn        | 61.8           |
| Policy-MinReturn        | -1.08          |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 14.3           |
| Policy-TimeAlgoOpt      | 0.527          |
| Policy-TimeSampleProc   | 0.35           |
| Policy-TimeSampling     | 1.55           |
| Policy-TimeStep         | 2.48           |
| Time                    | 485            |
| n_timesteps             | 22000          |
--------------------------------------------

 ---------------- Iteration 22 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 220 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 221 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 222 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 223 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 224 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 225 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 226 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 227 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 228 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 229 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
---------------------------------------------
| Data-EnvSampler-EnvE... | 0.36            |
| Data-EnvSampler-Poli... | 0.874           |
| Data-EnvTrajs-Averag... | 48.4            |
| Data-EnvTrajs-MaxReturn | 63.2            |
| Data-EnvTrajs-MinReturn | 38.6            |
| Data-EnvTrajs-NumTrajs  | 5               |
| Data-EnvTrajs-StdReturn | 8.12            |
| Data-TimeEnvSampleProc  | 0.000703        |
| Data-TimeEnvSampling    | 1.28            |
| Iteration               | 22              |
| ItrTime                 | 32.9            |
| LossAfter               | -0.00087538565  |
| LossBefore              | -1.00475345e-05 |
| Model-TimeModelFit      | 29              |
| ModelSampler-n_times... | 920000          |
| Policy-AverageAbsPol... | 1.4854659       |
| Policy-AverageDiscou... | 40              |
| Policy-AveragePolicyStd | 0.6674616       |
| Policy-AverageReturn    | 96.7            |
| Policy-MaxReturn        | 134             |
| Policy-MinReturn        | 69.3            |
| Policy-NumTrajs         | 20              |
| Policy-StdReturn        | 16.2            |
| Policy-TimeAlgoOpt      | 0.595           |
| Policy-TimeSampleProc   | 0.331           |
| Policy-TimeSampling     | 1.68            |
| Policy-TimeStep         | 2.64            |
| Time                    | 518             |
| n_timesteps             | 23000           |
---------------------------------------------

 ---------------- Iteration 23 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 230 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 231 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 232 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 233 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 234 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 235 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 236 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 237 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 238 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 239 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.423         |
| Data-EnvSampler-Poli... | 0.928         |
| Data-EnvTrajs-Averag... | 38.7          |
| Data-EnvTrajs-MaxReturn | 55.8          |
| Data-EnvTrajs-MinReturn | 24            |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 12            |
| Data-TimeEnvSampleProc  | 0.00112       |
| Data-TimeEnvSampling    | 1.39          |
| Iteration               | 23            |
| ItrTime                 | 33.4          |
| LossAfter               | -0.0036966016 |
| LossBefore              | -9.920357e-06 |
| Model-TimeModelFit      | 29.5          |
| ModelSampler-n_times... | 960000        |
| Policy-AverageAbsPol... | 1.6879144     |
| Policy-AverageDiscou... | 28.5          |
| Policy-AveragePolicyStd | 0.6573469     |
| Policy-AverageReturn    | 68.6          |
| Policy-MaxReturn        | 85.7          |
| Policy-MinReturn        | 50.7          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 9.39          |
| Policy-TimeAlgoOpt      | 0.577         |
| Policy-TimeSampleProc   | 0.401         |
| Policy-TimeSampling     | 1.47          |
| Policy-TimeStep         | 2.51          |
| Time                    | 551           |
| n_timesteps             | 24000         |
-------------------------------------------

 ---------------- Iteration 24 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 240 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 241 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 242 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 243 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 244 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 245 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 246 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 247 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 248 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 249 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.433          |
| Data-EnvSampler-Poli... | 1.11           |
| Data-EnvTrajs-Averag... | 49             |
| Data-EnvTrajs-MaxReturn | 58.6           |
| Data-EnvTrajs-MinReturn | 42.3           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 6.24           |
| Data-TimeEnvSampleProc  | 0.001          |
| Data-TimeEnvSampling    | 1.6            |
| Iteration               | 24             |
| ItrTime                 | 32.9           |
| LossAfter               | -1.4953691e-05 |
| LossBefore              | -9.860887e-06  |
| Model-TimeModelFit      | 28.5           |
| ModelSampler-n_times... | 1000000        |
| Policy-AverageAbsPol... | 1.6306039      |
| Policy-AverageDiscou... | 25.2           |
| Policy-AveragePolicyStd | 0.65172637     |
| Policy-AverageReturn    | 60             |
| Policy-MaxReturn        | 75             |
| Policy-MinReturn        | 37.9           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 11.2           |
| Policy-TimeAlgoOpt      | 0.615          |
| Policy-TimeSampleProc   | 0.414          |
| Policy-TimeSampling     | 1.79           |
| Policy-TimeStep         | 2.87           |
| Time                    | 584            |
| n_timesteps             | 25000          |
--------------------------------------------

 ---------------- Iteration 25 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 250 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 251 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 252 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 253 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 254 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 255 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 256 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 257 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 258 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 259 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.349         |
| Data-EnvSampler-Poli... | 0.846         |
| Data-EnvTrajs-Averag... | 57.7          |
| Data-EnvTrajs-MaxReturn | 66.2          |
| Data-EnvTrajs-MinReturn | 46.5          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.34          |
| Data-TimeEnvSampleProc  | 0.00106       |
| Data-TimeEnvSampling    | 1.23          |
| Iteration               | 25            |
| ItrTime                 | 33.4          |
| LossAfter               | -0.0028878807 |
| LossBefore              | -9.569471e-06 |
| Model-TimeModelFit      | 29.7          |
| ModelSampler-n_times... | 1040000       |
| Policy-AverageAbsPol... | 1.5478874     |
| Policy-AverageDiscou... | 23.8          |
| Policy-AveragePolicyStd | 0.6318541     |
| Policy-AverageReturn    | 55.1          |
| Policy-MaxReturn        | 77.3          |
| Policy-MinReturn        | 33.9          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 10.3          |
| Policy-TimeAlgoOpt      | 0.574         |
| Policy-TimeSampleProc   | 0.447         |
| Policy-TimeSampling     | 1.47          |
| Policy-TimeStep         | 2.52          |
| Time                    | 618           |
| n_timesteps             | 26000         |
-------------------------------------------

 ---------------- Iteration 26 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 260 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 261 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 262 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 263 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 264 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 265 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 266 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 267 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 268 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 269 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.408         |
| Data-EnvSampler-Poli... | 1.12          |
| Data-EnvTrajs-Averag... | 51.2          |
| Data-EnvTrajs-MaxReturn | 58.5          |
| Data-EnvTrajs-MinReturn | 46.5          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 4.37          |
| Data-TimeEnvSampleProc  | 0.00093       |
| Data-TimeEnvSampling    | 1.56          |
| Iteration               | 26            |
| ItrTime                 | 33.3          |
| LossAfter               | -0.004019884  |
| LossBefore              | -9.321829e-06 |
| Model-TimeModelFit      | 28.8          |
| ModelSampler-n_times... | 1080000       |
| Policy-AverageAbsPol... | 1.2815017     |
| Policy-AverageDiscou... | 23.4          |
| Policy-AveragePolicyStd | 0.61740637    |
| Policy-AverageReturn    | 47            |
| Policy-MaxReturn        | 59            |
| Policy-MinReturn        | 36.8          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 7.2           |
| Policy-TimeAlgoOpt      | 0.588         |
| Policy-TimeSampleProc   | 0.523         |
| Policy-TimeSampling     | 1.73          |
| Policy-TimeStep         | 2.88          |
| Time                    | 651           |
| n_timesteps             | 27000         |
-------------------------------------------

 ---------------- Iteration 27 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 270 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 271 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 272 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 273 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 274 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 275 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 276 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 277 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 278 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 279 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.408         |
| Data-EnvSampler-Poli... | 0.88          |
| Data-EnvTrajs-Averag... | 48.1          |
| Data-EnvTrajs-MaxReturn | 58.7          |
| Data-EnvTrajs-MinReturn | 41            |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.07          |
| Data-TimeEnvSampleProc  | 0.00065       |
| Data-TimeEnvSampling    | 1.33          |
| Iteration               | 27            |
| ItrTime                 | 34.7          |
| LossAfter               | -0.0036107735 |
| LossBefore              | -9.164092e-06 |
| Model-TimeModelFit      | 30.6          |
| ModelSampler-n_times... | 1120000       |
| Policy-AverageAbsPol... | 1.4455925     |
| Policy-AverageDiscou... | 28.2          |
| Policy-AveragePolicyStd | 0.6070924     |
| Policy-AverageReturn    | 65.5          |
| Policy-MaxReturn        | 139           |
| Policy-MinReturn        | 24.3          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 30.9          |
| Policy-TimeAlgoOpt      | 0.618         |
| Policy-TimeSampleProc   | 0.455         |
| Policy-TimeSampling     | 1.68          |
| Policy-TimeStep         | 2.78          |
| Time                    | 686           |
| n_timesteps             | 28000         |
-------------------------------------------

 ---------------- Iteration 28 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 280 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 281 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 282 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 283 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 284 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 285 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 286 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 287 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 288 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 289 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.396         |
| Data-EnvSampler-Poli... | 1.19          |
| Data-EnvTrajs-Averag... | 52.9          |
| Data-EnvTrajs-MaxReturn | 61.8          |
| Data-EnvTrajs-MinReturn | 41.6          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 7.26          |
| Data-TimeEnvSampleProc  | 0.000939      |
| Data-TimeEnvSampling    | 1.62          |
| Iteration               | 28            |
| ItrTime                 | 33.3          |
| LossAfter               | -0.004225263  |
| LossBefore              | -8.970972e-06 |
| Model-TimeModelFit      | 29.1          |
| ModelSampler-n_times... | 1160000       |
| Policy-AverageAbsPol... | 1.4079055     |
| Policy-AverageDiscou... | 27            |
| Policy-AveragePolicyStd | 0.5950886     |
| Policy-AverageReturn    | 74.2          |
| Policy-MaxReturn        | 112           |
| Policy-MinReturn        | 16.2          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 26.9          |
| Policy-TimeAlgoOpt      | 0.533         |
| Policy-TimeSampleProc   | 0.412         |
| Policy-TimeSampling     | 1.53          |
| Policy-TimeStep         | 2.51          |
| Time                    | 719           |
| n_timesteps             | 29000         |
-------------------------------------------

 ---------------- Iteration 29 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 290 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 291 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 292 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 293 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 294 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 295 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 296 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 297 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 298 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 299 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.384         |
| Data-EnvSampler-Poli... | 0.87          |
| Data-EnvTrajs-Averag... | 53.4          |
| Data-EnvTrajs-MaxReturn | 63            |
| Data-EnvTrajs-MinReturn | 48.7          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 4.98          |
| Data-TimeEnvSampleProc  | 0.000884      |
| Data-TimeEnvSampling    | 1.29          |
| Iteration               | 29            |
| ItrTime                 | 34.4          |
| LossAfter               | -0.0030092543 |
| LossBefore              | -8.558212e-06 |
| Model-TimeModelFit      | 30.6          |
| ModelSampler-n_times... | 1200000       |
| Policy-AverageAbsPol... | 1.4399766     |
| Policy-AverageDiscou... | 24.4          |
| Policy-AveragePolicyStd | 0.5714076     |
| Policy-AverageReturn    | 58.3          |
| Policy-MaxReturn        | 86.6          |
| Policy-MinReturn        | 13.1          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 13.6          |
| Policy-TimeAlgoOpt      | 0.558         |
| Policy-TimeSampleProc   | 0.438         |
| Policy-TimeSampling     | 1.47          |
| Policy-TimeStep         | 2.48          |
| Time                    | 753           |
| n_timesteps             | 30000         |
-------------------------------------------

 ---------------- Iteration 30 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 300 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 301 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 302 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 303 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 304 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 305 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 306 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 307 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 308 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 309 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.381         |
| Data-EnvSampler-Poli... | 0.914         |
| Data-EnvTrajs-Averag... | 55.2          |
| Data-EnvTrajs-MaxReturn | 61.1          |
| Data-EnvTrajs-MinReturn | 48.5          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 5.55          |
| Data-TimeEnvSampleProc  | 0.00101       |
| Data-TimeEnvSampling    | 1.33          |
| Iteration               | 30            |
| ItrTime                 | 34.7          |
| LossAfter               | -0.0039777304 |
| LossBefore              | -8.225026e-06 |
| Model-TimeModelFit      | 30.9          |
| ModelSampler-n_times... | 1240000       |
| Policy-AverageAbsPol... | 1.3776108     |
| Policy-AverageDiscou... | 22.5          |
| Policy-AveragePolicyStd | 0.5506644     |
| Policy-AverageReturn    | 48            |
| Policy-MaxReturn        | 79            |
| Policy-MinReturn        | -64.2         |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 28.3          |
| Policy-TimeAlgoOpt      | 0.54          |
| Policy-TimeSampleProc   | 0.383         |
| Policy-TimeSampling     | 1.51          |
| Policy-TimeStep         | 2.47          |
| Time                    | 788           |
| n_timesteps             | 31000         |
-------------------------------------------

 ---------------- Iteration 31 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 310 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 311 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 312 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 313 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 314 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 315 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 316 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 317 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 318 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 319 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.336         |
| Data-EnvSampler-Poli... | 0.787         |
| Data-EnvTrajs-Averag... | 56            |
| Data-EnvTrajs-MaxReturn | 58.1          |
| Data-EnvTrajs-MinReturn | 53.4          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 1.67          |
| Data-TimeEnvSampleProc  | 0.000977      |
| Data-TimeEnvSampling    | 1.16          |
| Iteration               | 31            |
| ItrTime                 | 34.6          |
| LossAfter               | -0.0014794391 |
| LossBefore              | -8.015567e-06 |
| Model-TimeModelFit      | 31            |
| ModelSampler-n_times... | 1280000       |
| Policy-AverageAbsPol... | 1.4943994     |
| Policy-AverageDiscou... | 33.8          |
| Policy-AveragePolicyStd | 0.5394907     |
| Policy-AverageReturn    | 83.5          |
| Policy-MaxReturn        | 99.8          |
| Policy-MinReturn        | 63            |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 10.8          |
| Policy-TimeAlgoOpt      | 0.511         |
| Policy-TimeSampleProc   | 0.336         |
| Policy-TimeSampling     | 1.57          |
| Policy-TimeStep         | 2.44          |
| Time                    | 823           |
| n_timesteps             | 32000         |
-------------------------------------------

 ---------------- Iteration 32 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 320 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 321 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 322 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 323 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 324 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 325 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 326 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 327 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 328 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 329 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.382         |
| Data-EnvSampler-Poli... | 0.9           |
| Data-EnvTrajs-Averag... | 55.3          |
| Data-EnvTrajs-MaxReturn | 61.1          |
| Data-EnvTrajs-MinReturn | 45.2          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 5.79          |
| Data-TimeEnvSampleProc  | 0.00104       |
| Data-TimeEnvSampling    | 1.32          |
| Iteration               | 32            |
| ItrTime                 | 34.8          |
| LossAfter               | -0.0025355145 |
| LossBefore              | -7.839111e-06 |
| Model-TimeModelFit      | 30.8          |
| ModelSampler-n_times... | 1320000       |
| Policy-AverageAbsPol... | 1.4493504     |
| Policy-AverageDiscou... | 30.8          |
| Policy-AveragePolicyStd | 0.53135157    |
| Policy-AverageReturn    | 70.1          |
| Policy-MaxReturn        | 89            |
| Policy-MinReturn        | 49.9          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 9.13          |
| Policy-TimeAlgoOpt      | 0.538         |
| Policy-TimeSampleProc   | 0.444         |
| Policy-TimeSampling     | 1.57          |
| Policy-TimeStep         | 2.61          |
| Time                    | 857           |
| n_timesteps             | 33000         |
-------------------------------------------

 ---------------- Iteration 33 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 330 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 331 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 332 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 333 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 334 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 335 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 336 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 337 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 338 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 339 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.42          |
| Data-EnvSampler-Poli... | 0.945         |
| Data-EnvTrajs-Averag... | 55.8          |
| Data-EnvTrajs-MaxReturn | 60.1          |
| Data-EnvTrajs-MinReturn | 43.6          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.15          |
| Data-TimeEnvSampleProc  | 0.00102       |
| Data-TimeEnvSampling    | 1.41          |
| Iteration               | 33            |
| ItrTime                 | 35.6          |
| LossAfter               | -0.0044323574 |
| LossBefore              | -7.465715e-06 |
| Model-TimeModelFit      | 31.6          |
| ModelSampler-n_times... | 1360000       |
| Policy-AverageAbsPol... | 1.5195651     |
| Policy-AverageDiscou... | 26.5          |
| Policy-AveragePolicyStd | 0.51072806    |
| Policy-AverageReturn    | 57.5          |
| Policy-MaxReturn        | 69.2          |
| Policy-MinReturn        | 46            |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 5.14          |
| Policy-TimeAlgoOpt      | 0.589         |
| Policy-TimeSampleProc   | 0.407         |
| Policy-TimeSampling     | 1.58          |
| Policy-TimeStep         | 2.61          |
| Time                    | 893           |
| n_timesteps             | 34000         |
-------------------------------------------

 ---------------- Iteration 34 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 340 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 341 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 342 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 343 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 344 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 345 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 346 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 347 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 348 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 349 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.362         |
| Data-EnvSampler-Poli... | 0.924         |
| Data-EnvTrajs-Averag... | 52            |
| Data-EnvTrajs-MaxReturn | 56.9          |
| Data-EnvTrajs-MinReturn | 45.3          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 3.96          |
| Data-TimeEnvSampleProc  | 0.00102       |
| Data-TimeEnvSampling    | 1.32          |
| Iteration               | 34            |
| ItrTime                 | 35.9          |
| LossAfter               | -0.0028917068 |
| LossBefore              | -7.30781e-06  |
| Model-TimeModelFit      | 31.8          |
| ModelSampler-n_times... | 1400000       |
| Policy-AverageAbsPol... | 1.4807212     |
| Policy-AverageDiscou... | 33.2          |
| Policy-AveragePolicyStd | 0.50249714    |
| Policy-AverageReturn    | 68            |
| Policy-MaxReturn        | 81.6          |
| Policy-MinReturn        | 55.6          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 6.35          |
| Policy-TimeAlgoOpt      | 0.586         |
| Policy-TimeSampleProc   | 0.447         |
| Policy-TimeSampling     | 1.73          |
| Policy-TimeStep         | 2.82          |
| Time                    | 929           |
| n_timesteps             | 35000         |
-------------------------------------------

 ---------------- Iteration 35 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 350 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 351 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 352 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 353 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 354 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 355 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 356 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 357 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 358 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 359 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.379          |
| Data-EnvSampler-Poli... | 0.907          |
| Data-EnvTrajs-Averag... | 53.2           |
| Data-EnvTrajs-MaxReturn | 57             |
| Data-EnvTrajs-MinReturn | 50.9           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 2.74           |
| Data-TimeEnvSampleProc  | 0.000886       |
| Data-TimeEnvSampling    | 1.32           |
| Iteration               | 35             |
| ItrTime                 | 36             |
| LossAfter               | -0.0054815006  |
| LossBefore              | -7.0230353e-06 |
| Model-TimeModelFit      | 31.3           |
| ModelSampler-n_times... | 1440000        |
| Policy-AverageAbsPol... | 1.4348289      |
| Policy-AverageDiscou... | 27.7           |
| Policy-AveragePolicyStd | 0.48877668     |
| Policy-AverageReturn    | 52.5           |
| Policy-MaxReturn        | 68             |
| Policy-MinReturn        | 38.3           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 7.39           |
| Policy-TimeAlgoOpt      | 0.579          |
| Policy-TimeSampleProc   | 0.706          |
| Policy-TimeSampling     | 1.96           |
| Policy-TimeStep         | 3.33           |
| Time                    | 965            |
| n_timesteps             | 36000          |
--------------------------------------------

 ---------------- Iteration 36 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 360 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 361 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 362 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 363 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 364 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 365 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 366 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 367 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 368 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 369 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.392          |
| Data-EnvSampler-Poli... | 0.902          |
| Data-EnvTrajs-Averag... | 45.6           |
| Data-EnvTrajs-MaxReturn | 52.7           |
| Data-EnvTrajs-MinReturn | 34.1           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 6.36           |
| Data-TimeEnvSampleProc  | 0.00103        |
| Data-TimeEnvSampling    | 1.33           |
| Iteration               | 36             |
| ItrTime                 | 34.7           |
| LossAfter               | -0.0028404186  |
| LossBefore              | -6.7441783e-06 |
| Model-TimeModelFit      | 30.8           |
| ModelSampler-n_times... | 1480000        |
| Policy-AverageAbsPol... | 1.4199412      |
| Policy-AverageDiscou... | 29.6           |
| Policy-AveragePolicyStd | 0.47542775     |
| Policy-AverageReturn    | 62.4           |
| Policy-MaxReturn        | 69.1           |
| Policy-MinReturn        | 53.3           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 4.89           |
| Policy-TimeAlgoOpt      | 0.57           |
| Policy-TimeSampleProc   | 0.421          |
| Policy-TimeSampling     | 1.53           |
| Policy-TimeStep         | 2.56           |
| Time                    | 1e+03          |
| n_timesteps             | 37000          |
--------------------------------------------

 ---------------- Iteration 37 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 370 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 371 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 372 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 373 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 374 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 375 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 376 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 377 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 378 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 379 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.372          |
| Data-EnvSampler-Poli... | 0.853          |
| Data-EnvTrajs-Averag... | 54.1           |
| Data-EnvTrajs-MaxReturn | 56.1           |
| Data-EnvTrajs-MinReturn | 52.6           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 1.26           |
| Data-TimeEnvSampleProc  | 0.00113        |
| Data-TimeEnvSampling    | 1.27           |
| Iteration               | 37             |
| ItrTime                 | 35.1           |
| LossAfter               | -0.0034848957  |
| LossBefore              | -6.7227384e-06 |
| Model-TimeModelFit      | 31.4           |
| ModelSampler-n_times... | 1520000        |
| Policy-AverageAbsPol... | 1.4953213      |
| Policy-AverageDiscou... | 36.1           |
| Policy-AveragePolicyStd | 0.47475153     |
| Policy-AverageReturn    | 78.2           |
| Policy-MaxReturn        | 92.7           |
| Policy-MinReturn        | 23.5           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 16.4           |
| Policy-TimeAlgoOpt      | 0.549          |
| Policy-TimeSampleProc   | 0.379          |
| Policy-TimeSampling     | 1.43           |
| Policy-TimeStep         | 2.4            |
| Time                    | 1.03e+03       |
| n_timesteps             | 38000          |
--------------------------------------------

 ---------------- Iteration 38 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 380 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 381 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 382 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 383 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 384 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 385 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 386 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 387 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 388 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 389 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.35          |
| Data-EnvSampler-Poli... | 0.892         |
| Data-EnvTrajs-Averag... | 59.7          |
| Data-EnvTrajs-MaxReturn | 62.2          |
| Data-EnvTrajs-MinReturn | 56            |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 2.32          |
| Data-TimeEnvSampleProc  | 0.000951      |
| Data-TimeEnvSampling    | 1.27          |
| Iteration               | 38            |
| ItrTime                 | 36.3          |
| LossAfter               | -0.0029581413 |
| LossBefore              | -6.593211e-06 |
| Model-TimeModelFit      | 32.5          |
| ModelSampler-n_times... | 1560000       |
| Policy-AverageAbsPol... | 1.3991328     |
| Policy-AverageDiscou... | 30.2          |
| Policy-AveragePolicyStd | 0.46843314    |
| Policy-AverageReturn    | 61.2          |
| Policy-MaxReturn        | 77.2          |
| Policy-MinReturn        | 30.4          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 11.6          |
| Policy-TimeAlgoOpt      | 0.557         |
| Policy-TimeSampleProc   | 0.412         |
| Policy-TimeSampling     | 1.56          |
| Policy-TimeStep         | 2.57          |
| Time                    | 1.07e+03      |
| n_timesteps             | 39000         |
-------------------------------------------

 ---------------- Iteration 39 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 390 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 391 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 392 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 393 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 394 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 395 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 396 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 397 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 398 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 399 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.38          |
| Data-EnvSampler-Poli... | 0.94          |
| Data-EnvTrajs-Averag... | 56.6          |
| Data-EnvTrajs-MaxReturn | 59.8          |
| Data-EnvTrajs-MinReturn | 52.6          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 2.49          |
| Data-TimeEnvSampleProc  | 0.00104       |
| Data-TimeEnvSampling    | 1.36          |
| Iteration               | 39            |
| ItrTime                 | 35.8          |
| LossAfter               | -0.002312376  |
| LossBefore              | -6.351952e-06 |
| Model-TimeModelFit      | 31.3          |
| ModelSampler-n_times... | 1600000       |
| Policy-AverageAbsPol... | 1.4016995     |
| Policy-AverageDiscou... | 33            |
| Policy-AveragePolicyStd | 0.45692626    |
| Policy-AverageReturn    | 63.4          |
| Policy-MaxReturn        | 76.1          |
| Policy-MinReturn        | 42.7          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 8.95          |
| Policy-TimeAlgoOpt      | 0.602         |
| Policy-TimeSampleProc   | 0.584         |
| Policy-TimeSampling     | 1.88          |
| Policy-TimeStep         | 3.1           |
| Time                    | 1.11e+03      |
| n_timesteps             | 40000         |
-------------------------------------------

 ---------------- Iteration 40 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 400 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 401 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 402 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 403 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 404 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 405 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 406 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 407 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 408 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 409 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.473         |
| Data-EnvSampler-Poli... | 1.38          |
| Data-EnvTrajs-Averag... | 50.7          |
| Data-EnvTrajs-MaxReturn | 57            |
| Data-EnvTrajs-MinReturn | 43.3          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 4.59          |
| Data-TimeEnvSampleProc  | 0.00123       |
| Data-TimeEnvSampling    | 1.92          |
| Iteration               | 40            |
| ItrTime                 | 35.9          |
| LossAfter               | -0.0021994777 |
| LossBefore              | -6.080603e-06 |
| Model-TimeModelFit      | 31.6          |
| ModelSampler-n_times... | 1640000       |
| Policy-AverageAbsPol... | 1.4298643     |
| Policy-AverageDiscou... | 20            |
| Policy-AveragePolicyStd | 0.4458652     |
| Policy-AverageReturn    | 35.7          |
| Policy-MaxReturn        | 45.3          |
| Policy-MinReturn        | 20.9          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 6.46          |
| Policy-TimeAlgoOpt      | 0.512         |
| Policy-TimeSampleProc   | 0.333         |
| Policy-TimeSampling     | 1.49          |
| Policy-TimeStep         | 2.36          |
| Time                    | 1.14e+03      |
| n_timesteps             | 41000         |
-------------------------------------------

 ---------------- Iteration 41 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 410 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 411 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 412 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 413 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 414 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 415 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 416 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 417 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 418 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 419 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.448         |
| Data-EnvSampler-Poli... | 1.27          |
| Data-EnvTrajs-Averag... | 52.2          |
| Data-EnvTrajs-MaxReturn | 53            |
| Data-EnvTrajs-MinReturn | 51            |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 0.773         |
| Data-TimeEnvSampleProc  | 0.00619       |
| Data-TimeEnvSampling    | 1.77          |
| Iteration               | 41            |
| ItrTime                 | 37.7          |
| LossAfter               | -0.004693189  |
| LossBefore              | -5.801227e-06 |
| Model-TimeModelFit      | 32.9          |
| ModelSampler-n_times... | 1680000       |
| Policy-AverageAbsPol... | 1.329404      |
| Policy-AverageDiscou... | 22            |
| Policy-AveragePolicyStd | 0.43317384    |
| Policy-AverageReturn    | 39.5          |
| Policy-MaxReturn        | 53.6          |
| Policy-MinReturn        | 0.308         |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 12.1          |
| Policy-TimeAlgoOpt      | 0.6           |
| Policy-TimeSampleProc   | 0.521         |
| Policy-TimeSampling     | 1.89          |
| Policy-TimeStep         | 3.03          |
| Time                    | 1.18e+03      |
| n_timesteps             | 42000         |
-------------------------------------------

 ---------------- Iteration 42 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 420 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 421 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 422 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 423 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 424 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 425 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 426 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 427 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 428 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 429 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.364        |
| Data-EnvSampler-Poli... | 0.713        |
| Data-EnvTrajs-Averag... | 51.5         |
| Data-EnvTrajs-MaxReturn | 54.1         |
| Data-EnvTrajs-MinReturn | 45.3         |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 3.17         |
| Data-TimeEnvSampleProc  | 0.00102      |
| Data-TimeEnvSampling    | 1.11         |
| Iteration               | 42           |
| ItrTime                 | 35.8         |
| LossAfter               | -0.004385279 |
| LossBefore              | -5.66543e-06 |
| Model-TimeModelFit      | 32           |
| ModelSampler-n_times... | 1720000      |
| Policy-AverageAbsPol... | 1.3551004    |
| Policy-AverageDiscou... | 36.4         |
| Policy-AveragePolicyStd | 0.42692566   |
| Policy-AverageReturn    | 73.8         |
| Policy-MaxReturn        | 84.5         |
| Policy-MinReturn        | 62.2         |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 6.79         |
| Policy-TimeAlgoOpt      | 0.563        |
| Policy-TimeSampleProc   | 0.417        |
| Policy-TimeSampling     | 1.67         |
| Policy-TimeStep         | 2.7          |
| Time                    | 1.22e+03     |
| n_timesteps             | 43000        |
------------------------------------------

 ---------------- Iteration 43 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 430 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 431 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 432 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 433 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 434 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 435 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 436 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 437 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 438 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 439 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.356         |
| Data-EnvSampler-Poli... | 0.797         |
| Data-EnvTrajs-Averag... | 56.4          |
| Data-EnvTrajs-MaxReturn | 60.1          |
| Data-EnvTrajs-MinReturn | 54            |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 2.18          |
| Data-TimeEnvSampleProc  | 0.000627      |
| Data-TimeEnvSampling    | 1.2           |
| Iteration               | 43            |
| ItrTime                 | 37            |
| LossAfter               | -0.0036704058 |
| LossBefore              | -5.293053e-06 |
| Model-TimeModelFit      | 32.4          |
| ModelSampler-n_times... | 1760000       |
| Policy-AverageAbsPol... | 1.3865707     |
| Policy-AverageDiscou... | 33.3          |
| Policy-AveragePolicyStd | 0.41174492    |
| Policy-AverageReturn    | 66.9          |
| Policy-MaxReturn        | 75            |
| Policy-MinReturn        | 51.9          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 5.55          |
| Policy-TimeAlgoOpt      | 0.633         |
| Policy-TimeSampleProc   | 0.607         |
| Policy-TimeSampling     | 2.14          |
| Policy-TimeStep         | 3.48          |
| Time                    | 1.25e+03      |
| n_timesteps             | 44000         |
-------------------------------------------

 ---------------- Iteration 44 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 440 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 441 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 442 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 443 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 444 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 445 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 446 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 447 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 448 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 449 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.348          |
| Data-EnvSampler-Poli... | 0.774          |
| Data-EnvTrajs-Averag... | 53.7           |
| Data-EnvTrajs-MaxReturn | 59.5           |
| Data-EnvTrajs-MinReturn | 47.9           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 3.73           |
| Data-TimeEnvSampleProc  | 0.000677       |
| Data-TimeEnvSampling    | 1.16           |
| Iteration               | 44             |
| ItrTime                 | 36             |
| LossAfter               | -0.0034526344  |
| LossBefore              | -5.0693775e-06 |
| Model-TimeModelFit      | 32.2           |
| ModelSampler-n_times... | 1800000        |
| Policy-AverageAbsPol... | 1.417992       |
| Policy-AverageDiscou... | 38.2           |
| Policy-AveragePolicyStd | 0.40308502     |
| Policy-AverageReturn    | 78.3           |
| Policy-MaxReturn        | 90.4           |
| Policy-MinReturn        | 59.2           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 6.68           |
| Policy-TimeAlgoOpt      | 0.638          |
| Policy-TimeSampleProc   | 0.291          |
| Policy-TimeSampling     | 1.74           |
| Policy-TimeStep         | 2.69           |
| Time                    | 1.29e+03       |
| n_timesteps             | 45000          |
--------------------------------------------

 ---------------- Iteration 45 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 450 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 451 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 452 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 453 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 454 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 455 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 456 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 457 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 458 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 459 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.362          |
| Data-EnvSampler-Poli... | 0.72           |
| Data-EnvTrajs-Averag... | 53.8           |
| Data-EnvTrajs-MaxReturn | 58.5           |
| Data-EnvTrajs-MinReturn | 45.4           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 4.42           |
| Data-TimeEnvSampleProc  | 0.000954       |
| Data-TimeEnvSampling    | 1.12           |
| Iteration               | 45             |
| ItrTime                 | 38.5           |
| LossAfter               | -0.0033264107  |
| LossBefore              | -4.8020406e-06 |
| Model-TimeModelFit      | 34.8           |
| ModelSampler-n_times... | 1840000        |
| Policy-AverageAbsPol... | 1.4012882      |
| Policy-AverageDiscou... | 31.4           |
| Policy-AveragePolicyStd | 0.39138147     |
| Policy-AverageReturn    | 59.2           |
| Policy-MaxReturn        | 68.3           |
| Policy-MinReturn        | 45.4           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 5.44           |
| Policy-TimeAlgoOpt      | 0.556          |
| Policy-TimeSampleProc   | 0.357          |
| Policy-TimeSampling     | 1.65           |
| Policy-TimeStep         | 2.59           |
| Time                    | 1.33e+03       |
| n_timesteps             | 46000          |
--------------------------------------------

 ---------------- Iteration 46 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 460 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 461 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 462 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 463 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 464 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 465 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 466 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 467 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 468 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 469 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.375         |
| Data-EnvSampler-Poli... | 0.782         |
| Data-EnvTrajs-Averag... | 45.3          |
| Data-EnvTrajs-MaxReturn | 54.8          |
| Data-EnvTrajs-MinReturn | 30.6          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 8.16          |
| Data-TimeEnvSampleProc  | 0.00102       |
| Data-TimeEnvSampling    | 1.19          |
| Iteration               | 46            |
| ItrTime                 | 37.3          |
| LossAfter               | -0.004727648  |
| LossBefore              | -4.781393e-06 |
| Model-TimeModelFit      | 32.8          |
| ModelSampler-n_times... | 1880000       |
| Policy-AverageAbsPol... | 1.5395538     |
| Policy-AverageDiscou... | 41.6          |
| Policy-AveragePolicyStd | 0.3903225     |
| Policy-AverageReturn    | 98.5          |
| Policy-MaxReturn        | 115           |
| Policy-MinReturn        | 80            |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 9.28          |
| Policy-TimeAlgoOpt      | 0.647         |
| Policy-TimeSampleProc   | 0.547         |
| Policy-TimeSampling     | 2.06          |
| Policy-TimeStep         | 3.33          |
| Time                    | 1.37e+03      |
| n_timesteps             | 47000         |
-------------------------------------------

 ---------------- Iteration 47 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 470 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 471 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 472 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 473 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 474 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 475 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 476 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 477 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 478 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 479 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.383          |
| Data-EnvSampler-Poli... | 0.92           |
| Data-EnvTrajs-Averag... | 60.4           |
| Data-EnvTrajs-MaxReturn | 62.8           |
| Data-EnvTrajs-MinReturn | 53.6           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 3.43           |
| Data-TimeEnvSampleProc  | 0.000842       |
| Data-TimeEnvSampling    | 1.34           |
| Iteration               | 47             |
| ItrTime                 | 36.8           |
| LossAfter               | -0.0019072194  |
| LossBefore              | -4.4560534e-06 |
| Model-TimeModelFit      | 32.7           |
| ModelSampler-n_times... | 1920000        |
| Policy-AverageAbsPol... | 1.4193563      |
| Policy-AverageDiscou... | 37.6           |
| Policy-AveragePolicyStd | 0.37839276     |
| Policy-AverageReturn    | 80.5           |
| Policy-MaxReturn        | 102            |
| Policy-MinReturn        | 72.2           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 7.37           |
| Policy-TimeAlgoOpt      | 0.515          |
| Policy-TimeSampleProc   | 0.655          |
| Policy-TimeSampling     | 1.6            |
| Policy-TimeStep         | 2.82           |
| Time                    | 1.4e+03        |
| n_timesteps             | 48000          |
--------------------------------------------

 ---------------- Iteration 48 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 480 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 481 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 482 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 483 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 484 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 485 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 486 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 487 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 488 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 489 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.319          |
| Data-EnvSampler-Poli... | 0.589          |
| Data-EnvTrajs-Averag... | 55.6           |
| Data-EnvTrajs-MaxReturn | 63.6           |
| Data-EnvTrajs-MinReturn | 44.2           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 7.98           |
| Data-TimeEnvSampleProc  | 0.000703       |
| Data-TimeEnvSampling    | 0.934          |
| Iteration               | 48             |
| ItrTime                 | 37             |
| LossAfter               | -0.0036137123  |
| LossBefore              | -4.1575154e-06 |
| Model-TimeModelFit      | 33.1           |
| ModelSampler-n_times... | 1960000        |
| Policy-AverageAbsPol... | 1.43572        |
| Policy-AverageDiscou... | 27.4           |
| Policy-AveragePolicyStd | 0.36736414     |
| Policy-AverageReturn    | 55.7           |
| Policy-MaxReturn        | 65.8           |
| Policy-MinReturn        | 28.7           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 9.42           |
| Policy-TimeAlgoOpt      | 0.664          |
| Policy-TimeSampleProc   | 0.41           |
| Policy-TimeSampling     | 1.81           |
| Policy-TimeStep         | 2.9            |
| Time                    | 1.44e+03       |
| n_timesteps             | 49000          |
--------------------------------------------

 ---------------- Iteration 49 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 490 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 491 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 492 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 493 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 494 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 495 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 496 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 497 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 498 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 499 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.386          |
| Data-EnvSampler-Poli... | 0.81           |
| Data-EnvTrajs-Averag... | 50.5           |
| Data-EnvTrajs-MaxReturn | 57.1           |
| Data-EnvTrajs-MinReturn | 40.8           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 6.11           |
| Data-TimeEnvSampleProc  | 0.0012         |
| Data-TimeEnvSampling    | 1.23           |
| Iteration               | 49             |
| ItrTime                 | 37.9           |
| LossAfter               | -0.0011015582  |
| LossBefore              | -3.9185306e-06 |
| Model-TimeModelFit      | 34.1           |
| ModelSampler-n_times... | 2000000        |
| Policy-AverageAbsPol... | 1.415711       |
| Policy-AverageDiscou... | 29.7           |
| Policy-AveragePolicyStd | 0.35863563     |
| Policy-AverageReturn    | 58             |
| Policy-MaxReturn        | 68.9           |
| Policy-MinReturn        | 45.9           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 6.78           |
| Policy-TimeAlgoOpt      | 0.546          |
| Policy-TimeSampleProc   | 0.349          |
| Policy-TimeSampling     | 1.64           |
| Policy-TimeStep         | 2.55           |
| Time                    | 1.48e+03       |
| n_timesteps             | 50000          |
--------------------------------------------

 ---------------- Iteration 50 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 500 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 501 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 502 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 503 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 504 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 505 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 506 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 507 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 508 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 509 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.438         |
| Data-EnvSampler-Poli... | 0.96          |
| Data-EnvTrajs-Averag... | 51.3          |
| Data-EnvTrajs-MaxReturn | 56.2          |
| Data-EnvTrajs-MinReturn | 47            |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 3.34          |
| Data-TimeEnvSampleProc  | 0.0011        |
| Data-TimeEnvSampling    | 1.43          |
| Iteration               | 50            |
| ItrTime                 | 38.1          |
| LossAfter               | -0.0032467425 |
| LossBefore              | -3.911217e-06 |
| Model-TimeModelFit      | 34.3          |
| ModelSampler-n_times... | 2040000       |
| Policy-AverageAbsPol... | 1.3609959     |
| Policy-AverageDiscou... | 33.7          |
| Policy-AveragePolicyStd | 0.35985258    |
| Policy-AverageReturn    | 64.3          |
| Policy-MaxReturn        | 72            |
| Policy-MinReturn        | 56.9          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 3.98          |
| Policy-TimeAlgoOpt      | 0.547         |
| Policy-TimeSampleProc   | 0.347         |
| Policy-TimeSampling     | 1.45          |
| Policy-TimeStep         | 2.36          |
| Time                    | 1.52e+03      |
| n_timesteps             | 51000         |
-------------------------------------------
Training finished
