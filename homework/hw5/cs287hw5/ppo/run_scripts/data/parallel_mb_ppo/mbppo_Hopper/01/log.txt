Logging to /home/vincent/workplace/CS_287_-_Advanced_Robotics/homework/hw5/cs287hw5/ppo/run_scripts/data/parallel_mb_ppo/mbppo_Hopper//01

 ---------------- Iteration 0 ----------------
Obtaining random samples from the environment...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 0 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 1 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 2 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 3 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 4 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 5 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 6 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 7 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 8 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 9 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.235          |
| Data-EnvSampler-Poli... | 0.0521         |
| Data-EnvTrajs-Averag... | -293           |
| Data-EnvTrajs-MaxReturn | -201           |
| Data-EnvTrajs-MinReturn | -341           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 52.5           |
| Data-TimeEnvSampleProc  | 0.000612       |
| Data-TimeEnvSampling    | 0.302          |
| Iteration               | 0              |
| ItrTime                 | 10.7           |
| LossAfter               | -0.00410442    |
| LossBefore              | -1.3755912e-05 |
| Model-TimeModelFit      | 3.61           |
| ModelSampler-n_times... | 40000          |
| Policy-AverageAbsPol... | 0.32378218     |
| Policy-AverageDiscou... | -1.84e+06      |
| Policy-AveragePolicyStd | 0.95809245     |
| Policy-AverageReturn    | -5.01e+06      |
| Policy-MaxReturn        | -3.64e+06      |
| Policy-MinReturn        | -5.49e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 5.93e+05       |
| Policy-TimeAlgoOpt      | 1.11           |
| Policy-TimeSampleProc   | 0.449          |
| Policy-TimeSampling     | 5.21           |
| Policy-TimeStep         | 6.81           |
| Time                    | 10.7           |
| n_timesteps             | 1000           |
--------------------------------------------

 ---------------- Iteration 1 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 10 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 11 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 12 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 13 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 14 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 15 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 16 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 17 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 18 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 19 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.334          |
| Data-EnvSampler-Poli... | 0.633          |
| Data-EnvTrajs-Averag... | -269           |
| Data-EnvTrajs-MaxReturn | 71.2           |
| Data-EnvTrajs-MinReturn | -450           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 182            |
| Data-TimeEnvSampleProc  | 0.000587       |
| Data-TimeEnvSampling    | 0.994          |
| Iteration               | 1              |
| ItrTime                 | 8.7            |
| LossAfter               | -0.004484682   |
| LossBefore              | -1.3549013e-05 |
| Model-TimeModelFit      | 4.77           |
| ModelSampler-n_times... | 80000          |
| Policy-AverageAbsPol... | 0.33194035     |
| Policy-AverageDiscou... | -2.06e+06      |
| Policy-AveragePolicyStd | 0.94040614     |
| Policy-AverageReturn    | -5.35e+06      |
| Policy-MaxReturn        | -5.3e+06       |
| Policy-MinReturn        | -5.4e+06       |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 2.23e+04       |
| Policy-TimeAlgoOpt      | 0.667          |
| Policy-TimeSampleProc   | 0.46           |
| Policy-TimeSampling     | 1.77           |
| Policy-TimeStep         | 2.93           |
| Time                    | 19.5           |
| n_timesteps             | 2000           |
--------------------------------------------

 ---------------- Iteration 2 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 20 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 21 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 22 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 23 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 24 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 25 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 26 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 27 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 28 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 29 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.388          |
| Data-EnvSampler-Poli... | 0.566          |
| Data-EnvTrajs-Averag... | -384           |
| Data-EnvTrajs-MaxReturn | -314           |
| Data-EnvTrajs-MinReturn | -438           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 48.2           |
| Data-TimeEnvSampleProc  | 0.000886       |
| Data-TimeEnvSampling    | 0.983          |
| Iteration               | 2              |
| ItrTime                 | 11.7           |
| LossAfter               | -0.0035396086  |
| LossBefore              | -1.3461925e-05 |
| Model-TimeModelFit      | 7.68           |
| ModelSampler-n_times... | 120000         |
| Policy-AverageAbsPol... | 0.50287163     |
| Policy-AverageDiscou... | -1.35e+06      |
| Policy-AveragePolicyStd | 0.92910635     |
| Policy-AverageReturn    | -4.31e+06      |
| Policy-MaxReturn        | -3.62e+06      |
| Policy-MinReturn        | -4.9e+06       |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 4.42e+05       |
| Policy-TimeAlgoOpt      | 0.653          |
| Policy-TimeSampleProc   | 0.357          |
| Policy-TimeSampling     | 1.95           |
| Policy-TimeStep         | 3              |
| Time                    | 31.2           |
| n_timesteps             | 3000           |
--------------------------------------------

 ---------------- Iteration 3 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 30 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 31 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 32 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 33 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 34 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 35 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 36 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 37 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 38 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 39 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.333          |
| Data-EnvSampler-Poli... | 0.553          |
| Data-EnvTrajs-Averag... | -249           |
| Data-EnvTrajs-MaxReturn | -107           |
| Data-EnvTrajs-MinReturn | -501           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 135            |
| Data-TimeEnvSampleProc  | 0.000765       |
| Data-TimeEnvSampling    | 0.914          |
| Iteration               | 3              |
| ItrTime                 | 13.1           |
| LossAfter               | -0.005981165   |
| LossBefore              | -1.3178409e-05 |
| Model-TimeModelFit      | 9.22           |
| ModelSampler-n_times... | 160000         |
| Policy-AverageAbsPol... | 0.5639526      |
| Policy-AverageDiscou... | -1.07e+06      |
| Policy-AveragePolicyStd | 0.90289646     |
| Policy-AverageReturn    | -3.69e+06      |
| Policy-MaxReturn        | -2e+06         |
| Policy-MinReturn        | -4.71e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 9.27e+05       |
| Policy-TimeAlgoOpt      | 0.647          |
| Policy-TimeSampleProc   | 0.394          |
| Policy-TimeSampling     | 1.88           |
| Policy-TimeStep         | 2.95           |
| Time                    | 44.3           |
| n_timesteps             | 4000           |
--------------------------------------------

 ---------------- Iteration 4 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 40 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 41 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 42 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 43 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 44 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 45 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 46 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 47 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 48 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 49 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
---------------------------------------------
| Data-EnvSampler-EnvE... | 0.307           |
| Data-EnvSampler-Poli... | 0.459           |
| Data-EnvTrajs-Averag... | -118            |
| Data-EnvTrajs-MaxReturn | 182             |
| Data-EnvTrajs-MinReturn | -304            |
| Data-EnvTrajs-NumTrajs  | 5               |
| Data-EnvTrajs-StdReturn | 177             |
| Data-TimeEnvSampleProc  | 0.000614        |
| Data-TimeEnvSampling    | 0.791           |
| Iteration               | 4               |
| ItrTime                 | 14.5            |
| LossAfter               | -0.0053269183   |
| LossBefore              | -1.30641765e-05 |
| Model-TimeModelFit      | 11              |
| ModelSampler-n_times... | 200000          |
| Policy-AverageAbsPol... | 0.94868         |
| Policy-AverageDiscou... | -4.82e+05       |
| Policy-AveragePolicyStd | 0.89310956      |
| Policy-AverageReturn    | -2.05e+06       |
| Policy-MaxReturn        | -561            |
| Policy-MinReturn        | -4.03e+06       |
| Policy-NumTrajs         | 20              |
| Policy-StdReturn        | 1.29e+06        |
| Policy-TimeAlgoOpt      | 0.591           |
| Policy-TimeSampleProc   | 0.521           |
| Policy-TimeSampling     | 1.61            |
| Policy-TimeStep         | 2.75            |
| Time                    | 58.9            |
| n_timesteps             | 5000            |
---------------------------------------------

 ---------------- Iteration 5 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 50 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 51 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 52 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 53 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 54 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 55 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 56 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 57 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 58 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 59 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.288          |
| Data-EnvSampler-Poli... | 0.42           |
| Data-EnvTrajs-Averag... | -124           |
| Data-EnvTrajs-MaxReturn | -56.4          |
| Data-EnvTrajs-MinReturn | -260           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 72.9           |
| Data-TimeEnvSampleProc  | 0.00062        |
| Data-TimeEnvSampling    | 0.731          |
| Iteration               | 5              |
| ItrTime                 | 16.3           |
| LossAfter               | -0.0066060163  |
| LossBefore              | -1.2925328e-05 |
| Model-TimeModelFit      | 12.7           |
| ModelSampler-n_times... | 240000         |
| Policy-AverageAbsPol... | 0.8409564      |
| Policy-AverageDiscou... | -1.2e+06       |
| Policy-AveragePolicyStd | 0.8826273      |
| Policy-AverageReturn    | -3.96e+06      |
| Policy-MaxReturn        | -255           |
| Policy-MinReturn        | -4.36e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 9.2e+05        |
| Policy-TimeAlgoOpt      | 0.591          |
| Policy-TimeSampleProc   | 0.59           |
| Policy-TimeSampling     | 1.68           |
| Policy-TimeStep         | 2.89           |
| Time                    | 75.2           |
| n_timesteps             | 6000           |
--------------------------------------------

 ---------------- Iteration 6 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 60 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 61 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 62 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 63 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 64 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 65 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 66 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 67 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 68 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 69 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.398          |
| Data-EnvSampler-Poli... | 0.634          |
| Data-EnvTrajs-Averag... | -309           |
| Data-EnvTrajs-MaxReturn | -109           |
| Data-EnvTrajs-MinReturn | -477           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 118            |
| Data-TimeEnvSampleProc  | 0.00112        |
| Data-TimeEnvSampling    | 1.06           |
| Iteration               | 6              |
| ItrTime                 | 19.8           |
| LossAfter               | -0.006358407   |
| LossBefore              | -1.2792448e-05 |
| Model-TimeModelFit      | 15.9           |
| ModelSampler-n_times... | 280000         |
| Policy-AverageAbsPol... | 0.90596807     |
| Policy-AverageDiscou... | -2.95e+05      |
| Policy-AveragePolicyStd | 0.8705782      |
| Policy-AverageReturn    | -1.36e+06      |
| Policy-MaxReturn        | 77.7           |
| Policy-MinReturn        | -3.31e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.25e+06       |
| Policy-TimeAlgoOpt      | 0.562          |
| Policy-TimeSampleProc   | 0.579          |
| Policy-TimeSampling     | 1.57           |
| Policy-TimeStep         | 2.81           |
| Time                    | 95             |
| n_timesteps             | 7000           |
--------------------------------------------

 ---------------- Iteration 7 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 70 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 71 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 72 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 73 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 74 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 75 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 76 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 77 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 78 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 79 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.403          |
| Data-EnvSampler-Poli... | 0.677          |
| Data-EnvTrajs-Averag... | -311           |
| Data-EnvTrajs-MaxReturn | -123           |
| Data-EnvTrajs-MinReturn | -419           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 120            |
| Data-TimeEnvSampleProc  | 0.00109        |
| Data-TimeEnvSampling    | 1.11           |
| Iteration               | 7              |
| ItrTime                 | 5.38           |
| LossAfter               | -0.006514754   |
| LossBefore              | -1.2705815e-05 |
| Model-TimeModelFit      | 1.34           |
| ModelSampler-n_times... | 320000         |
| Policy-AverageAbsPol... | 1.2154295      |
| Policy-AverageDiscou... | -1.08e+05      |
| Policy-AveragePolicyStd | 0.863646       |
| Policy-AverageReturn    | -4e+05         |
| Policy-MaxReturn        | -762           |
| Policy-MinReturn        | -3.79e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.13e+06       |
| Policy-TimeAlgoOpt      | 0.639          |
| Policy-TimeSampleProc   | 0.351          |
| Policy-TimeSampling     | 1.91           |
| Policy-TimeStep         | 2.93           |
| Time                    | 100            |
| n_timesteps             | 8000           |
--------------------------------------------

 ---------------- Iteration 8 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 80 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 81 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 82 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 83 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 84 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 85 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 86 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 87 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 88 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 89 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.507          |
| Data-EnvSampler-Poli... | 0.911          |
| Data-EnvTrajs-Averag... | -193           |
| Data-EnvTrajs-MaxReturn | 8.71           |
| Data-EnvTrajs-MinReturn | -401           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 136            |
| Data-TimeEnvSampleProc  | 0.00128        |
| Data-TimeEnvSampling    | 1.46           |
| Iteration               | 8              |
| ItrTime                 | 22.9           |
| LossAfter               | -0.008298495   |
| LossBefore              | -1.2724668e-05 |
| Model-TimeModelFit      | 18.7           |
| ModelSampler-n_times... | 360000         |
| Policy-AverageAbsPol... | 1.1994605      |
| Policy-AverageDiscou... | -2.75e+05      |
| Policy-AveragePolicyStd | 0.86323714     |
| Policy-AverageReturn    | -1.28e+06      |
| Policy-MaxReturn        | -510           |
| Policy-MinReturn        | -3.19e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.26e+06       |
| Policy-TimeAlgoOpt      | 0.613          |
| Policy-TimeSampleProc   | 0.319          |
| Policy-TimeSampling     | 1.72           |
| Policy-TimeStep         | 2.67           |
| Time                    | 123            |
| n_timesteps             | 9000           |
--------------------------------------------

 ---------------- Iteration 9 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 90 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 91 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 92 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 93 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 94 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 95 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 96 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 97 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 98 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 99 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.468          |
| Data-EnvSampler-Poli... | 0.878          |
| Data-EnvTrajs-Averag... | -271           |
| Data-EnvTrajs-MaxReturn | -108           |
| Data-EnvTrajs-MinReturn | -470           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 143            |
| Data-TimeEnvSampleProc  | 0.00104        |
| Data-TimeEnvSampling    | 1.4            |
| Iteration               | 9              |
| ItrTime                 | 26.6           |
| LossAfter               | -0.008422801   |
| LossBefore              | -1.2604967e-05 |
| Model-TimeModelFit      | 22.3           |
| ModelSampler-n_times... | 400000         |
| Policy-AverageAbsPol... | 1.0501215      |
| Policy-AverageDiscou... | -5.44e+04      |
| Policy-AveragePolicyStd | 0.85291326     |
| Policy-AverageReturn    | -2.16e+05      |
| Policy-MaxReturn        | -396           |
| Policy-MinReturn        | -3.65e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 8e+05          |
| Policy-TimeAlgoOpt      | 0.664          |
| Policy-TimeSampleProc   | 0.414          |
| Policy-TimeSampling     | 1.73           |
| Policy-TimeStep         | 2.84           |
| Time                    | 150            |
| n_timesteps             | 10000          |
--------------------------------------------

 ---------------- Iteration 10 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...
Stopping DynamicsEnsemble Training since valid_loss_rolling_average decreased

 ---------------- Grad-Step 100 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 101 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 102 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 103 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 104 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 105 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 106 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 107 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 108 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 109 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.492         |
| Data-EnvSampler-Poli... | 0.942         |
| Data-EnvTrajs-Averag... | -211          |
| Data-EnvTrajs-MaxReturn | -179          |
| Data-EnvTrajs-MinReturn | -262          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 28.6          |
| Data-TimeEnvSampleProc  | 0.0192        |
| Data-TimeEnvSampling    | 1.48          |
| Iteration               | 10            |
| ItrTime                 | 5.88          |
| LossAfter               | -0.005959592  |
| LossBefore              | -1.258091e-05 |
| Model-TimeModelFit      | 1.67          |
| ModelSampler-n_times... | 440000        |
| Policy-AverageAbsPol... | 0.9682252     |
| Policy-AverageDiscou... | -3.12e+05     |
| Policy-AveragePolicyStd | 0.85192555    |
| Policy-AverageReturn    | -1.69e+06     |
| Policy-MaxReturn        | -1.2e+06      |
| Policy-MinReturn        | -2.42e+06     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 2.87e+05      |
| Policy-TimeAlgoOpt      | 0.618         |
| Policy-TimeSampleProc   | 0.369         |
| Policy-TimeSampling     | 1.7           |
| Policy-TimeStep         | 2.71          |
| Time                    | 156           |
| n_timesteps             | 11000         |
-------------------------------------------

 ---------------- Iteration 11 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 110 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 111 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 112 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 113 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 114 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 115 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 116 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 117 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 118 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 119 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
---------------------------------------------
| Data-EnvSampler-EnvE... | 0.515           |
| Data-EnvSampler-Poli... | 0.997           |
| Data-EnvTrajs-Averag... | -397            |
| Data-EnvTrajs-MaxReturn | -354            |
| Data-EnvTrajs-MinReturn | -433            |
| Data-EnvTrajs-NumTrajs  | 5               |
| Data-EnvTrajs-StdReturn | 30.9            |
| Data-TimeEnvSampleProc  | 0.00107         |
| Data-TimeEnvSampling    | 1.56            |
| Iteration               | 11              |
| ItrTime                 | 30.3            |
| LossAfter               | -0.008212766    |
| LossBefore              | -1.24500975e-05 |
| Model-TimeModelFit      | 26.3            |
| ModelSampler-n_times... | 480000          |
| Policy-AverageAbsPol... | 0.8458291       |
| Policy-AverageDiscou... | -4.57e+03       |
| Policy-AveragePolicyStd | 0.8430092       |
| Policy-AverageReturn    | -2.92e+04       |
| Policy-MaxReturn        | -763            |
| Policy-MinReturn        | -5.66e+05       |
| Policy-NumTrajs         | 20              |
| Policy-StdReturn        | 1.23e+05        |
| Policy-TimeAlgoOpt      | 0.583           |
| Policy-TimeSampleProc   | 0.317           |
| Policy-TimeSampling     | 1.58            |
| Policy-TimeStep         | 2.5             |
| Time                    | 186             |
| n_timesteps             | 12000           |
---------------------------------------------

 ---------------- Iteration 12 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 120 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 121 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 122 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 123 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 124 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 125 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 126 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 127 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 128 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 129 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.49           |
| Data-EnvSampler-Poli... | 0.939          |
| Data-EnvTrajs-Averag... | -448           |
| Data-EnvTrajs-MaxReturn | -420           |
| Data-EnvTrajs-MinReturn | -476           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 18.3           |
| Data-TimeEnvSampleProc  | 0.00101        |
| Data-TimeEnvSampling    | 1.47           |
| Iteration               | 12             |
| ItrTime                 | 31.5           |
| LossAfter               | -0.006512361   |
| LossBefore              | -1.2321847e-05 |
| Model-TimeModelFit      | 27.6           |
| ModelSampler-n_times... | 520000         |
| Policy-AverageAbsPol... | 0.89506155     |
| Policy-AverageDiscou... | -76.4          |
| Policy-AveragePolicyStd | 0.828632       |
| Policy-AverageReturn    | -203           |
| Policy-MaxReturn        | 1.06           |
| Policy-MinReturn        | -410           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 93.1           |
| Policy-TimeAlgoOpt      | 0.575          |
| Policy-TimeSampleProc   | 0.335          |
| Policy-TimeSampling     | 1.49           |
| Policy-TimeStep         | 2.42           |
| Time                    | 218            |
| n_timesteps             | 13000          |
--------------------------------------------

 ---------------- Iteration 13 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 130 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 131 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 132 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 133 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 134 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 135 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 136 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 137 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 138 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 139 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.44           |
| Data-EnvSampler-Poli... | 0.838          |
| Data-EnvTrajs-Averag... | -347           |
| Data-EnvTrajs-MaxReturn | -224           |
| Data-EnvTrajs-MinReturn | -467           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 86.4           |
| Data-TimeEnvSampleProc  | 0.000949       |
| Data-TimeEnvSampling    | 1.32           |
| Iteration               | 13             |
| ItrTime                 | 32             |
| LossAfter               | -0.0093069505  |
| LossBefore              | -1.2202009e-05 |
| Model-TimeModelFit      | 27.9           |
| ModelSampler-n_times... | 560000         |
| Policy-AverageAbsPol... | 0.71844554     |
| Policy-AverageDiscou... | -5.23e+03      |
| Policy-AveragePolicyStd | 0.8218792      |
| Policy-AverageReturn    | -3.58e+04      |
| Policy-MaxReturn        | 424            |
| Policy-MinReturn        | -5.84e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.28e+05       |
| Policy-TimeAlgoOpt      | 0.689          |
| Policy-TimeSampleProc   | 0.396          |
| Policy-TimeSampling     | 1.74           |
| Policy-TimeStep         | 2.85           |
| Time                    | 250            |
| n_timesteps             | 14000          |
--------------------------------------------

 ---------------- Iteration 14 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 140 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 141 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 142 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 143 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 144 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 145 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 146 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 147 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 148 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 149 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.465          |
| Data-EnvSampler-Poli... | 0.87           |
| Data-EnvTrajs-Averag... | -414           |
| Data-EnvTrajs-MaxReturn | -344           |
| Data-EnvTrajs-MinReturn | -452           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 41.9           |
| Data-TimeEnvSampleProc  | 0.000903       |
| Data-TimeEnvSampling    | 1.37           |
| Iteration               | 14             |
| ItrTime                 | 31.6           |
| LossAfter               | -0.007903015   |
| LossBefore              | -1.2164823e-05 |
| Model-TimeModelFit      | 27.8           |
| ModelSampler-n_times... | 600000         |
| Policy-AverageAbsPol... | 0.7227378      |
| Policy-AverageDiscou... | -1.26e+04      |
| Policy-AveragePolicyStd | 0.8184685      |
| Policy-AverageReturn    | -8.24e+04      |
| Policy-MaxReturn        | 318            |
| Policy-MinReturn        | -9.21e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 2.21e+05       |
| Policy-TimeAlgoOpt      | 0.598          |
| Policy-TimeSampleProc   | 0.319          |
| Policy-TimeSampling     | 1.46           |
| Policy-TimeStep         | 2.42           |
| Time                    | 281            |
| n_timesteps             | 15000          |
--------------------------------------------

 ---------------- Iteration 15 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 150 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 151 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 152 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 153 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 154 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 155 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 156 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 157 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 158 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 159 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.427          |
| Data-EnvSampler-Poli... | 0.828          |
| Data-EnvTrajs-Averag... | -332           |
| Data-EnvTrajs-MaxReturn | -245           |
| Data-EnvTrajs-MinReturn | -398           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 62.9           |
| Data-TimeEnvSampleProc  | 0.000965       |
| Data-TimeEnvSampling    | 1.29           |
| Iteration               | 15             |
| ItrTime                 | 32.1           |
| LossAfter               | -0.0076720365  |
| LossBefore              | -1.2009884e-05 |
| Model-TimeModelFit      | 28.2           |
| ModelSampler-n_times... | 640000         |
| Policy-AverageAbsPol... | 0.644774       |
| Policy-AverageDiscou... | -2.73e+04      |
| Policy-AveragePolicyStd | 0.8055023      |
| Policy-AverageReturn    | -1.7e+05       |
| Policy-MaxReturn        | 455            |
| Policy-MinReturn        | -1.43e+06      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 3.69e+05       |
| Policy-TimeAlgoOpt      | 0.555          |
| Policy-TimeSampleProc   | 0.479          |
| Policy-TimeSampling     | 1.46           |
| Policy-TimeStep         | 2.54           |
| Time                    | 313            |
| n_timesteps             | 16000          |
--------------------------------------------

 ---------------- Iteration 16 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 160 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 161 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 162 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 163 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 164 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 165 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 166 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 167 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 168 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 169 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
---------------------------------------------
| Data-EnvSampler-EnvE... | 0.475           |
| Data-EnvSampler-Poli... | 0.941           |
| Data-EnvTrajs-Averag... | -441            |
| Data-EnvTrajs-MaxReturn | -406            |
| Data-EnvTrajs-MinReturn | -500            |
| Data-EnvTrajs-NumTrajs  | 5               |
| Data-EnvTrajs-StdReturn | 33.2            |
| Data-TimeEnvSampleProc  | 0.00105         |
| Data-TimeEnvSampling    | 1.45            |
| Iteration               | 16              |
| ItrTime                 | 32.3            |
| LossAfter               | -0.0061392915   |
| LossBefore              | -1.18135595e-05 |
| Model-TimeModelFit      | 28.4            |
| ModelSampler-n_times... | 680000          |
| Policy-AverageAbsPol... | 0.5845039       |
| Policy-AverageDiscou... | -101            |
| Policy-AveragePolicyStd | 0.787411        |
| Policy-AverageReturn    | -208            |
| Policy-MaxReturn        | 86.8            |
| Policy-MinReturn        | -452            |
| Policy-NumTrajs         | 20              |
| Policy-StdReturn        | 128             |
| Policy-TimeAlgoOpt      | 0.577           |
| Policy-TimeSampleProc   | 0.324           |
| Policy-TimeSampling     | 1.49            |
| Policy-TimeStep         | 2.43            |
| Time                    | 346             |
| n_timesteps             | 17000           |
---------------------------------------------

 ---------------- Iteration 17 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 170 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 171 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 172 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 173 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 174 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 175 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 176 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 177 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 178 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 179 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
---------------------------------------------
| Data-EnvSampler-EnvE... | 0.447           |
| Data-EnvSampler-Poli... | 0.842           |
| Data-EnvTrajs-Averag... | -423            |
| Data-EnvTrajs-MaxReturn | -376            |
| Data-EnvTrajs-MinReturn | -451            |
| Data-EnvTrajs-NumTrajs  | 5               |
| Data-EnvTrajs-StdReturn | 25.1            |
| Data-TimeEnvSampleProc  | 0.00112         |
| Data-TimeEnvSampling    | 1.32            |
| Iteration               | 17              |
| ItrTime                 | 33              |
| LossAfter               | -0.009341564    |
| LossBefore              | -1.16857955e-05 |
| Model-TimeModelFit      | 28.9            |
| ModelSampler-n_times... | 720000          |
| Policy-AverageAbsPol... | 0.4528841       |
| Policy-AverageDiscou... | -6.5e+03        |
| Policy-AveragePolicyStd | 0.7791793       |
| Policy-AverageReturn    | -4.22e+04       |
| Policy-MaxReturn        | -44.4           |
| Policy-MinReturn        | -7.64e+05       |
| Policy-NumTrajs         | 20              |
| Policy-StdReturn        | 1.66e+05        |
| Policy-TimeAlgoOpt      | 0.597           |
| Policy-TimeSampleProc   | 0.397           |
| Policy-TimeSampling     | 1.72            |
| Policy-TimeStep         | 2.75            |
| Time                    | 379             |
| n_timesteps             | 18000           |
---------------------------------------------

 ---------------- Iteration 18 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 180 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 181 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 182 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 183 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 184 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 185 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 186 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 187 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 188 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 189 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
---------------------------------------------
| Data-EnvSampler-EnvE... | 0.458           |
| Data-EnvSampler-Poli... | 0.848           |
| Data-EnvTrajs-Averag... | -462            |
| Data-EnvTrajs-MaxReturn | -421            |
| Data-EnvTrajs-MinReturn | -529            |
| Data-EnvTrajs-NumTrajs  | 5               |
| Data-EnvTrajs-StdReturn | 37.1            |
| Data-TimeEnvSampleProc  | 0.000957        |
| Data-TimeEnvSampling    | 1.34            |
| Iteration               | 18              |
| ItrTime                 | 33.6            |
| LossAfter               | -0.004607594    |
| LossBefore              | -1.14701215e-05 |
| Model-TimeModelFit      | 29.2            |
| ModelSampler-n_times... | 760000          |
| Policy-AverageAbsPol... | 0.48886156      |
| Policy-AverageDiscou... | -151            |
| Policy-AveragePolicyStd | 0.7628145       |
| Policy-AverageReturn    | -447            |
| Policy-MaxReturn        | -347            |
| Policy-MinReturn        | -506            |
| Policy-NumTrajs         | 20              |
| Policy-StdReturn        | 42.1            |
| Policy-TimeAlgoOpt      | 0.674           |
| Policy-TimeSampleProc   | 0.491           |
| Policy-TimeSampling     | 1.87            |
| Policy-TimeStep         | 3.07            |
| Time                    | 412             |
| n_timesteps             | 19000           |
---------------------------------------------

 ---------------- Iteration 19 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 190 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 191 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 192 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 193 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 194 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 195 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 196 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 197 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 198 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 199 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.566          |
| Data-EnvSampler-Poli... | 1.04           |
| Data-EnvTrajs-Averag... | -423           |
| Data-EnvTrajs-MaxReturn | -348           |
| Data-EnvTrajs-MinReturn | -498           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 48.7           |
| Data-TimeEnvSampleProc  | 0.000979       |
| Data-TimeEnvSampling    | 1.65           |
| Iteration               | 19             |
| ItrTime                 | 33.1           |
| LossAfter               | -0.003378795   |
| LossBefore              | -1.1211996e-05 |
| Model-TimeModelFit      | 28.8           |
| ModelSampler-n_times... | 800000         |
| Policy-AverageAbsPol... | 0.4839393      |
| Policy-AverageDiscou... | -148           |
| Policy-AveragePolicyStd | 0.74693245     |
| Policy-AverageReturn    | -419           |
| Policy-MaxReturn        | -271           |
| Policy-MinReturn        | -461           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 44             |
| Policy-TimeAlgoOpt      | 0.612          |
| Policy-TimeSampleProc   | 0.407          |
| Policy-TimeSampling     | 1.52           |
| Policy-TimeStep         | 2.57           |
| Time                    | 445            |
| n_timesteps             | 20000          |
--------------------------------------------

 ---------------- Iteration 20 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 200 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 201 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 202 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 203 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 204 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 205 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 206 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 207 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 208 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 209 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.478          |
| Data-EnvSampler-Poli... | 0.867          |
| Data-EnvTrajs-Averag... | -371           |
| Data-EnvTrajs-MaxReturn | -338           |
| Data-EnvTrajs-MinReturn | -407           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 23.6           |
| Data-TimeEnvSampleProc  | 0.000983       |
| Data-TimeEnvSampling    | 1.38           |
| Iteration               | 20             |
| ItrTime                 | 33.4           |
| LossAfter               | -0.009106576   |
| LossBefore              | -1.1072897e-05 |
| Model-TimeModelFit      | 29.6           |
| ModelSampler-n_times... | 840000         |
| Policy-AverageAbsPol... | 0.49056643     |
| Policy-AverageDiscou... | -2.14e+03      |
| Policy-AveragePolicyStd | 0.73202544     |
| Policy-AverageReturn    | -1.45e+04      |
| Policy-MaxReturn        | 246            |
| Policy-MinReturn        | -2.85e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 6.2e+04        |
| Policy-TimeAlgoOpt      | 0.569          |
| Policy-TimeSampleProc   | 0.384          |
| Policy-TimeSampling     | 1.49           |
| Policy-TimeStep         | 2.48           |
| Time                    | 479            |
| n_timesteps             | 21000          |
--------------------------------------------

 ---------------- Iteration 21 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 210 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 211 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 212 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 213 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 214 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 215 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 216 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 217 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 218 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 219 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.467          |
| Data-EnvSampler-Poli... | 0.904          |
| Data-EnvTrajs-Averag... | -358           |
| Data-EnvTrajs-MaxReturn | -311           |
| Data-EnvTrajs-MinReturn | -420           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 35.1           |
| Data-TimeEnvSampleProc  | 0.000716       |
| Data-TimeEnvSampling    | 1.41           |
| Iteration               | 21             |
| ItrTime                 | 32.9           |
| LossAfter               | -0.00882895    |
| LossBefore              | -1.0952007e-05 |
| Model-TimeModelFit      | 29             |
| ModelSampler-n_times... | 880000         |
| Policy-AverageAbsPol... | 0.54912364     |
| Policy-AverageDiscou... | -94.5          |
| Policy-AveragePolicyStd | 0.7251053      |
| Policy-AverageReturn    | -271           |
| Policy-MaxReturn        | 195            |
| Policy-MinReturn        | -591           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 180            |
| Policy-TimeAlgoOpt      | 0.594          |
| Policy-TimeSampleProc   | 0.301          |
| Policy-TimeSampling     | 1.62           |
| Policy-TimeStep         | 2.52           |
| Time                    | 512            |
| n_timesteps             | 22000          |
--------------------------------------------

 ---------------- Iteration 22 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 220 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 221 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 222 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 223 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 224 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 225 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 226 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 227 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 228 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 229 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.555          |
| Data-EnvSampler-Poli... | 0.973          |
| Data-EnvTrajs-Averag... | -364           |
| Data-EnvTrajs-MaxReturn | -275           |
| Data-EnvTrajs-MinReturn | -450           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 71.3           |
| Data-TimeEnvSampleProc  | 0.00364        |
| Data-TimeEnvSampling    | 1.57           |
| Iteration               | 22             |
| ItrTime                 | 32.9           |
| LossAfter               | -0.008775209   |
| LossBefore              | -1.0787455e-05 |
| Model-TimeModelFit      | 28.7           |
| ModelSampler-n_times... | 920000         |
| Policy-AverageAbsPol... | 0.5653345      |
| Policy-AverageDiscou... | -4.05e+03      |
| Policy-AveragePolicyStd | 0.7118768      |
| Policy-AverageReturn    | -2.7e+04       |
| Policy-MaxReturn        | 367            |
| Policy-MinReturn        | -5.32e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 1.16e+05       |
| Policy-TimeAlgoOpt      | 0.608          |
| Policy-TimeSampleProc   | 0.46           |
| Policy-TimeSampling     | 1.48           |
| Policy-TimeStep         | 2.61           |
| Time                    | 545            |
| n_timesteps             | 23000          |
--------------------------------------------

 ---------------- Iteration 23 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 230 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 231 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 232 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 233 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 234 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 235 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 236 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 237 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 238 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 239 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.428          |
| Data-EnvSampler-Poli... | 0.859          |
| Data-EnvTrajs-Averag... | -301           |
| Data-EnvTrajs-MaxReturn | -273           |
| Data-EnvTrajs-MinReturn | -327           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 19.8           |
| Data-TimeEnvSampleProc  | 0.000859       |
| Data-TimeEnvSampling    | 1.32           |
| Iteration               | 23             |
| ItrTime                 | 34             |
| LossAfter               | -0.0064369086  |
| LossBefore              | -1.0565963e-05 |
| Model-TimeModelFit      | 29.8           |
| ModelSampler-n_times... | 960000         |
| Policy-AverageAbsPol... | 0.47466743     |
| Policy-AverageDiscou... | -42.6          |
| Policy-AveragePolicyStd | 0.69725955     |
| Policy-AverageReturn    | -150           |
| Policy-MaxReturn        | 954            |
| Policy-MinReturn        | -468           |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 383            |
| Policy-TimeAlgoOpt      | 0.638          |
| Policy-TimeSampleProc   | 0.473          |
| Policy-TimeSampling     | 1.81           |
| Policy-TimeStep         | 2.95           |
| Time                    | 579            |
| n_timesteps             | 24000          |
--------------------------------------------

 ---------------- Iteration 24 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 240 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 241 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 242 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 243 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 244 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 245 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 246 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 247 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 248 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 249 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.521         |
| Data-EnvSampler-Poli... | 0.91          |
| Data-EnvTrajs-Averag... | -469          |
| Data-EnvTrajs-MaxReturn | -445          |
| Data-EnvTrajs-MinReturn | -488          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 15.2          |
| Data-TimeEnvSampleProc  | 0.00112       |
| Data-TimeEnvSampling    | 1.47          |
| Iteration               | 24            |
| ItrTime                 | 32.6          |
| LossAfter               | -0.0043670144 |
| LossBefore              | -1.044766e-05 |
| Model-TimeModelFit      | 28.6          |
| ModelSampler-n_times... | 1000000       |
| Policy-AverageAbsPol... | 0.47101185    |
| Policy-AverageDiscou... | -59.7         |
| Policy-AveragePolicyStd | 0.6897814     |
| Policy-AverageReturn    | -165          |
| Policy-MaxReturn        | 304           |
| Policy-MinReturn        | -389          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 191           |
| Policy-TimeAlgoOpt      | 0.584         |
| Policy-TimeSampleProc   | 0.341         |
| Policy-TimeSampling     | 1.62          |
| Policy-TimeStep         | 2.57          |
| Time                    | 611           |
| n_timesteps             | 25000         |
-------------------------------------------

 ---------------- Iteration 25 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 250 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 251 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 252 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 253 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 254 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 255 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 256 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 257 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 258 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 259 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.47           |
| Data-EnvSampler-Poli... | 0.848          |
| Data-EnvTrajs-Averag... | -444           |
| Data-EnvTrajs-MaxReturn | -383           |
| Data-EnvTrajs-MinReturn | -497           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 44.4           |
| Data-TimeEnvSampleProc  | 0.00111        |
| Data-TimeEnvSampling    | 1.36           |
| Iteration               | 25             |
| ItrTime                 | 32.6           |
| LossAfter               | -0.010179319   |
| LossBefore              | -1.0419038e-05 |
| Model-TimeModelFit      | 28.7           |
| ModelSampler-n_times... | 1040000        |
| Policy-AverageAbsPol... | 0.5854177      |
| Policy-AverageDiscou... | -2.93e+03      |
| Policy-AveragePolicyStd | 0.6857119      |
| Policy-AverageReturn    | -2.03e+04      |
| Policy-MaxReturn        | 510            |
| Policy-MinReturn        | -3.5e+05       |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 7.65e+04       |
| Policy-TimeAlgoOpt      | 0.558          |
| Policy-TimeSampleProc   | 0.346          |
| Policy-TimeSampling     | 1.59           |
| Policy-TimeStep         | 2.55           |
| Time                    | 644            |
| n_timesteps             | 26000          |
--------------------------------------------

 ---------------- Iteration 26 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 260 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 261 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 262 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 263 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 264 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 265 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 266 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 267 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 268 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 269 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.498          |
| Data-EnvSampler-Poli... | 0.916          |
| Data-EnvTrajs-Averag... | -476           |
| Data-EnvTrajs-MaxReturn | -462           |
| Data-EnvTrajs-MinReturn | -487           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 8.88           |
| Data-TimeEnvSampleProc  | 0.000794       |
| Data-TimeEnvSampling    | 1.46           |
| Iteration               | 26             |
| ItrTime                 | 34.3           |
| LossAfter               | -0.010180182   |
| LossBefore              | -1.0374639e-05 |
| Model-TimeModelFit      | 30.4           |
| ModelSampler-n_times... | 1080000        |
| Policy-AverageAbsPol... | 0.5644366      |
| Policy-AverageDiscou... | -59.8          |
| Policy-AveragePolicyStd | 0.6814019      |
| Policy-AverageReturn    | -140           |
| Policy-MaxReturn        | 582            |
| Policy-MinReturn        | -1.6e+03       |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 410            |
| Policy-TimeAlgoOpt      | 0.596          |
| Policy-TimeSampleProc   | 0.328          |
| Policy-TimeSampling     | 1.49           |
| Policy-TimeStep         | 2.44           |
| Time                    | 678            |
| n_timesteps             | 27000          |
--------------------------------------------

 ---------------- Iteration 27 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 270 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 271 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 272 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 273 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 274 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 275 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 276 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 277 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 278 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 279 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.466          |
| Data-EnvSampler-Poli... | 0.825          |
| Data-EnvTrajs-Averag... | -349           |
| Data-EnvTrajs-MaxReturn | -309           |
| Data-EnvTrajs-MinReturn | -434           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 44.3           |
| Data-TimeEnvSampleProc  | 0.001          |
| Data-TimeEnvSampling    | 1.33           |
| Iteration               | 27             |
| ItrTime                 | 33             |
| LossAfter               | -0.007111636   |
| LossBefore              | -1.0136893e-05 |
| Model-TimeModelFit      | 29.1           |
| ModelSampler-n_times... | 1120000        |
| Policy-AverageAbsPol... | 0.58960253     |
| Policy-AverageDiscou... | -2.53e+04      |
| Policy-AveragePolicyStd | 0.6674929      |
| Policy-AverageReturn    | -1.65e+05      |
| Policy-MaxReturn        | 266            |
| Policy-MinReturn        | -9.57e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 2.92e+05       |
| Policy-TimeAlgoOpt      | 0.594          |
| Policy-TimeSampleProc   | 0.385          |
| Policy-TimeSampling     | 1.54           |
| Policy-TimeStep         | 2.57           |
| Time                    | 711            |
| n_timesteps             | 28000          |
--------------------------------------------

 ---------------- Iteration 28 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 280 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 281 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 282 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 283 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 284 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 285 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 286 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 287 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 288 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 289 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.492          |
| Data-EnvSampler-Poli... | 0.866          |
| Data-EnvTrajs-Averag... | -402           |
| Data-EnvTrajs-MaxReturn | -369           |
| Data-EnvTrajs-MinReturn | -450           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 30.3           |
| Data-TimeEnvSampleProc  | 0.000882       |
| Data-TimeEnvSampling    | 1.39           |
| Iteration               | 28             |
| ItrTime                 | 34.1           |
| LossAfter               | -0.008901454   |
| LossBefore              | -1.0033012e-05 |
| Model-TimeModelFit      | 29.9           |
| ModelSampler-n_times... | 1160000        |
| Policy-AverageAbsPol... | 0.45230502     |
| Policy-AverageDiscou... | -1.93e+03      |
| Policy-AveragePolicyStd | 0.660305       |
| Policy-AverageReturn    | -1.35e+04      |
| Policy-MaxReturn        | 13.8           |
| Policy-MinReturn        | -2.66e+05      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 5.79e+04       |
| Policy-TimeAlgoOpt      | 0.669          |
| Policy-TimeSampleProc   | 0.397          |
| Policy-TimeSampling     | 1.66           |
| Policy-TimeStep         | 2.79           |
| Time                    | 745            |
| n_timesteps             | 29000          |
--------------------------------------------

 ---------------- Iteration 29 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 290 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 291 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 292 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 293 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 294 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 295 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 296 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 297 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 298 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 299 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.468         |
| Data-EnvSampler-Poli... | 0.855         |
| Data-EnvTrajs-Averag... | -329          |
| Data-EnvTrajs-MaxReturn | -308          |
| Data-EnvTrajs-MinReturn | -360          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 17            |
| Data-TimeEnvSampleProc  | 0.000561      |
| Data-TimeEnvSampling    | 1.36          |
| Iteration               | 29            |
| ItrTime                 | 34.2          |
| LossAfter               | -0.00814073   |
| LossBefore              | -9.983461e-06 |
| Model-TimeModelFit      | 30.4          |
| ModelSampler-n_times... | 1200000       |
| Policy-AverageAbsPol... | 0.5239729     |
| Policy-AverageDiscou... | -87.9         |
| Policy-AveragePolicyStd | 0.6578449     |
| Policy-AverageReturn    | -288          |
| Policy-MaxReturn        | 43.9          |
| Policy-MinReturn        | -398          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 96.7          |
| Policy-TimeAlgoOpt      | 0.574         |
| Policy-TimeSampleProc   | 0.349         |
| Policy-TimeSampling     | 1.51          |
| Policy-TimeStep         | 2.46          |
| Time                    | 779           |
| n_timesteps             | 30000         |
-------------------------------------------

 ---------------- Iteration 30 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 300 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 301 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 302 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 303 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 304 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 305 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 306 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 307 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 308 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 309 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.666         |
| Data-EnvSampler-Poli... | 1.3           |
| Data-EnvTrajs-Averag... | -369          |
| Data-EnvTrajs-MaxReturn | -341          |
| Data-EnvTrajs-MinReturn | -401          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 20            |
| Data-TimeEnvSampleProc  | 0.0011        |
| Data-TimeEnvSampling    | 2.01          |
| Iteration               | 30            |
| ItrTime                 | 35.7          |
| LossAfter               | -0.009399229  |
| LossBefore              | -9.754907e-06 |
| Model-TimeModelFit      | 30.4          |
| ModelSampler-n_times... | 1240000       |
| Policy-AverageAbsPol... | 0.63254917    |
| Policy-AverageDiscou... | -6.4e+04      |
| Policy-AveragePolicyStd | 0.64244044    |
| Policy-AverageReturn    | -4.03e+05     |
| Policy-MaxReturn        | -388          |
| Policy-MinReturn        | -1.24e+06     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 4.5e+05       |
| Policy-TimeAlgoOpt      | 0.628         |
| Policy-TimeSampleProc   | 0.521         |
| Policy-TimeSampling     | 2.08          |
| Policy-TimeStep         | 3.29          |
| Time                    | 815           |
| n_timesteps             | 31000         |
-------------------------------------------

 ---------------- Iteration 31 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 310 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 311 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 312 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 313 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 314 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 315 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 316 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 317 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 318 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 319 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.473         |
| Data-EnvSampler-Poli... | 0.967         |
| Data-EnvTrajs-Averag... | -378          |
| Data-EnvTrajs-MaxReturn | -276          |
| Data-EnvTrajs-MinReturn | -434          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 66.1          |
| Data-TimeEnvSampleProc  | 0.000825      |
| Data-TimeEnvSampling    | 1.48          |
| Iteration               | 31            |
| ItrTime                 | 34.5          |
| LossAfter               | -0.008345983  |
| LossBefore              | -9.599565e-06 |
| Model-TimeModelFit      | 30.2          |
| ModelSampler-n_times... | 1280000       |
| Policy-AverageAbsPol... | 0.55323076    |
| Policy-AverageDiscou... | -4.64e+03     |
| Policy-AveragePolicyStd | 0.63157046    |
| Policy-AverageReturn    | -3.06e+04     |
| Policy-MaxReturn        | 340           |
| Policy-MinReturn        | -6.07e+05     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 1.32e+05      |
| Policy-TimeAlgoOpt      | 0.653         |
| Policy-TimeSampleProc   | 0.352         |
| Policy-TimeSampling     | 1.75          |
| Policy-TimeStep         | 2.78          |
| Time                    | 850           |
| n_timesteps             | 32000         |
-------------------------------------------

 ---------------- Iteration 32 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 320 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 321 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 322 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 323 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 324 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 325 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 326 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 327 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 328 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 329 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.616         |
| Data-EnvSampler-Poli... | 1.35          |
| Data-EnvTrajs-Averag... | -341          |
| Data-EnvTrajs-MaxReturn | -309          |
| Data-EnvTrajs-MinReturn | -366          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 20.9          |
| Data-TimeEnvSampleProc  | 0.0013        |
| Data-TimeEnvSampling    | 2.02          |
| Iteration               | 32            |
| ItrTime                 | 35.3          |
| LossAfter               | -0.0060610045 |
| LossBefore              | -9.425057e-06 |
| Model-TimeModelFit      | 30.7          |
| ModelSampler-n_times... | 1320000       |
| Policy-AverageAbsPol... | 0.54029274    |
| Policy-AverageDiscou... | -101          |
| Policy-AveragePolicyStd | 0.62198573    |
| Policy-AverageReturn    | -264          |
| Policy-MaxReturn        | 106           |
| Policy-MinReturn        | -740          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 204           |
| Policy-TimeAlgoOpt      | 0.591         |
| Policy-TimeSampleProc   | 0.279         |
| Policy-TimeSampling     | 1.69          |
| Policy-TimeStep         | 2.58          |
| Time                    | 885           |
| n_timesteps             | 33000         |
-------------------------------------------

 ---------------- Iteration 33 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 330 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 331 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 332 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 333 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 334 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 335 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 336 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 337 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 338 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 339 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.462         |
| Data-EnvSampler-Poli... | 0.951         |
| Data-EnvTrajs-Averag... | -375          |
| Data-EnvTrajs-MaxReturn | -250          |
| Data-EnvTrajs-MinReturn | -464          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 88            |
| Data-TimeEnvSampleProc  | 0.000954      |
| Data-TimeEnvSampling    | 1.45          |
| Iteration               | 33            |
| ItrTime                 | 35.6          |
| LossAfter               | -0.007191284  |
| LossBefore              | -9.320697e-06 |
| Model-TimeModelFit      | 31.1          |
| ModelSampler-n_times... | 1360000       |
| Policy-AverageAbsPol... | 0.64930034    |
| Policy-AverageDiscou... | -105          |
| Policy-AveragePolicyStd | 0.6153383     |
| Policy-AverageReturn    | -308          |
| Policy-MaxReturn        | 253           |
| Policy-MinReturn        | -881          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 276           |
| Policy-TimeAlgoOpt      | 0.651         |
| Policy-TimeSampleProc   | 0.518         |
| Policy-TimeSampling     | 1.79          |
| Policy-TimeStep         | 3.02          |
| Time                    | 921           |
| n_timesteps             | 34000         |
-------------------------------------------

 ---------------- Iteration 34 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 340 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 341 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 342 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 343 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 344 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 345 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 346 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 347 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 348 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 349 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.528         |
| Data-EnvSampler-Poli... | 0.985         |
| Data-EnvTrajs-Averag... | -352          |
| Data-EnvTrajs-MaxReturn | -269          |
| Data-EnvTrajs-MinReturn | -473          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 71.7          |
| Data-TimeEnvSampleProc  | 0.00108       |
| Data-TimeEnvSampling    | 1.55          |
| Iteration               | 34            |
| ItrTime                 | 35.7          |
| LossAfter               | -0.00572489   |
| LossBefore              | -9.172808e-06 |
| Model-TimeModelFit      | 31.4          |
| ModelSampler-n_times... | 1400000       |
| Policy-AverageAbsPol... | 0.58305377    |
| Policy-AverageDiscou... | -90.1         |
| Policy-AveragePolicyStd | 0.6059054     |
| Policy-AverageReturn    | -336          |
| Policy-MaxReturn        | 26            |
| Policy-MinReturn        | -870          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 229           |
| Policy-TimeAlgoOpt      | 0.604         |
| Policy-TimeSampleProc   | 0.341         |
| Policy-TimeSampling     | 1.83          |
| Policy-TimeStep         | 2.82          |
| Time                    | 956           |
| n_timesteps             | 35000         |
-------------------------------------------

 ---------------- Iteration 35 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 350 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 351 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 352 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 353 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 354 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 355 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 356 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 357 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 358 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 359 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.579         |
| Data-EnvSampler-Poli... | 1.04          |
| Data-EnvTrajs-Averag... | -388          |
| Data-EnvTrajs-MaxReturn | -314          |
| Data-EnvTrajs-MinReturn | -474          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 62.2          |
| Data-TimeEnvSampleProc  | 0.00117       |
| Data-TimeEnvSampling    | 1.67          |
| Iteration               | 35            |
| ItrTime                 | 36.3          |
| LossAfter               | -0.006732552  |
| LossBefore              | -9.176469e-06 |
| Model-TimeModelFit      | 32.1          |
| ModelSampler-n_times... | 1440000       |
| Policy-AverageAbsPol... | 0.5358563     |
| Policy-AverageDiscou... | -43.9         |
| Policy-AveragePolicyStd | 0.6059597     |
| Policy-AverageReturn    | -180          |
| Policy-MaxReturn        | 79.9          |
| Policy-MinReturn        | -429          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 117           |
| Policy-TimeAlgoOpt      | 0.591         |
| Policy-TimeSampleProc   | 0.429         |
| Policy-TimeSampling     | 1.52          |
| Policy-TimeStep         | 2.56          |
| Time                    | 993           |
| n_timesteps             | 36000         |
-------------------------------------------

 ---------------- Iteration 36 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 360 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 361 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 362 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 363 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 364 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 365 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 366 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 367 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 368 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 369 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.581         |
| Data-EnvSampler-Poli... | 1.15          |
| Data-EnvTrajs-Averag... | -337          |
| Data-EnvTrajs-MaxReturn | -332          |
| Data-EnvTrajs-MinReturn | -348          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.11          |
| Data-TimeEnvSampleProc  | 0.00105       |
| Data-TimeEnvSampling    | 1.77          |
| Iteration               | 36            |
| ItrTime                 | 35.9          |
| LossAfter               | -0.007357879  |
| LossBefore              | -9.031799e-06 |
| Model-TimeModelFit      | 30.7          |
| ModelSampler-n_times... | 1480000       |
| Policy-AverageAbsPol... | 0.47912467    |
| Policy-AverageDiscou... | -106          |
| Policy-AveragePolicyStd | 0.5975561     |
| Policy-AverageReturn    | -239          |
| Policy-MaxReturn        | -45.3         |
| Policy-MinReturn        | -366          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 84.3          |
| Policy-TimeAlgoOpt      | 0.712         |
| Policy-TimeSampleProc   | 0.578         |
| Policy-TimeSampling     | 2             |
| Policy-TimeStep         | 3.35          |
| Time                    | 1.03e+03      |
| n_timesteps             | 37000         |
-------------------------------------------

 ---------------- Iteration 37 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 370 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 371 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 372 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 373 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 374 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 375 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 376 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 377 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 378 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 379 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
------------------------------------------
| Data-EnvSampler-EnvE... | 0.567        |
| Data-EnvSampler-Poli... | 1.31         |
| Data-EnvTrajs-Averag... | -394         |
| Data-EnvTrajs-MaxReturn | -375         |
| Data-EnvTrajs-MinReturn | -415         |
| Data-EnvTrajs-NumTrajs  | 5            |
| Data-EnvTrajs-StdReturn | 15           |
| Data-TimeEnvSampleProc  | 0.000993     |
| Data-TimeEnvSampling    | 1.95         |
| Iteration               | 37           |
| ItrTime                 | 36.4         |
| LossAfter               | -0.010343206 |
| LossBefore              | -8.87359e-06 |
| Model-TimeModelFit      | 32           |
| ModelSampler-n_times... | 1520000      |
| Policy-AverageAbsPol... | 0.47403032   |
| Policy-AverageDiscou... | -149         |
| Policy-AveragePolicyStd | 0.5870367    |
| Policy-AverageReturn    | -958         |
| Policy-MaxReturn        | 421          |
| Policy-MinReturn        | -2.12e+04    |
| Policy-NumTrajs         | 20           |
| Policy-StdReturn        | 4.64e+03     |
| Policy-TimeAlgoOpt      | 0.601        |
| Policy-TimeSampleProc   | 0.245        |
| Policy-TimeSampling     | 1.55         |
| Policy-TimeStep         | 2.42         |
| Time                    | 1.06e+03     |
| n_timesteps             | 38000        |
------------------------------------------

 ---------------- Iteration 38 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 380 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 381 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 382 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 383 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 384 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 385 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 386 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 387 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 388 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 389 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.449         |
| Data-EnvSampler-Poli... | 0.891         |
| Data-EnvTrajs-Averag... | -349          |
| Data-EnvTrajs-MaxReturn | -339          |
| Data-EnvTrajs-MinReturn | -357          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.58          |
| Data-TimeEnvSampleProc  | 0.00104       |
| Data-TimeEnvSampling    | 1.38          |
| Iteration               | 38            |
| ItrTime                 | 36.5          |
| LossAfter               | -0.0056048967 |
| LossBefore              | -8.61275e-06  |
| Model-TimeModelFit      | 31.7          |
| ModelSampler-n_times... | 1560000       |
| Policy-AverageAbsPol... | 0.53583       |
| Policy-AverageDiscou... | -184          |
| Policy-AveragePolicyStd | 0.57191515    |
| Policy-AverageReturn    | -862          |
| Policy-MaxReturn        | 408           |
| Policy-MinReturn        | -9.4e+03      |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 2.66e+03      |
| Policy-TimeAlgoOpt      | 0.635         |
| Policy-TimeSampleProc   | 0.634         |
| Policy-TimeSampling     | 2.07          |
| Policy-TimeStep         | 3.4           |
| Time                    | 1.1e+03       |
| n_timesteps             | 39000         |
-------------------------------------------

 ---------------- Iteration 39 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 390 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 391 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 392 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 393 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 394 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 395 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 396 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 397 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 398 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 399 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.556         |
| Data-EnvSampler-Poli... | 1.01          |
| Data-EnvTrajs-Averag... | -343          |
| Data-EnvTrajs-MaxReturn | -316          |
| Data-EnvTrajs-MinReturn | -363          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 17.4          |
| Data-TimeEnvSampleProc  | 0.00274       |
| Data-TimeEnvSampling    | 1.61          |
| Iteration               | 39            |
| ItrTime                 | 37            |
| LossAfter               | -0.007508345  |
| LossBefore              | -8.509025e-06 |
| Model-TimeModelFit      | 32.7          |
| ModelSampler-n_times... | 1600000       |
| Policy-AverageAbsPol... | 0.6516618     |
| Policy-AverageDiscou... | -736          |
| Policy-AveragePolicyStd | 0.566425      |
| Policy-AverageReturn    | -5.07e+03     |
| Policy-MaxReturn        | 122           |
| Policy-MinReturn        | -9.18e+04     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 2e+04         |
| Policy-TimeAlgoOpt      | 0.621         |
| Policy-TimeSampleProc   | 0.369         |
| Policy-TimeSampling     | 1.65          |
| Policy-TimeStep         | 2.7           |
| Time                    | 1.14e+03      |
| n_timesteps             | 40000         |
-------------------------------------------

 ---------------- Iteration 40 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 400 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 401 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 402 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 403 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 404 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 405 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 406 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 407 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 408 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 409 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.544         |
| Data-EnvSampler-Poli... | 1.03          |
| Data-EnvTrajs-Averag... | -327          |
| Data-EnvTrajs-MaxReturn | -316          |
| Data-EnvTrajs-MinReturn | -338          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.86          |
| Data-TimeEnvSampleProc  | 0.00099       |
| Data-TimeEnvSampling    | 1.62          |
| Iteration               | 40            |
| ItrTime                 | 36.9          |
| LossAfter               | -0.0036687877 |
| LossBefore              | -8.349631e-06 |
| Model-TimeModelFit      | 32.8          |
| ModelSampler-n_times... | 1640000       |
| Policy-AverageAbsPol... | 0.5673441     |
| Policy-AverageDiscou... | -48.5         |
| Policy-AveragePolicyStd | 0.5579765     |
| Policy-AverageReturn    | -91.3         |
| Policy-MaxReturn        | 221           |
| Policy-MinReturn        | -242          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 105           |
| Policy-TimeAlgoOpt      | 0.587         |
| Policy-TimeSampleProc   | 0.3           |
| Policy-TimeSampling     | 1.57          |
| Policy-TimeStep         | 2.49          |
| Time                    | 1.18e+03      |
| n_timesteps             | 41000         |
-------------------------------------------

 ---------------- Iteration 41 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 410 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 411 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 412 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 413 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 414 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 415 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 416 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 417 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 418 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 419 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.47          |
| Data-EnvSampler-Poli... | 0.963         |
| Data-EnvTrajs-Averag... | -324          |
| Data-EnvTrajs-MaxReturn | -307          |
| Data-EnvTrajs-MinReturn | -332          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 9.14          |
| Data-TimeEnvSampleProc  | 0.000949      |
| Data-TimeEnvSampling    | 1.47          |
| Iteration               | 41            |
| ItrTime                 | 37.1          |
| LossAfter               | -0.007451035  |
| LossBefore              | -8.330065e-06 |
| Model-TimeModelFit      | 32.8          |
| ModelSampler-n_times... | 1680000       |
| Policy-AverageAbsPol... | 0.5642398     |
| Policy-AverageDiscou... | -56.7         |
| Policy-AveragePolicyStd | 0.55636513    |
| Policy-AverageReturn    | -226          |
| Policy-MaxReturn        | 371           |
| Policy-MinReturn        | -1.72e+03     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 416           |
| Policy-TimeAlgoOpt      | 0.63          |
| Policy-TimeSampleProc   | 0.431         |
| Policy-TimeSampling     | 1.7           |
| Policy-TimeStep         | 2.82          |
| Time                    | 1.21e+03      |
| n_timesteps             | 42000         |
-------------------------------------------

 ---------------- Iteration 42 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 420 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 421 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 422 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 423 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 424 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 425 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 426 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 427 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 428 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 429 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.489         |
| Data-EnvSampler-Poli... | 0.896         |
| Data-EnvTrajs-Averag... | -309          |
| Data-EnvTrajs-MaxReturn | -295          |
| Data-EnvTrajs-MinReturn | -322          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 9.25          |
| Data-TimeEnvSampleProc  | 0.000685      |
| Data-TimeEnvSampling    | 1.42          |
| Iteration               | 42            |
| ItrTime                 | 37.2          |
| LossAfter               | -0.008053411  |
| LossBefore              | -8.220451e-06 |
| Model-TimeModelFit      | 32.9          |
| ModelSampler-n_times... | 1720000       |
| Policy-AverageAbsPol... | 0.5717049     |
| Policy-AverageDiscou... | -7.22         |
| Policy-AveragePolicyStd | 0.5510105     |
| Policy-AverageReturn    | 63.7          |
| Policy-MaxReturn        | 403           |
| Policy-MinReturn        | -1.94e+03     |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 492           |
| Policy-TimeAlgoOpt      | 0.617         |
| Policy-TimeSampleProc   | 0.491         |
| Policy-TimeSampling     | 1.69          |
| Policy-TimeStep         | 2.86          |
| Time                    | 1.25e+03      |
| n_timesteps             | 43000         |
-------------------------------------------

 ---------------- Iteration 43 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 430 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 431 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 432 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 433 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 434 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 435 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 436 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 437 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 438 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 439 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.522         |
| Data-EnvSampler-Poli... | 1.03          |
| Data-EnvTrajs-Averag... | -345          |
| Data-EnvTrajs-MaxReturn | -323          |
| Data-EnvTrajs-MinReturn | -403          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 29.6          |
| Data-TimeEnvSampleProc  | 0.00135       |
| Data-TimeEnvSampling    | 1.58          |
| Iteration               | 43            |
| ItrTime                 | 37.7          |
| LossAfter               | -0.005985422  |
| LossBefore              | -7.978518e-06 |
| Model-TimeModelFit      | 33.4          |
| ModelSampler-n_times... | 1760000       |
| Policy-AverageAbsPol... | 0.572493      |
| Policy-AverageDiscou... | -76.5         |
| Policy-AveragePolicyStd | 0.5375928     |
| Policy-AverageReturn    | -180          |
| Policy-MaxReturn        | 44.3          |
| Policy-MinReturn        | -331          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 92.3          |
| Policy-TimeAlgoOpt      | 0.593         |
| Policy-TimeSampleProc   | 0.357         |
| Policy-TimeSampling     | 1.72          |
| Policy-TimeStep         | 2.7           |
| Time                    | 1.29e+03      |
| n_timesteps             | 44000         |
-------------------------------------------

 ---------------- Iteration 44 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 440 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 441 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 442 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 443 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 444 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 445 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 446 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 447 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 448 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 449 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.445         |
| Data-EnvSampler-Poli... | 0.896         |
| Data-EnvTrajs-Averag... | -292          |
| Data-EnvTrajs-MaxReturn | -272          |
| Data-EnvTrajs-MinReturn | -312          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 15.4          |
| Data-TimeEnvSampleProc  | 0.000597      |
| Data-TimeEnvSampling    | 1.38          |
| Iteration               | 44            |
| ItrTime                 | 36.6          |
| LossAfter               | -0.005965618  |
| LossBefore              | -7.636627e-06 |
| Model-TimeModelFit      | 32.7          |
| ModelSampler-n_times... | 1800000       |
| Policy-AverageAbsPol... | 0.6583535     |
| Policy-AverageDiscou... | -113          |
| Policy-AveragePolicyStd | 0.519405      |
| Policy-AverageReturn    | -339          |
| Policy-MaxReturn        | -300          |
| Policy-MinReturn        | -365          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 18            |
| Policy-TimeAlgoOpt      | 0.622         |
| Policy-TimeSampleProc   | 0.299         |
| Policy-TimeSampling     | 1.57          |
| Policy-TimeStep         | 2.53          |
| Time                    | 1.32e+03      |
| n_timesteps             | 45000         |
-------------------------------------------

 ---------------- Iteration 45 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 450 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 451 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 452 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 453 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 454 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 455 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 456 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 457 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 458 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 459 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.586         |
| Data-EnvSampler-Poli... | 1.1           |
| Data-EnvTrajs-Averag... | -319          |
| Data-EnvTrajs-MaxReturn | -295          |
| Data-EnvTrajs-MinReturn | -343          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 19.9          |
| Data-TimeEnvSampleProc  | 0.00114       |
| Data-TimeEnvSampling    | 1.73          |
| Iteration               | 45            |
| ItrTime                 | 37.7          |
| LossAfter               | -0.010022679  |
| LossBefore              | -7.430046e-06 |
| Model-TimeModelFit      | 32.7          |
| ModelSampler-n_times... | 1840000       |
| Policy-AverageAbsPol... | 0.5850533     |
| Policy-AverageDiscou... | -67.4         |
| Policy-AveragePolicyStd | 0.5090924     |
| Policy-AverageReturn    | -185          |
| Policy-MaxReturn        | 248           |
| Policy-MinReturn        | -334          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 114           |
| Policy-TimeAlgoOpt      | 0.601         |
| Policy-TimeSampleProc   | 0.553         |
| Policy-TimeSampling     | 2.03          |
| Policy-TimeStep         | 3.25          |
| Time                    | 1.36e+03      |
| n_timesteps             | 46000         |
-------------------------------------------

 ---------------- Iteration 46 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 460 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 461 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 462 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 463 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 464 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 465 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 466 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 467 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 468 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 469 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.457         |
| Data-EnvSampler-Poli... | 0.886         |
| Data-EnvTrajs-Averag... | -314          |
| Data-EnvTrajs-MaxReturn | -300          |
| Data-EnvTrajs-MinReturn | -331          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 10.8          |
| Data-TimeEnvSampleProc  | 0.000757      |
| Data-TimeEnvSampling    | 1.38          |
| Iteration               | 46            |
| ItrTime                 | 37.6          |
| LossAfter               | -0.006472925  |
| LossBefore              | -7.330325e-06 |
| Model-TimeModelFit      | 32.6          |
| ModelSampler-n_times... | 1880000       |
| Policy-AverageAbsPol... | 0.5962273     |
| Policy-AverageDiscou... | -43.5         |
| Policy-AveragePolicyStd | 0.5040959     |
| Policy-AverageReturn    | -89           |
| Policy-MaxReturn        | 454           |
| Policy-MinReturn        | -244          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 189           |
| Policy-TimeAlgoOpt      | 0.629         |
| Policy-TimeSampleProc   | 0.776         |
| Policy-TimeSampling     | 2.12          |
| Policy-TimeStep         | 3.59          |
| Time                    | 1.4e+03       |
| n_timesteps             | 47000         |
-------------------------------------------

 ---------------- Iteration 47 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 470 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 471 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 472 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 473 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 474 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 475 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 476 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 477 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 478 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 479 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
--------------------------------------------
| Data-EnvSampler-EnvE... | 0.587          |
| Data-EnvSampler-Poli... | 1.11           |
| Data-EnvTrajs-Averag... | -343           |
| Data-EnvTrajs-MaxReturn | -329           |
| Data-EnvTrajs-MinReturn | -358           |
| Data-EnvTrajs-NumTrajs  | 5              |
| Data-EnvTrajs-StdReturn | 9.91           |
| Data-TimeEnvSampleProc  | 0.00102        |
| Data-TimeEnvSampling    | 1.74           |
| Iteration               | 47             |
| ItrTime                 | 38             |
| LossAfter               | -0.010543498   |
| LossBefore              | -7.3434826e-06 |
| Model-TimeModelFit      | 32.7           |
| ModelSampler-n_times... | 1920000        |
| Policy-AverageAbsPol... | 0.6229742      |
| Policy-AverageDiscou... | -211           |
| Policy-AveragePolicyStd | 0.5050116      |
| Policy-AverageReturn    | -1.07e+03      |
| Policy-MaxReturn        | 347            |
| Policy-MinReturn        | -1.57e+04      |
| Policy-NumTrajs         | 20             |
| Policy-StdReturn        | 3.37e+03       |
| Policy-TimeAlgoOpt      | 0.681          |
| Policy-TimeSampleProc   | 0.613          |
| Policy-TimeSampling     | 2.15           |
| Policy-TimeStep         | 3.51           |
| Time                    | 1.44e+03       |
| n_timesteps             | 48000          |
--------------------------------------------

 ---------------- Iteration 48 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 480 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 481 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 482 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 483 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 484 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 485 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 486 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 487 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 488 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 489 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.487         |
| Data-EnvSampler-Poli... | 0.912         |
| Data-EnvTrajs-Averag... | -362          |
| Data-EnvTrajs-MaxReturn | -354          |
| Data-EnvTrajs-MinReturn | -374          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 6.67          |
| Data-TimeEnvSampleProc  | 0.00124       |
| Data-TimeEnvSampling    | 1.44          |
| Iteration               | 48            |
| ItrTime                 | 36.7          |
| LossAfter               | -0.0037267276 |
| LossBefore              | -7.245528e-06 |
| Model-TimeModelFit      | 32.6          |
| ModelSampler-n_times... | 1960000       |
| Policy-AverageAbsPol... | 0.5222271     |
| Policy-AverageDiscou... | -76.8         |
| Policy-AveragePolicyStd | 0.49873683    |
| Policy-AverageReturn    | -189          |
| Policy-MaxReturn        | 14.8          |
| Policy-MinReturn        | -332          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 119           |
| Policy-TimeAlgoOpt      | 0.64          |
| Policy-TimeSampleProc   | 0.381         |
| Policy-TimeSampling     | 1.62          |
| Policy-TimeStep         | 2.66          |
| Time                    | 1.47e+03      |
| n_timesteps             | 49000         |
-------------------------------------------

 ---------------- Iteration 49 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 490 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 491 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 492 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 493 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 494 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 495 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 496 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 497 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 498 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 499 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.837         |
| Data-EnvSampler-Poli... | 1.7           |
| Data-EnvTrajs-Averag... | -333          |
| Data-EnvTrajs-MaxReturn | -324          |
| Data-EnvTrajs-MinReturn | -344          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 8.31          |
| Data-TimeEnvSampleProc  | 0.00115       |
| Data-TimeEnvSampling    | 2.61          |
| Iteration               | 49            |
| ItrTime                 | 40.5          |
| LossAfter               | -0.0039307475 |
| LossBefore              | -7.05048e-06  |
| Model-TimeModelFit      | 34.5          |
| ModelSampler-n_times... | 2000000       |
| Policy-AverageAbsPol... | 0.4904813     |
| Policy-AverageDiscou... | -103          |
| Policy-AveragePolicyStd | 0.4892578     |
| Policy-AverageReturn    | -279          |
| Policy-MaxReturn        | -122          |
| Policy-MinReturn        | -347          |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 55.5          |
| Policy-TimeAlgoOpt      | 0.72          |
| Policy-TimeSampleProc   | 0.35          |
| Policy-TimeSampling     | 2.24          |
| Policy-TimeStep         | 3.34          |
| Time                    | 1.51e+03      |
| n_timesteps             | 50000         |
-------------------------------------------

 ---------------- Iteration 50 ----------------
Obtaining samples from the environment using the policy...
Processing environment samples...
Training dynamics model for 50 epochs ...

 ---------------- Grad-Step 500 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 501 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 502 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 503 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 504 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 505 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 506 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 507 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 508 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...

 ---------------- Grad-Step 509 ----------------
Obtaining samples from the model...
Processing samples from the model...
Optimizing policy...
Saving snapshot...
Saved
-------------------------------------------
| Data-EnvSampler-EnvE... | 0.572         |
| Data-EnvSampler-Poli... | 1.08          |
| Data-EnvTrajs-Averag... | -311          |
| Data-EnvTrajs-MaxReturn | -294          |
| Data-EnvTrajs-MinReturn | -328          |
| Data-EnvTrajs-NumTrajs  | 5             |
| Data-EnvTrajs-StdReturn | 12.6          |
| Data-TimeEnvSampleProc  | 0.0011        |
| Data-TimeEnvSampling    | 1.69          |
| Iteration               | 50            |
| ItrTime                 | 36            |
| LossAfter               | -0.0103692245 |
| LossBefore              | -6.852668e-06 |
| Model-TimeModelFit      | 31.9          |
| ModelSampler-n_times... | 2040000       |
| Policy-AverageAbsPol... | 0.51808053    |
| Policy-AverageDiscou... | -63.3         |
| Policy-AveragePolicyStd | 0.4805847     |
| Policy-AverageReturn    | -372          |
| Policy-MaxReturn        | 784           |
| Policy-MinReturn        | -7.7e+03      |
| Policy-NumTrajs         | 20            |
| Policy-StdReturn        | 1.7e+03       |
| Policy-TimeAlgoOpt      | 0.603         |
| Policy-TimeSampleProc   | 0.336         |
| Policy-TimeSampling     | 1.51          |
| Policy-TimeStep         | 2.48          |
| Time                    | 1.55e+03      |
| n_timesteps             | 51000         |
-------------------------------------------
Training finished
